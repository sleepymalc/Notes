\lecture{18}{24 Oct.\ 11:00}{Discrete Multiplicative Weight Update Algorithm}
\subsection{Discrete Time Multiplicative Weight Update Algorithm}
We want to obtain an algorithm that terminate in a few iterations.

\begin{prev}
	Recall that we're working with a convex program
	\[
		\begin{aligned}
			\max~ & f(x)                              \\
			      & g_i(x) \leq 1 & \forall i \in [m] \\
			      & x \in P,
		\end{aligned}
	\]
	where \(f\) is concave and \(g_i\)'s are convex. The underlying linear programs we're interested in are
	\[
		\begin{aligned}
			\max~ & c^{\top} x \\
			      & Ax \leq b  \\
			      & x \geq 0,
		\end{aligned}
	\]
	where \(c \in \mathbb{R} _{+}^n\), \(A \in \mathbb{R} _{+}^{m \times n}\), and \(b \in \mathbb{R} _{+}^m\).
\end{prev}

For this, we will work with the extra condition that \(g_i(x) \geq 0\) for all \(x \in P\). This models the \hyperref[def:packing-LP]{packing} constraints of interest and is needed for the algorithms and analysis, unlike the \hyperref[algo:MWU-continuous]{continuous algorithm}.

Firstly, consider the simple uniform step size algorithm \autoref{algo:MWU-discrete-uniform}. For simplicity, \(\delta \) is chosen that there is an integer \(T\) such that \(T \delta = 1\). Thus, the algorithm runs for \(T\) time steps, and the goal is to minimize \(T\) such that the discrete implementation ensures that the constraints are approximately satisfied. It turns out that \(T\) is related to a parameter called the \hyperref[def:width]{width} of the given instance.

\begin{definition}[Width]\label{def:width}
	The \emph{width} \(\rho \) of the given instance is defined as \(\rho \coloneqq \sup _{x \in P, i \in [m]} g_i(x)\).
\end{definition}

In other words, it is asking how much can some arbitrary point in \(P\) violate a specific constraint in a \emph{relative sense}.\footnote{Since we have normalized the right-hand-side of each constraint to \(1\) by considering \(g_i(x) \leq 1\) as the constraint.} Recall that we solve a Lagrangian relaxation in each step and find a point \(v(t)\), which can violate some specific constraint \(i\) by  significant factor since we are only solving the relaxation by taking weighted sum of the constraints. Hence, \(\rho \) allows us to bound the worst-case violation.

It may seem that \(\rho \) is unbounded for even simple instances because \(g_i\) may be an increasing function and \(P\) is typically the non-negative orthant. However, for many problems of interest, there are \hyperref[def:implicit-LP]{implicit} bounds on the variables and \(P\) can be implicitly taken to be a bounding box that restrict \(\rho \). We will give some examples later on to illustrate this. Suppose for now that \(\rho \) is some bounded number.

\begin{theorem}\label{thm:MWU-uniform-step}
	Suppose \(\eta = \ln m / \epsilon \), \(\delta = \epsilon / \eta \rho \), and \(\epsilon \in (0, 1 / 2)\). Then in \(O(\rho \ln m / \epsilon ^2)\) iterations, \autoref{algo:MWU-discrete-uniform} will output a point \(x_{\text{out} }\) such that
	\begin{enumerate}[(a)]
		\item \(f(x_{\text{out} }) \geq \OPT\), and
		\item \(g_i(x_{\text{out} }) \leq 1 + 2\epsilon \) for all \(i \in [m]\).
	\end{enumerate}
\end{theorem}

Instead of proving \autoref{thm:MWU-uniform-step}, we consider the non-uniform step size case, which turns out to be stronger and \autoref{thm:MWU-uniform-step} can be reduced to which. Specifically, Garg and Konemann~\cite{garg2007faster} obtained a \hyperref[def:width]{width}-independent algorithm and analysis by a seemingly simple but important tweak to \autoref{algo:MWU-discrete-uniform}:

\begin{intuition}
	We choose the maximum step size at any point greedily so that the analysis goes through.
\end{intuition}

In particular, all we need is that we want to find \(\delta \) such that \(w_i(t + \delta ) \leq w_i(t) \cdot e^{\epsilon } \), i.e., \(\delta \) is the maximum value such that \(\eta \delta g_i(v(t)) \leq \epsilon \).

\begin{note}
	The choice of the step size requires \(g_i \geq 0\) over the domain \(P\), otherwise, it is not well-defined.
\end{note}

Surprisingly, this yields an algorithm that bounds the number of iterations to \(O(m \log m / \epsilon ^2)\), which depends only on the number of constraints and \(\epsilon \) and is \hyperref[def:width]{width}-independent. We see the algorithm directly.

\begin{algorithm}[H]\label{algo:MWU-discrete-non-uniform}
	\DontPrintSemicolon{}
	\caption{Multiplicative Weight Update Non-Uniform Step}
	\KwData{An objective \(f\), constraints \(\{ g_i \} _{i=1}^{m}\), feasible set \(P\),  parameter \(\eta \)}
	\KwResult{A solution \(x_{\text{out} }\)}
	\BlankLine

	\For(){\(i = 1, \dots , m\)}{
		\(w_i(0) \gets 1\)\;
	}
	\(x(0) = 0\)\Comment*[r]{\(x \in \mathbb{R} ^n\)}
	\(t = 0\)\;
	\;
	\While(){\(t < 1\)}{
	\(v(t) \gets\)\Oracle{\(f\), \(\{ g_i \} _{i=1}^{m}\), \(\{ w_i(t) \} _{i=1}^{m}\), \(P\)}\;
	\(\delta \gets \max \delta ^{\prime} \) such that \(\max _{i \in [m]} \delta ^{\prime} \eta g_i(v(t)) \leq \epsilon \)\Comment*[r]{Max step size}
	\(\delta \gets \min (\delta , 1 - t)\)\Comment*[r]{Handle the last iteration}
	\For(){\(i = 1, \dots , m\)}{
		\(w_i(t+\delta ) \gets w_i(t) \cdot e^{\eta \delta g_i(v (t))}\)\;
	}
	\(x (t+\delta ) \gets x (t) + \delta v (t)\)\;
	\(t \gets t + \delta \)\;
	}
	\Return{\(x(1)\)}\;
\end{algorithm}

We will mimic the continuous time algorithm (\autoref{algo:MWU-continuous}) analysis in some ways and point out where the discretization matters and how we bound the errors. First, let's set up some notations.

\begin{notation}
	The algorithm runs for some number \(T\) of iterations, and we use \(j\) to index the iterations and \(i\) as before to index the constraints. We think of the time before the start of iteration \(j+1\) as \(t_j\), and the step size in that iteration as \(\delta _j\). We have \(t_0 = 0\) and \(t_{j+1} = t_j + \delta _j\) for \(j = 0, 1, \dots , T-1\) such that \(\sum_{j=0}^{T-1} \delta _j = 1\). Note that in iteration \(j+1\), we compute \(v(t_j)\) and \(\delta _j\) w.r.t.\ weights \(w_i(t_j)\)'s. In other words, \(t_{j+1}\) is the time at the end of iteration \(j+1\).
\end{notation}

The following is easy to prove even with a discrete step size (compared to \autoref{lma:MWU-continuous-OPT}).

\begin{lemma}\label{lma:MWU-discrete-non-uniform-OPT}
	In \autoref{algo:MWU-discrete-non-uniform}, if \(f\) is concave, then \(f(x _{\text{out} }) = f(x (1)) \geq \OPT\).
\end{lemma}
\begin{proof}
	From the definition and concavity of \(f\), we have
	\[
		f(x _{\text{out} })
		= f \left( \sum_{j=0}^{T-1} \delta _{j} v (t_j) \right)
		\geq \sum_{j=0}^{T-1} \delta _j f(v (t_j))
		\geq \sum_{j=0}^{T-1} \delta _j \OPT
		\geq \OPT,
	\]
	where we again use the fact that \(v(t_j)\) is the solution of the Lagrangian relaxation.
\end{proof}

A similar claim to \autoref{lma:MWU-continuous-constraint} still holds in the discrete setting, but the proof is useful to see.

\begin{lemma}\label{lma:MWU-discrete-non-uniform-constraint}
	In \autoref{algo:MWU-discrete-non-uniform}, for each \(i \in [m]\), we have \(g_i(x _{\text{out} }) \leq \ln w_i(1) / \eta \).
\end{lemma}
\begin{proof}
	We have \(x_{\text{out} } = \sum_{j=0}^{T-1} \delta _j v(t_j)\), and hence by convexity of \(g_i\), \(g(x_{\text{out} }) \leq \sum_{j=0}^{T-1} \delta _j g(v(t_j))\). We see that in iteration \(j+1\), the weight of constraint \(i\) is updated as
	\[
		w_i(t_{j+1})
		= w_i(t_j) \exp (\eta \delta _j g_i(v(t_j)))
		\implies \delta _j g_i(v(t_j))
		= \frac{\ln w_i(t_{j+1}) - \ln w_i(t_j)}{\eta }.
	\]
	Summing up two sides over \(j\) and using telescoping, we have
	\[
		g(x_{\text{out} })
		\leq \sum_{j=0}^{T-1} \delta _j g(v(t_j))
		\leq \frac{\ln w_i(1) - \ln w_i(0)}{\eta },
	\]
	with \(w_i(0) = 1\), we're done.
\end{proof}

The main technical lemma is the following, which bounds the evolution of the total weight and this is where the choice of the step size is crucial.

\begin{lemma}\label{lma:MWU-discrete-non-uniform-weight}
	In \autoref{algo:MWU-discrete-non-uniform}, for all \(t_j\), \(\sum_{i=1}^{m} w_i(t_j) \leq e^{(1 + \epsilon ) \eta t_j} \sum_{i=1}^{m} w_i(0)\). Hence, \(\sum_{i=1}^{m} w_i(1) \leq m e^{(1 + \epsilon ) \eta }\).
\end{lemma}
\begin{proof}
	Consider iteration \(j + 1\). Since \(g_i\)'s are all positive on the domain, all numbers involved are positive. We see that
	\[
		\sum_{i=1}^{m} w_i(t_j + \delta _j)
		= \sum_{i=1}^{m} w_i(t_j) \exp (\eta \delta _j g_i(v (t_j))),
	\]
	where we ensure that \(\eta \delta _j g_i(v (t_j)) \leq \epsilon \leq 1 / 2\) in \autoref{algo:MWU-discrete-non-uniform}. Observe that it suffices to prove
	\[
		\sum_{i=1}^{m} w_i(t_j + \delta _j)
		\leq e^{(1 + \epsilon ) \delta _j \eta } \left( \sum_{i=1}^{m} w_i(t_j) \right) .
	\]

	\begin{claim}
		If \(a \in (0, 1 / 2)\), we have \(e^a \leq 1 + a + a^2\).
	\end{claim}
	\begin{explanation}
		This is immediate from Taylor's expansion.
	\end{explanation}

	Hence, by writing \(\eta \delta _j g_i(v(t_j)) = a_i\), we have
	\[
		\begin{split}
			\sum_{i=1}^{m} w_i(t_j) \exp (\eta \delta _j g_i(v (t_j)))
			 & \leq \sum_{i=1}^{m} w_i(t_j) \exp (1 + a_i + a_i^2)                                                                                            \\
			 & \leq \sum_{i=1}^{m} w_i(t_j) \left( 1 + a_i (1 + \epsilon ) \right)                                                                            \\
			 & \leq \sum_{i=1}^{m} w_i(t_j) + (1 + \epsilon ) \eta \delta _j \underbrace{\sum_{i=1}^{m} w_i(t_j) g_i(v (t_j))}_{\leq \sum_{i=1}^{m} w_i(t_j)} \\
			 & \leq \left( \sum_{i=1}^{m} w_i(t_j) \right) \left( 1 + (1 + \epsilon ) \eta \delta _j \right)
			\leq \left( \sum_{i=1}^{m} w_i(t_j) \right) \exp ((1 + \epsilon ) \eta \delta _j)
		\end{split}
	\]
	since \(1 + b \leq e^b\) for all \(b \geq 0\). Hence, we're done.
\end{proof}

\begin{remark}
	The proof of \autoref{lma:MWU-discrete-non-uniform-weight} will go through easily for a fixed step size as long as the step size guarantees that no weight of a constraint goes up by more than an \(e^{\epsilon } \) factor. This is satisfied by the choice of the step size based on the \hyperref[def:width]{width}! The rest of the analysis is the same.
\end{remark}

\begin{theorem}\label{thm:MWU-discrete-non-uniform}
	If \(\eta \geq \ln m / \epsilon \), then the output \(x_{\text{out} }\) of \autoref{algo:MWU-discrete-non-uniform} satisfies the following:
	\begin{enumerate}[(a)]
		\item\label{thm:MWU-discrete-non-uniform-a} \(f(x_{\text{out} }) \geq \OPT\);
		\item\label{thm:MWU-discrete-non-uniform-b} \(g_i(x_{\text{out} }) \leq 1 + 2\epsilon \) for all \(i \in [m]\).
	\end{enumerate}
	Moreover, \autoref{algo:MWU-discrete-non-uniform} terminates in \(O(m \log m / \epsilon ^2)\) iterations.
\end{theorem}
\begin{proof}
	Firstly, \autoref{thm:MWU-discrete-non-uniform-a} is shown in \autoref{thm:MWU-discrete-non-uniform}. For \autoref{thm:MWU-discrete-non-uniform-b}, from \autoref{lma:MWU-discrete-non-uniform-constraint}, we have \(g_i(x_{\text{out} }) \leq \ln w_i(1) / \eta \) and that \(\ln \sum_{i=1}^{m} w(1) \leq (1 + \epsilon ) \eta \ln m\) from \autoref{lma:MWU-discrete-non-uniform-weight}. This implies
	\[
		g_i(x _{\text{out} })
		\leq \frac{1}{\eta } \ln (w_i(1))
		\leq \frac{1}{\eta } \ln (\sum_{i=1}^{m} w_i(1))
		\leq \frac{1}{\eta } \ln (m e^{(1 + \epsilon ) \eta })
		= 1 + \epsilon + \frac{\ln m}{\eta }
		\leq 1 + 2\epsilon .
	\]
	Next, we switch to exponential weights instead of reasoning with logarithms since it will be useful later. We will also use a loose argument though one can do a tighter analysis. Let \(T\) be the total number of iterations \autoref{algo:MWU-discrete-non-uniform} takes.

	\begin{claim}
		For some fixed small constant \(c\), \(T \leq c m \ln m / \epsilon ^2\).
	\end{claim}
	\begin{explanation}
		Firstly, in the end, the total weight \(\sum_{i=1}^{m} w(1)\) is
		\[
			e^{(1 + \epsilon ) \eta }m
			= m \cdot e^{(1 + \epsilon ) (\ln m) / \epsilon }
			= m^{1 + \frac{1 + \epsilon }{\epsilon }}
			= m^{O(1 / \epsilon )}.
		\]
		Observe that since we're being greedy, in each iteration we choose \(\delta _j\) to be the maximum such that there is some \(i\) in that iteration with \(\eta \delta _j g_i(v(t_j)) = \epsilon \) exactly. But this implies that for that \(i\), its weight \(w_i\) increases by a factor of \(e^{\epsilon } \). Since each weight starts at \(1\), with the fact that the total weight at the end is \(m^{O(1 / \epsilon )}\), no weight \(w_i\) can have its weight updated by an \(e^{\epsilon } \) factor more than \(O(\log m / \epsilon ^2)\) times. But there are only \(m\) constraints and each iteration must increase at least one constraint's weight by a factor of \(e^{\epsilon } \), thus, we cannot have \(T\) more than \(cm \log m / \epsilon ^2\) for some small but fixed constant \(c\).
	\end{explanation}
	This finishes the proof.
\end{proof}

\subsection{Scaling Weights}
In the analysis so far, we have sued constraints of the form \(g_i(x) \leq 1\) which makes the notation clean. Of course, any constraint of the form \(g_i(x) \leq b_i\) with \(b_i > 0\) can be scaled to achieve the standard form. However, in some combinatorial applications, when \(A\) the packing matrix corresponds to an incidence matrix of a combinatorial object and \(b_i\) represents some capacity, scaling makes the matrix unnatural.

We can avoid this by modifying \autoref{algo:MWU-discrete-non-uniform} as follows. Suppose we have constraints of the form \(g_i(x) \leq b_i\) for all \(i \in [m]\) where \(b > 0\). we start with \(w_i(0) = 1 / b_i\) for each \(i\). Then, we do the analysis with \(b_i w_i\) instead of \(w_i\). Since the weights are updated in a multiplicative fashion, the whole analysis goes through cleanly.

\begin{note}
	With weights set up like this, we still solve the \hyperref[eq:MWU-oracle]{oracle} in each iteration with the constraint \(\sum_{i=1}^{m} w_i g_i(y) \leq \sum_{i=1}^{m} w_i\); the \(b_i\)'s are taken care of automatically.
\end{note}

\section{Application to Packing Linear Program}
We show how to generic scheme that we have described via \hyperref[algo:MWU-discrete-non-uniform]{MWU} can be used to derive fast approximation algorithms for \hyperref[def:packing-LP]{packing linear programs}. Recall that the general \hyperref[def:packing-LP]{packing linear program} looks like
\[
	\begin{aligned}
		\max~ & c^{\top} x  \\
		      & Ax \leq b ; \\
		      & x \geq 0.
	\end{aligned}
\]
In terms of the generic framework, we have \(f(x) = c^{\top} x\) and \(g_i(x) = \sum_{j=1}^{n} A_{ij} x_j\) corresponds to the \(i^{\text{th} }\) row of \(A\), and \(P\) corresponds to \(\mathbb{R} _{+}^n\).

\begin{note}
	In some settings, we think of \(P\) as a bounding box implied by constraints with \(0 \leq x_j \leq u_j\) where \(u_j\) is an upper bound on \(x_j\) implicitly implied by the constraints in \(Ax \leq b\). This often helps in figuring out the \hyperref[def:width]{width} of the system.
\end{note}

Firstly, we can interpret the weights as exponential in loads.

\begin{intuition}
	In the \hyperref[thm:MWU-continuous]{continuous MWU method}, we are maintaining \(w_i(t)\) as \(\exp (\eta \int_{0}^{t} g_i(v_t) \,\mathrm{d}t )\). When \(g_i\) is linear and \(P\) is the non-negative orthant, \(\int_{0}^{t} g_i(v_t) \,\mathrm{d}t \) is simply \(\sum_{i=1}^{n} A_{ij} x_j(t)\), where \(x(t)\) is the current accumulated vector. We think of this as the total load \(\ell _i(t)\) on constraint \(i\) at time \(t\), and hence \(w_i(t) = \exp (\eta \ell _i(t))\).
\end{intuition}

Next, we note that in the case of linear objective, it is possible to transform between the approximation guarantees for constraints (\autoref{thm:MWU-discrete-non-uniform}) and objective value.

\begin{remark}
	The \hyperref[thm:MWU-discrete-non-uniform]{MWU method}, as described, naturally give an optimum solution that violates the constraints. We can then scale the solution down to obtain a feasible solution while losing a bit in the objective. This is particularly easy for \hyperref[def:packing-LP]{packing} problems with linear objectives. Thus, we can obtain a \((1 - \epsilon )\)-approximation in the objective while obtaining a feasible solution.
\end{remark}

\subsection{Efficient Oracle for Positive Packing Linear Program}
Recall that we need an \hyperref[eq:MWU-oracle]{oracle} that given weights \(w_i\)'s, solve
\[
	\begin{aligned}
		\max~ & f(x)                                                \\
		      & \sum_{i=1}^{m} w_i g_i(x) \leq \sum_{i=1}^{m} w_i ; \\
		      & x \geq 0,
	\end{aligned}
\]
In the case of \hyperref[def:packing-LP]{packing linear programs}, consider the normalized linear program with constraints \(Ax \leq 1\), then the problem becomes
\begin{equation}\label{eq:MWU-oracle-LP}
	\begin{aligned}
		\max~ & \sum_{j=1}^{n} c_j x_j                                               \\
		      & \sum_{i=1}^{m} w_i \sum_{j=1}^{n} A_{ij} x_j \leq \sum_{i=1}^{m} w_i \\
		      & x\geq 0;
	\end{aligned}
	\iff \begin{aligned}
		\max~ & \sum_{j=1}^{n} c_j x_j                                                              \\
		      & \sum_{j=1}^{n} \left( \sum_{i=1}^{m} w_i A_{ij} \right) x_j \leq \sum_{i=1}^{m} w_i \\
		      & x\geq 0;
	\end{aligned}
	\eqqcolon \begin{aligned}
		\max~ & \sum_{j=1}^{n} c_j x_j              \\
		      & \sum_{j=1}^{n} \alpha _j x_j \leq 1 \\
		      & x\geq 0,
	\end{aligned}
\end{equation}
where
\[
	\alpha _j
	\coloneqq \frac{1}{\sum_{i=1}^{m} w_i} \sum_{i=1}^{m} w_i A_{ij} \geq 0
\]
since all entries are positive.

\begin{intuition}
	This is a fractional knapsack problem! We know that the optimum solution \(x^{\ast} \) is just to pick the coordinate \(j^{\ast} \coloneqq \argmax_{j \in [n]} c_j / \alpha _j\) and setting \(x^{\ast} _{j^{\ast} } \coloneqq 1 / \alpha _{j^{\ast} }\) and the rest to \(0\).
\end{intuition}

We summarize the \hyperref[algo:MWU-discrete-non-uniform]{MWU algorithm} for the case of positive \hyperref[def:packing-LP]{packing linear program} as follows.

\begin{algorithm}[H]\label{algo:MWU-discrete-linear}
	\DontPrintSemicolon{}
	\caption{Multiplicative Weight Update for Positive \hyperref[def:packing-LP]{Packing Linear Program}}
	\KwData{A cost vector \(c \in \mathbb{R} ^n_{+}\), constraint matrix-vector pair \(A \in \mathbb{R} ^{m \times n}_{+}\), \(b \in \mathbb{R} ^n_{+}\), error \(\epsilon \)}
	\KwResult{A solution \(x_{\text{out} }\)}
	\BlankLine

	\(\eta \gets \log m / \epsilon \)\;
	\;
	\For(){\(i = 1, \dots , m\)}{
		\(w_i(0) \gets 1 / b_i\)\;
	}
	\(x(0) = 0\)\Comment*[r]{\(x \in \mathbb{R} ^n\)}
	\(t = 0\)\;
	\;
	\While(){\(t < 1\)}{
		\(j^{\ast} \gets \argmax_{j \in [n]} c_j / (\sum_{i=1}^{m} A_{ij} w_i)\)\;
		\(\delta \gets \epsilon / (\eta \max _{i \in [m]} A_{ij^{\ast} })\)\Comment*[r]{Max step size}
		\(\delta \gets \min (\delta , 1 - t)\)\Comment*[r]{Handle the last iteration}
		\For(){\(i = 1, \dots , m\)}{
			\(w_i(t+\delta ) \gets w_i(t) \cdot e^{\eta \delta A_{ij^{\ast} }}\)\;
		}
		\(x (t+\delta ) \gets x (t) + \delta e_{j^{\ast} }\)\;
		\(t \gets t + \delta \)\;
	}
	\Return{\(x(1)\)}\;
\end{algorithm}

We note that an important aspect of \(j^{\ast} \) is that there is an optimum solution \(x^{\ast} \) to the \hyperref[eq:MWU-oracle-LP]{oracle} whose support is a \emph{single coordinate}.

\begin{prev}
	The \hyperref[def:width]{width}-independent analysis (\autoref{thm:MWU-discrete-non-uniform}) shows that the number of iterations depends only on the number of constraints \(m\).
\end{prev}

Thus, we can apply the method to \hyperref[def:implicit-LP]{implicit linear programs} with exponential (in some original problem size) number of variables but only a polynomial number of constraints as long as we can implement the \hyperref[eq:MWU-oracle-LP]{oracle} efficiently. Due to this structure, the sequence of \hyperref[eq:MWU-oracle-LP]{oracles} can often be maintained dynamically.

\begin{remark}[Maintaining the oracle]
	We see that at each time step \(t\), we solve for \(v(t) = x^{\ast} \), which is simply \(1 / \alpha _{j^{\ast} }\) at the \({j^{\ast} }^{\text{th} }\) coordinate and \(0\) otherwise. In the next round for time step \(t + \delta \), only those \(w_i\)'s such that \(g_i(v(t)) > 0\) needs to be updated, i.e., for those \(i\) such that \(\sum_{j=1}^{n} A_{ij} x^{\ast} _j \neq 0\). Since \(A\) is positive, this is equivalent to those \(i\) such that \(A_{i j^{\ast} } > 0\).
\end{remark}

Finally, another nice aspect of the \hyperref[algo:MWU-discrete-non-uniform]{MWU method} is that it accumulates a series of solutions of Lagrangian relaxations by taking non-negative sums. In the positive settings, this means that we do not use subtraction at all. This is convenient for approximation: Suppose we only have an \(\alpha \)-approximation \hyperref[eq:MWU-oracle-LP]{oracle} in the relative sense. Then, the process yields a \((1 - \epsilon )\alpha \)-approximation solution. This is convenient not only in obtaining approximate solutions, but also in speeding up \hyperref[algo:MWU-discrete-non-uniform]{MWU method}-based algorithms substantially by using various tricks and data structures that can avoid updating information at every step since we are allowed to use an approximate \hyperref[eq:MWU-oracle-LP]{oracle}.

\begin{note}
	If \(\alpha = (1 - \epsilon )\), then \((1 - \epsilon ) \alpha \geq 1 - 2\epsilon \).
\end{note}

\begin{remark}[Explicit packing linear program]
	From \autoref{thm:MWU-discrete-non-uniform}, the algorithm takes \(O(m \log m / \epsilon ^2)\) iterations. The \hyperref[eq:MWU-oracle-LP]{oracle} for each iteration can be done in \(O(N)\) time easily and naively. Thus, we can obtain a \((1 - \epsilon )\)-approximate solution in \(O(m N \log m / \epsilon ^2)\) time. By being slightly careful and using a simple approximate \hyperref[eq:MWU-oracle-LP]{oracle}, we can reduce the running time to \(O(N \log m / \epsilon ^2)\), which is near-linear.
\end{remark}