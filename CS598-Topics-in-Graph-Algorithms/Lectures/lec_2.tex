\lecture{2}{29 Aug.\ 11:00}{MST and Tree Packing}
\subsubsection{Linear-Time Randomized Algorithm}
Using randomization, it's possible to derive a linear-time algorithm for \hyperref[prb:MST]{MST}.

\begin{theorem}[\cite{karger1995randomized}]\label{thm:Karger-Klein-Tarjan}
	\hyperref[algo:Karger-Klein-Tarjan]{Karger-Klein-Tarjan's algorithm} takes \(O(m)\) time that computes the \hyperref[prb:MST]{MST} with probability at least \(1 - 1 / \poly(m)\).
\end{theorem}

\hyperref[algo:Karger-Klein-Tarjan]{Karger-Klein-Tarjan's algorithm} relies on the so-called \hyperref[lma:sampling]{sampling lemma}, which we first discussed.

\begin{lemma}[Sampling lemma]\label{lma:sampling}
	Given a graph \(G=(V, E)\), and let \(E^{\prime} \subseteq E\) be obtained by sampling each edge \(e\) with probability \(p \in (0, 1)\). Let \(F\) be a minimum spanning forest\footnote{As \(G^{\prime} \) can be disconnected.} in \(G^{\prime} = (V, E^{\prime}) \). Then the expected number of \hyperref[def:light]{\(F\)-light} edge in \(G\) is less than \((n-1) / p\).
\end{lemma}
\begin{proof}
	The proof is based on the \emph{principle of deferred decisions} in randomized analysis. Let \(A\) be the set of \hyperref[def:light]{\(F\)-light} edges. Note that both \(A\) and \(F\) are random sets that are generated by the process of sampling \(E^{\prime} \). To analyze \(\mathbb{E}_{}[\lvert A \rvert ] \), we consider \hyperref[algo:Kruskal]{Kruskal's algorithm} to obtain \(F\) from \(E^{\prime} \), where we generate \(E^{\prime} \) on the fly:

	\begin{algorithm}[H]
		\DontPrintSemicolon
		\caption{Sampling Process}
		\KwData{A connected graph \(G = (V, E)\) with edge weight \(c \colon E \to \mathbb{R} \), probability \(p \in (0, 1)\)}
		\KwResult{A minimum spanning forest \(F\) and the set of \hyperref[def:light]{\(F\)-light} edges \(A\)}
		\SetKwFunction{Ber}{Ber}
		\BlankLine

		Sort the edges such that \(c(e_1) \leq c(e_2) \leq \dots \leq c(e_m)\)\;
		\(A \gets \varnothing \), \(F \gets \varnothing \), \(E^{\prime} \gets \varnothing \)\;
		\For(){\(i = 1, \dots , m\)}{
			\(r \gets\)\Ber{\(p\)}\Comment*[r]{Toss a biased coin}
			\uIf(){\(r = 1\)}{
				\(E^{\prime} \gets E^{\prime} + e_i\)\;
				\If(){\(F + e_i\) is a forest}{
					\(F \gets F + e_i\)\;
					\(A \gets A + e_i\)\;
				}
			}
			\ElseIf(){\(e_i\) is \hyperref[def:light]{\(F\)-light}}{
				\(A \gets A + e_i\)\;
			}
		}
		\Return{\(F\), \(A\)}\;
	\end{algorithm}

	The following is exactly the same as the above, but easier to analyze:

	\begin{algorithm}[H]
		\DontPrintSemicolon
		\caption{Sampling Process with Tweaks}
		\KwData{A connected graph \(G = (V, E)\) with edge weight \(c \colon E \to \mathbb{R} \), probability \(p \in (0, 1)\)}
		\KwResult{A minimum spanning forest \(F\) and the set of \hyperref[def:light]{\(F\)-light} edges \(A\)}
		\SetKwFunction{Ber}{Ber}
		\BlankLine

		Sort the edges such that \(c(e_1) \leq c(e_2) \leq \dots \leq c(e_m)\)\;
		\(A \gets \varnothing \), \(F \gets \varnothing \)\;
		\For(){\(i = 1, \dots , m\)}{
			\If(\Comment*[f]{Sorting implies \(F + e_i\) is a forest \(\iff \) \(e_i\) is \hyperref[def:light]{\(F\)-light}}){\(e_i\) is \hyperref[def:light]{\(F\)-light}}{
				\(A \gets A + e_i\)\;
				\(r \gets\)\Ber{\(p\)}\Comment*[r]{Toss a biased coin}
				\If(){\(r = 1\)}{
					\(F \gets F + e_i\)\;
				}
			}
		}
		\Return{\(F\), \(A\)}\;
	\end{algorithm}

	The second algorithm makes the following observation clear.

	\begin{intuition}
		An edge \(e_i\) is added to \(A\) implies that it is added to \(F\) with probability \(p\).
	\end{intuition}

	Hence, \(p \mathbb{E}_{}[\lvert A \rvert ] = \mathbb{E}_{}[\lvert F \rvert ] \leq n - 1\), hence \(\mathbb{E}_{}[\lvert A \rvert ] \leq (n - 1) / p\).
\end{proof}

With the \hyperref[lma:sampling]{sampling lemma}, we know that when \(p = 1 / 2\), the number of \hyperref[def:light]{\(F\)-light} edges from \(E\) is at most \(2n\). Hence, we can eliminate most of the edges from \(E \setminus E^{\prime} \) from consideration given the fact that we can efficiently compute the \hyperref[def:heavy]{\(F\)-heavy} edges via the \hyperref[thm:MST-verification]{MST verification theorem}. It's worth noting that to work with the \hyperref[lma:sampling]{sampling lemma} via the natural recursion that it implies means that we need to work with potentially disconnected graph. That is, we will need to consider disconnected graph. Hence, we make the following generalization.

\begin{definition}[Spanning forest]\label{def:spanning-forest}
	A \emph{spanning forest} \(T\) of a graph \(G =(V, E)\) (potentially disconnected) is an induced subgraph of \(G\) which spans \(G\), i.e., \(V(T) = V\) and \(E(T) \subseteq E\).
\end{definition}

\begin{problem}[Minimum spanning forest]\label{prb:MSF}
Given a graph \(G=(V, E)\) (potentially disconnected) with edge weight \(c \colon E \to \mathbb{R} \), find the min-cost \hyperref[def:spanning-forest]{spanning forest}.
\end{problem}

\begin{note}
	\hyperref[prb:MST]{MST} and \hyperref[prb:MSF]{MSF} are closely related and one is reducible to the other in linear time, and the \hyperref[lma:cut-rule]{cut} and \hyperref[lma:cycle-rule]{cycle rules} can be generalized to \hyperref[prb:MSF]{MSF} easily.
\end{note}

Now, consider the following natural recursive divide and conquer algorithm for computing \hyperref[prb:MSF]{MSF}.

\begin{algorithm}[H]\label{algo:Karger-Klein-Tarjan*}
	\DontPrintSemicolon
	\caption{Natural Recursive Algorithm from \hyperref[lma:sampling]{Sampling Lemma}}
	\KwData{A graph \(G = (V, E)\) with edge weight \(c \colon E \to \mathbb{R} \)}
	\KwResult{A \hyperref[prb:MSF]{MSF} \(T = (V, F)\)}
	\SetKwFunction{std}{Standard-MST}
	\SetKwFunction{KKT}{\hyperref[algo:Karger-Klein-Tarjan]{Karger-Klein-Tarjan}}
	\SetKwFunction{Light}{Light-Edge}
	\BlankLine
	\If(\Comment*[f]{\(n_0\) is some constant}){\(\lvert V \rvert < n_0\)}{
		\Return{\std{\(G\), \(c\)}}\Comment*[r]{Use a standard deterministic algorithm}
	}
	\;
	Sample each edge i.i.d.\ from \(\mathrm{Ber} (1 / 2)\) to obtain \(E_1 \subseteq E\)\;
	\((V, F_1) \gets\)\KKT{\((V, E_1)\)}\label{algo:Karger-Klein-Tarjan*-recursive-1}\Comment*[r]{Recursively compute \hyperref[prb:MSF]{MSF}}
	\(E_2 \gets\)\Light{\(G\), \(F_1\)}\Comment*[r]{Compute all \hyperref[def:light]{\(F_1\)-light} edges with \autoref{thm:MST-verification}}
	\((V, F_2) \gets\)\KKT{\((V, E_2)\)}\label{algo:Karger-Klein-Tarjan*-recursive-2}\Comment*[r]{Recursively compute \hyperref[prb:MSF]{MSF}}
	\Return{\((V , F_2)\)}\;
\end{algorithm}

The correctness of \autoref{algo:Karger-Klein-Tarjan*} is clear from the \hyperref[lma:cut-rule]{cut} and \hyperref[lma:cycle-rule]{cycle rules}. The issue is the running time:

\begin{claim}
	\autoref{algo:Karger-Klein-Tarjan*} is not efficient enough.
\end{claim}
\begin{explanation}
	The expected number of edges in \(G_1 \coloneqq (V, E_1)\) is \(m / 2\), and the expected number of edges in \(G_2 \coloneqq (V, E_2)\), via the \hyperref[lma:sampling]{sampling lemma}, is at most \(2n\). We see that the algorithm does \(O(m + n)\) work outside the two recursive calls (\autoref{algo:Karger-Klein-Tarjan*-recursive-1}, \autoref{algo:Karger-Klein-Tarjan*-recursive-2}). Let \(T(m, n)\) be the expected running time of the algorithm on an \(m\)-edge \(n\)-node graph. Informally, we see the following recurrence:
	\[
		T(m, n)
		\leq c(m+n) + T(m / 2, n) + T(2n, n).
	\]
	If we take the problem size to be \(n + m\), then \autoref{algo:Karger-Klein-Tarjan*} generates two sub-problems of expected size \(m / 2 + n\) and \(2n + n\), with the total size being \(4n + m / 2\). If \(m > 10n\), say, then the total problem size is shrinking by a constant factor, and we obtain a linear-time algorithm. However, this is generally not the case.
\end{explanation}

The problem becomes reducing the graph size, which is the trick of \hyperref[algo:Karger-Klein-Tarjan]{Karger-Klein-Tarjan's algorithm}: we run \hyperref[algo:Boruvka]{Borůvka's algorithm} for a few iterations as a preprocessing step, reducing the number of vertices:

\begin{algorithm}[H]\label{algo:Karger-Klein-Tarjan}
	\DontPrintSemicolon
	\caption{Karger-Klein-Tarjan's Algorithm~\cite{karger1995randomized}}
	\KwData{A connected graph \(G = (V, E)\)\footnote{Assume no connected component of \(G\) is small.} with edge weight \(c \colon E \to \mathbb{R} \)}
	\KwResult{A \hyperref[prb:MSF]{MSF} \(T = (V, F)\)}
	\SetKwFunction{std}{Standard-MST}
	\SetKwFunction{KKT}{\hyperref[algo:Karger-Klein-Tarjan]{Karger-Klein-Tarjan}}
	\SetKwFunction{Light}{Light-Edge}
	\SetKwFunction{Boruvka}{\hyperref[algo:Boruvka]{Borůvka}}
	\BlankLine
	\If(\Comment*[f]{\(n_0\) is some large constant}){\(\lvert V \rvert < n_0\)}{
		\Return{\std{\(G\), \(c\)}}\Comment*[r]{Use a standard deterministic algorithm}
	}
	\;
	\(G^{\prime} = (V^{\prime} , E^{\prime} ), T^{\prime} = (V^{\prime} , F^{\prime} )\gets\)\Boruvka{\(G\), \(c\), \(2\)}\label{algo:Karger-Klein-Tarjan-Boruvka}\Comment*[r]{Run two iterations with \(\lvert V^{\prime} \rvert \leq \lvert V \rvert / 4\).}
	\;
	Sample each edge in \(G^{\prime} \) i.i.d.\ from \(\mathrm{Ber} (1 / 2)\) to obtain \(E_1 \subseteq E^{\prime} \)\;
	\((V^{\prime} , F_1) \gets\)\KKT{\((V^{\prime} , E_1)\)}\Comment*[r]{Recursively compute \hyperref[prb:MSF]{MSF}}
	\(E_2 \gets\)\Light{\(G_1\), \(F_1\)}\Comment*[r]{Compute \hyperref[def:light]{\(F_2\)-light} edges with \autoref{thm:MST-verification}}
	\((V^{\prime} , F_2) \gets\)\KKT{\((V^{\prime} , E_2)\)}\Comment*[r]{Recursively compute \hyperref[prb:MSF]{MSF}}
	\Return{\((V , F^{\prime} \cup F_2)\)}\;
\end{algorithm}

Now, we provide the proof sketch of \autoref{thm:Karger-Klein-Tarjan}, which can be made precise with expectation.

\begin{proof}[Proof Sketch of \autoref{thm:Karger-Klein-Tarjan}]
	The correctness is easy to see as before. As for the running time, we see that \hyperref[algo:Boruvka]{Borůvka's algorithm} takes \(O(m)\) time for each phase, so the total time for the preprocessing (\autoref{algo:Karger-Klein-Tarjan-Boruvka}) is \(O(m)\). Then, the recurrence for \(T(m, n)\) is
	\[
		T(m, n)
		\leq c(m+n) + T(m / 2, n / 4) + T(2n / 4 + n / 4),
	\]
	i.e., the resulting sub-problem is of size \(n / 4 + m / 2 + n / 4 + n / 2 = n + m / 2\), which is good enough assuming \(m \geq n - 1\).\footnote{Since we eliminate small components including singletons.} By a simple inductive proof, we can show that \(T(m, n) = O(n + m) \).
\end{proof}

\begin{remark}
	A more refined analysis of the \hyperref[lma:sampling]{sampling lemma} can be used to show that the running time is linear with high probability as well.
\end{remark}

Many properties of forests and spanning trees can be understood in the more general context of \emph{matroids}. In many cases this perspective is insightful and also useful. The \hyperref[lma:sampling]{sampling lemma} applies in this more general context and has various applications~\cite{karger1995random,karger1998random}. Obtaining a deterministic \(O(m)\) time algorithm is a major open problem. Obtaining a simpler linear-time \hyperref[thm:MST-verification]{MST verification} algorithm, even randomized, is also a very interesting open problem.

\section{Tree Packing}
We turn to another interesting problem, \hyperref[prb:TP]{tree packing}.

\begin{problem}[Tree packing]\label{prb:TP}
Given a multigraph \(G = (V, E)\), find all the edge-disjoint \hyperref[def:spanning-tree]{spanning trees} in \(G\). In particular, find the maximum number, \(\tau (G)\), of edge-disjoint \hyperref[def:spanning-tree]{spanning trees} of \(G\)
\end{problem}

\subsection{Bound on the Tree Packing Number}
There is a beautiful theorem that provides a min-max formula for this. We first introduce some notation.

\begin{notation}
	Let \(\mathcal{P} \) be the collection of partitions of \(V\), and \(E_P\) is the edge between connected components induced by a partition \(P \in \mathcal{P} \), i.e., \(e \in E_P\) if its endpoints are in different parts of \(P\).
\end{notation}

It's easy to see that any \hyperref[def:spanning-tree]{spanning tree} must contain at least \(\lvert P \rvert - 1\) edges from \(E_P\). Thus, if \(G\) has \(k\) edge-disjoint \hyperref[def:spanning-tree]{spanning trees}, then
\[
	k \leq \frac{\lvert E_P \rvert }{\lvert P \rvert - 1}.
\]
More generally, we have the following.

\begin{theorem}\label{thm:Tutte-Nash-Williams}
	The maximum number of edge-disjoint \hyperref[def:spanning-tree]{spanning trees} in a graph \(G\) is given by
	\[
		\tau (G)
		= \left\lfloor \min _{P \in \mathcal{P} } \frac{\lvert E_P \rvert }{\lvert P \rvert - 1} \right\rfloor.
	\]
\end{theorem}

\begin{remark}
	\autoref{thm:Tutte-Nash-Williams} is a special case of a theorem on matroid base packing where it is perhaps more natural to see~\cite{schrijver2003combinatorial}.
\end{remark}

A weaker version of the theorem is regarding fractional packing. In fractional packing, we allow one to use a fraction amount of a tree. The total amount to which an edge can be used is at most \(1\) (or \(c(e)\) in the capacitated case). Clearly, an integer packing is also a fractional packing. The advantage of fractional packings is that one can write a linear program for it, and they often have some nice properties. Let \(\tau _{\text{frac} } (G)\) be the fraction \hyperref[prb:TP]{tree packing} number. Clearly, we have \(\tau _{\text{frac} }(G) \geq \tau (G)\).

\begin{corollary}\label{col:Tutte-Nash-Williams}
	Given a graph \(G\), we have
	\[
		\tau _{\text{frac} } (G)
		= \min _{P \in \mathcal{P} } \frac{\lvert E_P \rvert }{\lvert P \rvert - 1}.
	\]
\end{corollary}

A second important corollary that is frequently used is about the min-cut. We see that while the min-cut size \(\lambda (G)\) of \(G\) is upper-bounding \(\tau (G)\), i.e., \(\tau (G) \leq \lambda (G)\), this is not tight at all.

\begin{corollary}\label{col:Tutte-Nash-Williams-min-cut}
	Let \(G\) be a capacitated graph and let \(\lambda (G)\) be the global min-cut size. Then
	\[
		\tau _{\text{frac} }(G)
		\geq \frac{\lambda (G)}{2} \frac{n}{n - 1}.
	\]
\end{corollary}
\begin{proof}
	Let \(P^{\ast} \) be the optimum partition that induces \(\tau _{\text{frac} }(G)\). Then, \(\tau (G) = \lvert E_{P^{\ast} } \rvert / (\lvert P^{\ast} \rvert - 1)\). Since for every connected component induced by \(P^{\ast} \), at least \(\lambda (G)\) edges are going out, hence
	\[
		\tau _{\text{frac} }(G)
		= \frac{\lvert E_{P^{\ast} } \rvert }{\lvert P^{\ast}  \rvert - 1}
		\geq \frac{\lambda (G) / 2 \cdot \lvert P^{\ast} \rvert }{\lvert P^{\ast} \rvert - 1}
		\geq \frac{\lambda (G)}{2} \frac{n}{n-1},
	\]
	where we use the fact that \(\lvert P^{\ast} \rvert \leq n\) and \(i / (i - 1)\) is decreasing.
\end{proof}

We first see a tight example.

\begin{eg}[Cycle]
	Consider the \(n\)-node cycle \(C_n\). Clearly, \(\tau (C_n) = 1\), and \(\tau _{\text{frac} }(C_n) \leq n / (n-1)\) since each tree has \(n-1\) edges and there are \(n\) edges in the graph. Indeed, we have \(\tau _{\text{frac} } (C_n) = n / (n-1)\). Finally, we see that \(\lambda (G) = 2\).
\end{eg}
\begin{explanation}
	Consider the \(n\) trees in \(C_n\) (corresponding to deleting each of the \(n\) edge) and assigning a fraction value of \(1 / (n-1)\) for each of them, with the corresponding tight partition consists of the \(n\) singleton vertices.
\end{explanation}

\begin{note}
	\autoref{thm:Tutte-Nash-Williams} and its corollaries naturally extend to the capacitated case. For integer packing, we can assume \(c_e\) is an integer for each edge \(e\), and the formula is changed to
	\[
		\tau (G)
		= \left\lfloor \min _{P \in \mathcal{P} } \frac{c(E_P)}{\lvert P \rvert - 1} \right\rfloor .
	\]
\end{note}

Typically, one uses the connection between \hyperref[prb:TP]{tree packing} and min-cut to argue about the existence of many disjoint \hyperref[def:spanning-tree]{trees}, since the global minimum cut is easier to understand than \(\tau (G)\). However, we will see that one can use \hyperref[prb:TP]{tree packing} to compute \(\lambda (G)\) exactly which may seem surprising at first due to the approximate relationship \autoref{col:Tutte-Nash-Williams-min-cut}.

\subsection{Proof of \autoref{thm:Tutte-Nash-Williams}}
First, we prove the fractional version of \autoref{thm:Tutte-Nash-Williams} (i.e., \autoref{col:Tutte-Nash-Williams}) via LP duality.

\begin{proof}[Proof of \autoref{col:Tutte-Nash-Williams}~\cite{chekuri2017near}]\label{pf:col:Tutte-Nash-Williams}
	Consider \(\mathcal{T} _G \coloneqq \{ T \mid T \text{ is a \hyperref[def:spanning-tree]{spanning tree} of } G \} \). Then, consider the following primal and the dual linear program:
	\[
		\begin{aligned}
			\max~           & \sum_{T \in \mathcal{T} _G} y_T                                 \\
			                & \sum_{T \ni e} y_T \leq c(e)    & \forall e \in E;              \\
			\text{(P)}\quad & y_T \geq 0                      & \forall T \in \mathcal{T} _G;
		\end{aligned}\qquad
		\begin{aligned}
			\min~           & \sum_{e \in E} c(e) x_e                                   \\
			                & \sum_{e \in T} x_e \geq 1 & \forall T \in \mathcal{T} _G; \\
			\text{(D)}\quad & x_e \geq 0                & \forall e \in E.
		\end{aligned}
	\]
	Let \(y^{\ast} \) and \(x^{\ast} \) be the optimal solution to the primal and the dual. Then from the strong duality,
	\[
		\sum_{T \in \mathcal{T} _G} y^{\ast} _T
		= \tau _{\text{frac} }(G)
		= \sum_{e \in E} c(e) x^{\ast} _e.
	\]
	We see that if there exists \(e\) such that \(x^{\ast} _e = 0\), then we can just contract all these edges, so without loss of generality, \(x^{\ast} _e > 0\) for all \(e \in E\).

	\begin{intuition}
		If \(x^{\ast} _e = 0\), we can effectively increase \(c(e)\) to \(\infty \) without affecting the value of the dual solution, i.e., \(e\) is not a bottleneck in the primal \hyperref[prb:TP]{tree packing}, hence safe to contract.
	\end{intuition}

	\begin{claim}
		If \(x^{\ast} _e > 0\) for all \(e \in E\), then \(\tau _{\text{frac} }(G)\) is achieved via the singleton partition \(P\). In particular,
		\[
			\tau _{\text{frac} }(G)
			= \frac{\sum_{e \in E} c(e)}{n - 1}.
		\]
	\end{claim}
	\begin{explanation}
		From complementary slackness, we know that \(\sum_{T \ni e} y^{\ast} _T = c(e)\) for all \(e\in E\). Hence,
		\[
			(n-1) \sum_{T \in \mathcal{T} _G} y^{\ast} _T
			= \sum_{T \in \mathcal{T} _G} \sum_{e \in T} y^{\ast} _T
			= \sum_{e \in E} \sum_{T \ni e} y^{\ast} _T
			= \sum_{e \in E} c(e),
		\]
		implying that \(\sum_{T \in \mathcal{T} _G} y^{\ast} _T = \sum_{e\in E} c(e) / (n-1)\).
	\end{explanation}
	The above claim gives us the desired conclusion via induction: this is true if \(x^{\ast} _e >0\) for all \(e\in E\); otherwise, we contract edges with \(x^{\ast} _e = 0\) and reduce to this case.
\end{proof}

\begin{remark}
	In the above proof, the dual can be interpreted as a relaxation for the min-cut problem. In fact, if \(x_e \in \{ 0, 1 \} \), then this is exact.
\end{remark}

\subsection{Finding an Optimum Tree Packing and Approximating Tree Packing}
If the linear program in the \hyperref[pf:col:Tutte-Nash-Williams]{proof} of \autoref{col:Tutte-Nash-Williams} can be solved efficient to get \(\tau _{\text{frac} }(G)\), then it will also yield an algorithm for the value of the integer packing \(\tau (G)\) since it's just the floor of which. The problem is that while the primal has an exponentially many variables, the dual has an exponentially many constraints. We recall the following fact.

\begin{prev}
	The Ellipsoid method needs a \emph{separation oracle}. For example, applying it to the dual, we need to answer the following question efficiently:
	\begin{itemize}
		\item Given \(x \in \mathbb{R} ^E\), is it the case that \(\sum_{e \in T} x_e \geq 1\) for all \(T \in \mathcal{T} _G\)?
		\item If not, find a tree \(T\) such that \(\sum_{e \in T} x_e < 1\).
	\end{itemize}
\end{prev}

We see that this corresponds to solving \hyperref[prb:MST]{MST}, hence, the dual admits an efficient solution via the Ellipsoid method. One can convert an exact algorithm for the dual to an exact algorithm for the primal.

\begin{remark}
	There are combinatorial algorithms for solving \hyperref[prb:TP]{tree packing} (both integer version and fraction versions) in strongly polynomial time~\cite{schrijver2003combinatorial}.
\end{remark}

On the other hand, we're also interested in whether we can find a faster algorithm for \hyperref[prb:TP]{tree packing} if one allows approximation with an adaption of the \emph{multiplicative weights update} (MWU) method and data structures for \hyperref[prb:MST]{MST} maintenance. We may see it later in the course.

\begin{theorem}\label{thm:approximate-TP}
	There isa deterministic algorithm to compute a \((1 - \epsilon )\)-approximate fractional \hyperref[prb:TP]{tree packing} in \(O(m \log ^3 n / \epsilon ^2)\).
\end{theorem}