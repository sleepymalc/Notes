\lecture{21}{7 Nov.\ 11:00}{Finishing Cut-Matching Game}
Specifically, in iteration \(i\), we have vectors \(x^{(i)}_u\) for \(u \in V\), where each \(x_u \in \mathbb{R} ^n\) and \(\sum_{v \in V} x_u^{(i)}(v) = 0\). We can then reduce the problem to the one dimensional case by using the well-known idea of random projection that maps points in high dimensions to a line. Recall the following simple facts:

\begin{prev}
	If \(Z_i \overset{\text{i.i.d.} }{\sim } \mathcal{N} (\mu _i, \sigma _i^2)\) for \(i = 1, 2\), then \(Z_i + Z_2 \sim \mathcal{N} (\mu _1 + \mu _2, \sigma _1^2 + \sigma _2^2)\).
\end{prev}

Furthermore, we also have the following.

\begin{lemma}\label{lma:Gaussian-random-projection}
	Let \(g \sim \mathcal{N} (0, I_n)\) and let \(x \in \mathbb{R} ^n\) and \(z \coloneqq \langle g, x \rangle \). Then:
	\begin{enumerate}[(i)]
		\item \(\mathbb{E}_{}[z] = 0\);
		\item \(\mathbb{E}_{}[z^2] = \sum_{i=1}^{k} x_i^2 = \lVert x \rVert _2^2\);
		\item for any \(t\), \(\Pr_{}(z^2 \geq t \lVert x \rVert _2^2) \leq e^{-t / 2}\).
	\end{enumerate}
\end{lemma}
\begin{proof}
	The first two are standard properties. We just make some additional remark on the third concentration bound for \(\chi ^2\). Suppose \(Z \sim \mathcal{N} (0, \sigma ^2)\). Then, it's well known that we have \(\Pr_{}(Z > \beta ) \leq e^{- \beta / 2 \sigma ^2}\). Now, \(Z^2\), the square of the Gaussian random variable, is a positive random variable following the so-called \(\chi ^2\) distribution. Note that its mean is \(\sigma ^2\). It also exhibits strong concentration around its mean: specifically, when \(Z \sim \mathcal{N} (0, 1)\), we have \(\Pr_{}(Z^2 \geq 1 + 4t) \geq e^{-t}\). Using this, the third concentration bound can be easily proved.
\end{proof}

Hence, in iteration \(i\), a natural strategy for the cut player is to project \(x_u^{(i-1)}\) to one dimension via \(z_u^{(i)} = \langle g, x_u^{(i-1)} \rangle \) and partition \(V\) into \((S_i, V\setminus S_i)\) by sorting \(z_u^{(i)}\) and select the first half to be \(S_i\):

\begin{center}
	\incfig{cut-matching-linear-projection}
\end{center}

This leads to the following guarantee.

\begin{lemma}\label{lma:cut-matching-linear-projection}
	Let \(M_i\) be any perfect matching between \(S_i\) and \(V\setminus S_i\) generated by the cut player with the linear projection strategy based on \(x_u^{(i-1)}\) for \(u \in V\). Then
	\[
		\mathbb{E}_{}\left[\sum_{uv \in M_i} (z_u - z_v)^2 \right]
		\geq \phi _{i-1},
	\]
	where \(z_u \coloneqq \langle g, x_u^{(i-1)} \rangle \) for all \(u \in V\), where \(g \sim \mathcal{N} (0, I_n)\).
\end{lemma}
\begin{proof}
	From the proof of \autoref{lma:cut-matching-potential-reduction-1-dimension}, we have\footnote{Since \(\sum_{u \in V} z_u\) is not necessarily \(0\) as \(z_u\) is random.}
	\[
		\sum_{uv \in M_i} (z_u - z_v)^2
		\geq \sum_{u \in V} z_u^2 - 2 \beta \sum_{u \in V} z_u,
	\]
	where \(\beta \) is the median of \(z_u\) for \(u \in V\). We see that by taking expectation on both sides, with \autoref{lma:Gaussian-random-projection}, we have
	\[
		\mathbb{E}_{}\left[\sum_{uv \in M_i} (z_u - z_v)^2 \right]
		\geq \mathbb{E}_{}\left[\sum_{u \in V} z_u^2 \right] - 2\beta \mathbb{E}_{}\left[\sum_{u \in V} z_u \right]
		= \sum_{u \in V} \mathbb{E}_{}\left[z_u^2 \right]
		= \sum_{u \in V} \lVert x_u^{(i-1)} \rVert _2^2
		= \phi _{i-1},
	\]
	where we use the fact that the sum of mean zero Gaussian's is still a mean zero Gaussian.
\end{proof}

With \autoref{lma:cut-matching-linear-projection}, we can then combine \autoref{lma:cut-matching-potential-reduction-1-dimension} and \autoref{lma:cut-matching-deviation} to prove the following:

\begin{lemma}\label{lma:cut-matching-potential-reduction}
	Given current flow vectors \(b_u^{(i-1)}\) and difference vectors \(x_u^{(i-1)}\) for all \(u \in V\), there is a randomized algorithm that produces a partition \((S_i, V\setminus S_i)\) such that for any perfect matching \(M_i\) between \(S_i\) and \(V\setminus S_i\), the potential \(\phi _{i}\) after mixing according to \(M_i\) satisfies
	\[
		\mathbb{E}_{}[\phi _i]
		\leq \left( 1 - \frac{1}{c \log n} \right) \phi _{i-1} + O(n^{-10}),
	\]
	where \(c\) is some sufficiently large fixed constant.
\end{lemma}
\begin{proof}
	Consider the linear projection strategy described above. Since \((z_u - z_v)^2 = \langle g, x_u^{(i-1)} - x_v^{(i-1)} \rangle ^2\), then by \autoref{lma:Gaussian-random-projection}, with high probability,
	\begin{equation}\label{eq:lma:cut-matching-potential-reduction}
		(z_u - z_v)^2
		= \langle g, x_u^{(i-1)} - x_v^{(i-1)} \rangle ^2
		\leq c \log n \cdot \lVert x_u^{(i-1)} - x_v^{(i-1)} \rVert _2^2.
	\end{equation}
	Assuming this happens deterministically. Then, from \autoref{lma:cut-matching-linear-projection} and \autoref{eq:cut-matching-deviation},
	\[
		\mathbb{E}_{}[\phi _{i-1} - \phi _i]
		= \frac{1}{2} \mathbb{E}_{}\left[\sum_{uv \in M_i} \lVert x_u^{(i-1)} - x_v^{(i-1)} \rVert _2^2 \right]
		\geq \frac{1}{c \log n} \mathbb{E}_{}\left[\sum_{uv \in M_i} (z_u - z_v)^2 \right]
		\geq \frac{1}{c \log n} \phi _{i-1},
	\]
	which implies \(\mathbb{E}_{}[\phi _i] \leq (1 - 1 / c \log n) \phi _{i-1}\). To handle the case that \autoref{eq:lma:cut-matching-potential-reduction} doesn't happen deterministically, we need to do a little of extra work, which is why we get the weaker bound with a slight additive bound. We omit the exact detail for simplicity.
\end{proof}

\begin{intuition}
	\autoref{lma:cut-matching-potential-reduction} states that using random projection, we only loses a \(1 / \log n\) factor compared to the true one-dimensional case.
\end{intuition}

\begin{remark}
	\autoref{lma:cut-matching-potential-reduction} implies that the algorithm uses only the information about the flow routed so far and nothing about the previous matchings.
\end{remark}

\autoref{lma:cut-matching-potential-reduction} immediately imply \autoref{thm:cut-matching-succeed}:

\begin{proof}[Proof of \autoref{thm:cut-matching-succeed}]
	From the previous \hyperref[clm:cut-matching-goal]{claim}, we know that we want to reduce \(\phi _0 \leq n\) to \(1 / 4n^2\). As \(\phi _i \leq (1 - 1 / c \log n) \phi _{i-1}\) for some constant \(c>0\) from \autoref{lma:cut-matching-potential-reduction}, we have
	\[
		\phi _i \leq \left( 1 - \frac{1}{c \log n} \right) ^i \phi _0 \leq \left( 1 - \frac{1}{c \log n} \right) ^i n.
	\]
	Setting the right-hand side to be less than \(1 / 4n^2\) and solve for \(i\), we have \(i = O(\log ^2 n)\).
\end{proof}

This completes the whole proof. Finally, we describe the whole \hyperref[def:cut-matching-game]{cut-matching game} in \autoref{algo:cut-matching-game} for completeness.

\begin{algorithm}[H]\label{algo:cut-matching-game}
	\DontPrintSemicolon{}
	\caption{\hyperref[def:cut-matching-game]{Cut-Matching Game}}
	\KwData{A graph \(G = (V, E)\) with edge capacity \(c \colon E \to \mathbb{R} _+ \), iteration \(k\)}
	\KwResult{A certification graph \(H = (V, \bigcup_{i=1}^{k} M_i)\)}
	\SetKwFunction{Flow}{Single-Commodity-Flow}
	\SetKwFunction{Normal}{Normal}
	\SetKwFunction{Median}{Median}
	\SetKwFunction{CutPlayer}{Cut-Player}
	\SetKwFunction{MatchingPlayer}{Matching-Player}
	\SetKwFunction{Build}{Build-Linkage-Routing-Graph}
	\BlankLine

	\For(\Comment*[f]{Initialization}){\(u \in V\)}{
	\(x_u^{(0)} \gets \mathbbm{1}_{u = v} - \vec{1} / n\)\;
	}
	\;
	\While(){\(i = 1, \dots , k\)}{
	\(S_i\gets\)\CutPlayer{\(\{ x_u^{(i-1)} \} _{u \in V}\)}\;
	\(M_i \gets\)\MatchingPlayer{\(G\), \(S_i\)}\;
	\For(){\(uv \in M_i\)}{
	\(x_u^{(i)} \gets (x_u^{(i-1)} + x_v^{(i-1)}) / 2\)\;
	\(x_v^{(i)} \gets (x_u^{(i-1)} + x_v^{(i-1)}) / 2\)\;
	}
	}
	\Return{\((V, \bigcup_{i=1}^{k} M_i)\)}\;
	\;
	\SetKwProg{Fn}{}{:}{}
	\Fn{\CutPlayer{\(\{ x_u \} _{u \in V}\)}}{
	\(g \gets\)\Normal{\(0\), \(I_n\)}\Comment*[r]{\(g \sim \mathcal{N} (0, I_n)\)}
	\For(){\(u \in V\)}{
		\(z_u \gets \langle g, x_u \rangle \)\;
	}
	\(\beta \gets\)\Median{\(\{ z_u \} _{u \in V}\)}\;
	\(S \gets \{ u \in V \colon z_u \leq \beta \}\)\Comment*[r]{Break ties arbitrarily such that \(\lvert S \rvert = n / 2\)}
	\Return{\(S\)}\;
	}
	\;
	\Fn{\MatchingPlayer{\(G\), \(S\)}}{
		\((G^{\prime} , s^{\ast} , t^{\ast} )\gets\)\Build{\(G\), \(S\), \(V\setminus S\)}\Comment*[r]{\autoref{fig:expansion-matching}}
		\(M \gets\)\Flow{\(G^{\prime} \), \(s^{\ast}\), \(t^{\ast} \)}\Comment*[r]{Compute \hyperref[def:routable]{routable} perfect matching}
		\Return{\(M\)}\;
	}
\end{algorithm}

\begin{note}
	Explicitly, we run \autoref{algo:cut-matching-game} with \(k = O(\log ^2 n)\), and whenever \(M\) is not a valid perfect matching or is not \hyperref[def:routable]{routable} (indicating that \(S\) and \(V\setminus S\) is not \hyperref[def:well-linked]{well-linked}), we just report that \(G\) is not a \hyperref[def:expander]{\(1\)-expander}.
\end{note}

\section{Application to Treewidth}
\todo{Cleanup}
Previously, we have mentioned \emph{\hyperref[def:treewidth]{treewidth}} briefly (e.g., \autoref{thm:node-well-linked-treewidth}). Now, we see another good application of the \hyperref[def:cut-matching-game]{cut-matching game} to \hyperref[def:treewidth]{treewidth}. We care about how ``tree-like'' does the graph look. One of the (among many) interesting properties of a tree is what can we separate when removing one node:

\begin{eg}[Tree]
	For a tree \(T\), there exists a single vertex such that after removing it, no connect component has more than \(2 n/ 3\) vertices.
\end{eg}

What if we allow removing more vertices?

\begin{eg}[Series-parallel graph]
	\href{https://en.wikipedia.org/wiki/Series%E2%80%93parallel_graph}{Series-parallel graph} has the same property when allowing removing \(2\) vertices.
\end{eg}

Taking these two examples into account, we consider the following notion of \hyperref[def:tree-decomposition]{tree decomposition}.

\begin{definition}[Tree decomposition]\label{def:tree-decomposition}
	Given a graph \(G = (V, E)\), a \emph{tree decomposition} of \(G\) is a tree \(T = (V_T, E_T)\) and a function \(B \colon V_T \to 2^V\) such that
	\begin{enumerate}[(i)]
		\item for all \(u \in V\), the set of ``bags'' that contains \(u \in T\) form a connected sub-tree of \(T\);
		\item for all \(uv \in E\), there exists some \(x \in V_T\) such that both \(u, v \in B(x)\).
	\end{enumerate}
	We let the \emph{width} of the tree decomposition to be the maximum size of the bag.
\end{definition}

\begin{definition}[Treewidth]\label{def:treewidth}
	Given a graph \(G\), the \emph{treewidth} \(\operatorname{tw}(G) \) is the minimum width among all possible \hyperref[def:tree-decomposition]{tree decomposition} minus \(1\).\footnote{This is because the minimum maximum width of the \hyperref[def:tree-decomposition]{tree decomposition} for a tree is \(2\).}
\end{definition}

Essentially, \autoref{def:treewidth} captures the desired properties we mentioned above:

\begin{claim}
	Suppose \(\operatorname{tw}(G) \leq k\). Then for any \(X \subseteq V\), there exists a set \(S\) such that \(\lvert S \rvert \leq k\) and \(G-S\) has no component with more than \(2 \lvert X \rvert / 3\) vertices from \(X\).
\end{claim}

Let us rephrase \autoref{thm:node-well-linked-treewidth} as follows:

\begin{lemma}\label{lma:node-well-linked-treewidth}
	For any graph \(G\), \(\operatorname{tw}(G) \geq k\) if and only if there exists \(X\) such that \(\lvert X \rvert \geq k / 4\) such that \(X\) is \hyperref[def:node-well-linked]{node-well-linked}.
\end{lemma}

Now, we try to verify the \hyperref[def:treewidth]{treewidth}. Verifying a small \hyperref[def:treewidth]{treewidth} is easy, we just ask for a \hyperref[def:tree-decomposition]{tree decomposition} as a certification.

\begin{theorem}
	Let \(G = (V, E)\) be a graph with \hyperref[def:treewidth]{treewidth} \(\geq k\). Then three exists a subgraph \(H = (V, E_H)\) such that
	\begin{enumerate}[(i)]
		\item \(\operatorname{tw}(H) \geq k / c \log ^2 k\);
		\item \(\deg_H(v) = O(\log ^2 k)\).
	\end{enumerate}
\end{theorem}

\begin{theorem}
	Let \(G = (V, E)\) be a graph with \hyperref[def:treewidth]{treewidth} \(\geq k\). Then three exists a subgraph \(H = (V, E_H)\) such that
	\begin{enumerate}[(i)]
		\item \(\operatorname{tw}(H) \geq k / c \log ^{100} k\);
		\item \(\deg_H(v) = 3\).
	\end{enumerate}
\end{theorem}

\begin{remark}[Planar Graph]
	For planar graph, we know that the grid has a \hyperref[def:treewidth]{treewidth} \(\sqrt{n} \). From the planar graph separator theorem, we also know that the \hyperref[def:treewidth]{treewidth} for planar graphs is \(O(\sqrt{n} )\). Hence, we can solve many problems in \(2^{\sqrt{n} }\) using dynamic programming for planar graphs.
\end{remark}

\begin{theorem}\label{thm:treewidth-planar-graph-color}
	For any planar graph \(G = (V, E)\) and integer \(h \geq 3\), \(V\) can be colored with \(h\) colors such that for any \(i \in [h]\), \(\operatorname{tr}(G - V_i) \leq 3h\) where \(V_i \subseteq V\) is the set of vertices colored with color \(i\).
\end{theorem}

The above immediately yield several fast approximation algorithm.

\begin{eg}[Maximum weighted independent set]
	Consider the \emph{maximum weighted independent set} (MWIS) problem on planar graphs. Let \(\epsilon \in (0, 1)\) and \(h = 1 / \epsilon \). From \autoref{thm:treewidth-planar-graph-color}, consider removing \(V_i\) from \(G\) and compute the corresponding MWIS to get \(\OPT_i\) in \(G - V_i\). From the simple pigeonhole principle, the optimum value of MWIS \(\max _{i\in [h]} \OPT_i\) is \((1-\epsilon)\)-approximation to \(\OPT\). Finally, we simply note that since \(\operatorname{tr}(G-V_i) \leq 3h\), computing \(\OPT_i\) takes only \(2^{O(1 / \epsilon )}\), hence in total.
\end{eg}