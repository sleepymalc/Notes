\lecture{21}{7 Nov.\ 11:00}{Finishing Cut-Matching Game}
Specifically, in iteration \(i\), we have vectors \(x^{(i)}_u\) for \(u \in V\), where each \(x_u \in \mathbb{R} ^n\) and \(\sum_{v \in V} x_u^{(i)}(v) = 0\). We can then reduce the problem to the one dimensional case by using the well-known idea of random projection that maps points in high dimensions to a line. Recall the following simple facts:

\begin{prev}
	If \(Z_i \overset{\text{i.i.d.} }{\sim } \mathcal{N} (\mu _i, \sigma _i^2)\) for \(i = 1, 2\), then \(Z_i + Z_2 \sim \mathcal{N} (\mu _1 + \mu _2, \sigma _1^2 + \sigma _2^2)\).
\end{prev}

Furthermore, we also have the following.

\begin{lemma}\label{lma:Gaussian-random-projection}
	Let \(g \sim \mathcal{N} (0, I_n)\) and let \(x \in \mathbb{R} ^n\) and \(z \coloneqq \langle g, x \rangle \). Then:
	\begin{enumerate}[(i)]
		\item \(\mathbb{E}_{}[z] = 0\);
		\item \(\mathbb{E}_{}[z^2] = \sum_{i=1}^{k} x_i^2 = \lVert x \rVert _2^2\);
		\item for any \(t\), \(\Pr_{}(z^2 \geq t \lVert x \rVert _2^2) \leq e^{-t / 2}\).
	\end{enumerate}
\end{lemma}
\begin{proof}
	The first two are standard properties. We just make some additional remark on the third concentration bound for \(\chi ^2\). Suppose \(Z \sim \mathcal{N} (0, \sigma ^2)\). Then, it's well known that we have \(\Pr_{}(Z > \beta ) \leq e^{- \beta / 2 \sigma ^2}\). Now, \(Z^2\), the square of the Gaussian random variable, is a positive random variable following the so-called \(\chi ^2\) distribution. Note that its mean is \(\sigma ^2\). It also exhibits strong concentration around its mean: specifically, when \(Z \sim \mathcal{N} (0, 1)\), we have \(\Pr_{}(Z^2 \geq 1 + 4t) \geq e^{-t}\). Using this, the third concentration bound can be easily proved.
\end{proof}

Hence, in iteration \(i\), a natural strategy for the cut player is to project \(x_u^{(i-1)}\) to one dimension via \(z_u^{(i)} = \langle g, x_u^{(i-1)} \rangle \) and partition \(V\) into \((S_i, V\setminus S_i)\) by sorting \(z_u^{(i)}\) and select the first half to be \(S_i\):

\begin{center}
	\incfig{cut-matching-linear-projection}
\end{center}

This leads to the following guarantee.

\begin{lemma}\label{lma:cut-matching-linear-projection}
	Let \(M_i\) be any perfect matching between \(S_i\) and \(V\setminus S_i\) generated by the cut player with the linear projection strategy based on \(x_u^{(i-1)}\) for \(u \in V\). Then
	\[
		\mathbb{E}_{}\left[\sum_{uv \in M_i} (z_u - z_v)^2 \right]
		\geq \phi _{i-1},
	\]
	where \(z_u \coloneqq \langle g, x_u^{(i-1)} \rangle \) for all \(u \in V\), where \(g \sim \mathcal{N} (0, I_n)\).
\end{lemma}
\begin{proof}
	From the proof of \autoref{lma:cut-matching-potential-reduction-1-dimension}, we have\footnote{Since \(\sum_{u \in V} z_u\) is not necessarily \(0\) as \(z_u\) is random.}
	\[
		\sum_{uv \in M_i} (z_u - z_v)^2
		\geq \sum_{u \in V} z_u^2 - 2 \beta \sum_{u \in V} z_u,
	\]
	where \(\beta \) is the median of \(z_u\) for \(u \in V\). We see that by taking expectation on both sides, with \autoref{lma:Gaussian-random-projection}, we have
	\[
		\mathbb{E}_{}\left[\sum_{uv \in M_i} (z_u - z_v)^2 \right]
		\geq \mathbb{E}_{}\left[\sum_{u \in V} z_u^2 \right] - 2\beta \mathbb{E}_{}\left[\sum_{u \in V} z_u \right]
		= \sum_{u \in V} \mathbb{E}_{}\left[z_u^2 \right]
		= \sum_{u \in V} \lVert x_u^{(i-1)} \rVert _2^2
		= \phi _{i-1},
	\]
	where we use the fact that the sum of mean zero Gaussian's is still a mean zero Gaussian.
\end{proof}

With \autoref{lma:cut-matching-linear-projection}, we can then combine \autoref{lma:cut-matching-potential-reduction-1-dimension} and \autoref{lma:cut-matching-deviation} to prove the following:

\begin{lemma}\label{lma:cut-matching-potential-reduction}
	Given current flow vectors \(b_u^{(i-1)}\) and difference vectors \(x_u^{(i-1)}\) for all \(u \in V\), there is a randomized algorithm that produces a partition \((S_i, V\setminus S_i)\) such that for any perfect matching \(M_i\) between \(S_i\) and \(V\setminus S_i\), the potential \(\phi _{i}\) after mixing according to \(M_i\) satisfies
	\[
		\mathbb{E}_{}[\phi _i]
		\leq \left( 1 - \frac{1}{c \log n} \right) \phi _{i-1} + O(n^{-10}),
	\]
	where \(c\) is some sufficiently large fixed constant.
\end{lemma}
\begin{proof}
	Consider the linear projection strategy described above. Since \((z_u - z_v)^2 = \langle g, x_u^{(i-1)} - x_v^{(i-1)} \rangle ^2\), then by \autoref{lma:Gaussian-random-projection}, with high probability,
	\begin{equation}\label{eq:lma:cut-matching-potential-reduction}
		(z_u - z_v)^2
		= \langle g, x_u^{(i-1)} - x_v^{(i-1)} \rangle ^2
		\leq c \log n \cdot \lVert x_u^{(i-1)} - x_v^{(i-1)} \rVert _2^2.
	\end{equation}
	Assuming this happens deterministically. Then, from \autoref{lma:cut-matching-linear-projection} and \autoref{eq:cut-matching-deviation},
	\[
		\mathbb{E}_{}[\phi _{i-1} - \phi _i]
		= \frac{1}{2} \mathbb{E}_{}\left[\sum_{uv \in M_i} \lVert x_u^{(i-1)} - x_v^{(i-1)} \rVert _2^2 \right]
		\geq \frac{1}{c \log n} \mathbb{E}_{}\left[\sum_{uv \in M_i} (z_u - z_v)^2 \right]
		\geq \frac{1}{c \log n} \phi _{i-1},
	\]
	which implies \(\mathbb{E}_{}[\phi _i] \leq (1 - 1 / c \log n) \phi _{i-1}\). To handle the case that \autoref{eq:lma:cut-matching-potential-reduction} doesn't happen deterministically, we need to do a little of extra work, which is why we get the weaker bound with a slight additive bound. We omit the exact detail for simplicity.
\end{proof}

\begin{intuition}
	\autoref{lma:cut-matching-potential-reduction} states that using random projection, we only loses a \(1 / \log n\) factor compared to the true one-dimensional case.
\end{intuition}

\begin{remark}
	\autoref{lma:cut-matching-potential-reduction} implies that the algorithm uses only the information about the flow routed so far and nothing about the previous matchings.
\end{remark}

\autoref{lma:cut-matching-potential-reduction} immediately imply \autoref{thm:cut-matching-succeed}:

\begin{proof}[Proof of \autoref{thm:cut-matching-succeed}]
	From the previous \hyperref[clm:cut-matching-goal]{claim}, we know that we want to reduce \(\phi _0 \leq n\) to \(1 / 4n^2\). As \(\phi _i \leq (1 - 1 / c \log n) \phi _{i-1}\) for some constant \(c>0\) from \autoref{lma:cut-matching-potential-reduction}, we have
	\[
		\phi _i \leq \left( 1 - \frac{1}{c \log n} \right) ^i \phi _0 \leq \left( 1 - \frac{1}{c \log n} \right) ^i n.
	\]
	Setting the right-hand side to be less than \(1 / 4n^2\) and solve for \(i\), we have \(i = O(\log ^2 n)\).
\end{proof}

This completes the whole proof. Finally, we describe the whole \hyperref[def:cut-matching-game]{cut-matching game} in \autoref{algo:cut-matching-game} for completeness.

\begin{algorithm}[H]\label{algo:cut-matching-game}
	\DontPrintSemicolon{}
	\caption{\hyperref[def:cut-matching-game]{Cut-Matching Game}}
	\KwData{A graph \(G = (V, E)\) with edge capacity \(c \colon E \to \mathbb{R} _+ \), iteration \(k\)}
	\KwResult{A certification graph \(H = (V, \bigcup_{i=1}^{k} M_i)\)}
	\SetKwFunction{Flow}{Single-Commodity-Flow}
	\SetKwFunction{Normal}{Normal}
	\SetKwFunction{Median}{Median}
	\SetKwFunction{CutPlayer}{Cut-Player}
	\SetKwFunction{MatchingPlayer}{Matching-Player}
	\SetKwFunction{Build}{Build-Linkage-Routing-Graph}
	\BlankLine

	\For(\Comment*[f]{Initialization}){\(u \in V\)}{
	\(x_u^{(0)} \gets \mathbbm{1}_{u = v} - \vec{1} / n\)\;
	}
	\;
	\While(){\(i = 1, \dots , k\)}{
	\(S_i\gets\)\CutPlayer{\(\{ x_u^{(i-1)} \} _{u \in V}\)}\;
	\(M_i \gets\)\MatchingPlayer{\(G\), \(S_i\)}\;
	\For(){\(uv \in M_i\)}{
	\(x_u^{(i)} \gets (x_u^{(i-1)} + x_v^{(i-1)}) / 2\)\;
	\(x_v^{(i)} \gets (x_u^{(i-1)} + x_v^{(i-1)}) / 2\)\;
	}
	}
	\Return{\((V, \bigcup_{i=1}^{k} M_i)\)}\;
	\;
	\SetKwProg{Fn}{}{:}{}
	\Fn{\CutPlayer{\(\{ x_u \} _{u \in V}\)}}{
	\(g \gets\)\Normal{\(0\), \(I_n\)}\Comment*[r]{\(g \sim \mathcal{N} (0, I_n)\)}
	\For(){\(u \in V\)}{
		\(z_u \gets \langle g, x_u \rangle \)\;
	}
	\(\beta \gets\)\Median{\(\{ z_u \} _{u \in V}\)}\;
	\(S \gets \{ u \in V \colon z_u \leq \beta \}\)\Comment*[r]{Break ties arbitrarily such that \(\lvert S \rvert = n / 2\)}
	\Return{\(S\)}\;
	}
	\;
	\Fn{\MatchingPlayer{\(G\), \(S\)}}{
		\((G^{\prime} , s^{\ast} , t^{\ast} )\gets\)\Build{\(G\), \(S\), \(V\setminus S\)}\Comment*[r]{\autoref{fig:expansion-matching}}
		\(M \gets\)\Flow{\(G^{\prime} \), \(s^{\ast}\), \(t^{\ast} \)}\Comment*[r]{Compute \hyperref[def:routable]{routable} perfect matching}
		\Return{\(M\)}\;
	}
\end{algorithm}

\begin{note}
	Explicitly, we run \autoref{algo:cut-matching-game} with \(k = O(\log ^2 n)\), and whenever \(M\) is not a valid perfect matching or is not \hyperref[def:routable]{routable} (indicating that \(S\) and \(V\setminus S\) is not \hyperref[def:well-linked]{well-linked}), we just report that \(G\) is not a \hyperref[def:expander]{\(1\)-expander}.
\end{note}

\section{Application to Treewidth}
Previously, we have mentioned \emph{\hyperref[def:treewidth]{treewidth}} briefly (e.g., \autoref{thm:node-well-linked-treewidth}). Now, we see another good application of the \hyperref[def:cut-matching-game]{cut-matching game} to \hyperref[def:treewidth]{treewidth}.

\subsection{Tree Decomposition and Treewidth}
We care about how ``tree-like'' does the graph look. One of the (among many) interesting properties of a tree is that we can separate the tree in a balanced way by removing only one node:

\begin{eg}[Tree]
	For a tree \(T\), there exists a single vertex such that after removing it, no connect component has more than \(2 n/ 3\) vertices, i.e., there exists a singleton \hyperref[def:balanced-separator]{balanced separator}.
\end{eg}

What if we allow removing more vertices?

\begin{eg}[Series-parallel graph]
	\href{https://en.wikipedia.org/wiki/Series%E2%80%93parallel_graph}{Series-parallel graph} has the same property when allowing removing \(2\) vertices.
\end{eg}

Taking these two examples into account, we consider the following notion of \hyperref[def:tree-decomposition]{tree decomposition}.

\begin{definition}[Tree decomposition]\label{def:tree-decomposition}
	Given a graph \(G = (V, E)\), a \emph{tree decomposition} of \(G\) is a tree \(T = (V_T, E_T)\) and a function \(B \colon V_T \to 2^V\) such that
	\begin{enumerate}[(i)]
		\item for all \(u \in V\), the set of ``bags'' \(B(t)\) that contains \(u\) form a connected sub-tree of \(T\);
		\item for all \(uv \in E\), there exists some \(t \in V_T\) such that both \(u, v \in B(t)\).
	\end{enumerate}
\end{definition}

\begin{definition}[Width]\label{def:tree-decomposition-width}
	The \emph{width} \(\operatorname{width}(T)\) of a \hyperref[def:tree-decomposition]{tree decomposition} \(T\) of \(G\) with the function \(B \colon V_T \to 2^V\) is defined as \(\max _{t \in V_T} \lvert B(t) \rvert \).
\end{definition}

\begin{figure}[H]
	\centering
	\incfig{tree-decomposition}
	\caption{The \hyperref[def:tree-decomposition]{tree decomposition} of \(G\) with its ``bag'' plotted explicitly.}
	\label{fig:tree-decomposition}
\end{figure}

Then, we can define the \hyperref[def:treewidth]{treewidth} formally.

\begin{definition}[Treewidth]\label{def:treewidth}
	Given a graph \(G\), the \emph{treewidth} \(\operatorname{tw}(G) \) is defined as \(\min _{T} \operatorname{width}(T) - 1\), where the minimum is taking over all possible \hyperref[def:tree-decomposition]{tree decompositions} of \(G\).
\end{definition}

The minus one is because the minimum maximum \hyperref[def:tree-decomposition-width]{width} of the \hyperref[def:tree-decomposition]{tree decomposition} for a tree is \(2\):

\begin{eg}[Tree]
	For any tree \(T\), there exists a \hyperref[def:tree-decomposition]{tree decomposition} with \hyperref[def:tree-decomposition-width]{width} \(2\), i.e., \(\operatorname{tw}(T) = 1\).
	\begin{center}
		\incfig{tree-decomposition-tree}
	\end{center}
\end{eg}

\begin{eg}[Cycle]
	For any cycle \(C_n\), there exists a \hyperref[def:tree-decomposition]{tree decomposition} with \hyperref[def:tree-decomposition-width]{width} \(3\), i.e., \(\operatorname{tw}(C_n) = 2\).
	\begin{center}
		\incfig{tree-decomposition-cycle}
	\end{center}
\end{eg}

\begin{eg}[Series-Parallel]
	We have that \(\operatorname{tw}(G) \leq 2\) if and only if \(G\) is series-parallel.
\end{eg}

\begin{eg}[Clique]
	For any clique \(K_n\), we have \(\operatorname{tw}(K_n) = n-1\) with the trivial \hyperref[def:tree-decomposition]{tree decomposition}, i.e., \(T = (V_T, E_T)\) with \(V_T = \{ t \} \), \(E_T = \varnothing \) such that \(B(t) = V\).
\end{eg}

\subsection{Balanced Separator and Treewidth}
Essentially, \autoref{def:treewidth} captures the desired \hyperref[def:balanced-separator]{balanced separator} properties we mentioned above:

\begin{intuition}
	Any graph \(G\) with \(\operatorname{tw}(G) \leq k\) implies that \(G\) can be recursively decomposed via \hyperref[def:balanced-separator]{balanced separators} of size \(k\). Approximate converse also holds, i.e., if there is a subgraph \(H\) of \(G\) with no \hyperref[def:balanced-separator]{balanced separator} of size \(k\), then \(\operatorname{tw}(G) \geq k / c\) for some \(c > 0\).
\end{intuition}

Formally, we have the following:

\begin{lemma}
	Suppose \(\operatorname{tw}(G) \leq k\). Then for any \(X \subseteq V\), there exists a \hyperref[def:balanced-separator]{balanced separator} \(S\) for \(X\) of size \(\lvert S \rvert \leq k\), i.e., \(G-S\) has no component with more than \(2 \lvert X \rvert / 3\) vertices from \(X\).
\end{lemma}

The idea is that for any edge \((t, t^{\prime} ) \in E_T\) of the \hyperref[def:tree-decomposition]{tree decomposition} \(T\) that achieves the optimal \hyperref[def:tree-decomposition-width]{width} (i.e., \(\operatorname{tw}(G) + 1\)), \(B(t) \cap B(t^{\prime} )\) is a separator of \(G\). Hence, \(\operatorname{tw}(G) \leq k \) implies \(G\) has a \hyperref[def:balanced-separator]{balance separator} \(S\) of size \(k\). The recursive property follows from that fact that \(\operatorname{tw}(H) \leq \operatorname{tw}(G) \) for any subgraph \(H\) of \(G\). Let us rephrase \autoref{thm:node-well-linked-treewidth} as follows:

\begin{lemma}\label{lma:node-well-linked-treewidth}
	For any graph \(G\), \(\operatorname{tw}(G) \geq k\) if and only if there exists \(X\) such that \(\lvert X \rvert \geq k / 4\) such that \(X\) is \hyperref[def:node-well-linked]{node-well-linked}.
\end{lemma}

With these characterizations, we have the following examples.

\begin{eg}[Grid]
	For \(\sqrt{n} \times \sqrt{n} \) grid \(G\), \(\operatorname{tw}(G) = \sqrt{n} - 1 \).
\end{eg}

Indeed, grid the worst case for planar graphs.

\begin{eg}[Planar separator theorem]\label{eg:planar-separator-theorem}
	For any planar graph \(G\), \(\operatorname{tw}(G) = O(\sqrt{n} )\)~\cite{lipton1979separator}.
\end{eg}

\begin{eg}[Random graph]
	For random \(d\)-regular graph \(G\), \(\operatorname{tw}(G) = \Theta (n)\) with high probability.
\end{eg}
\begin{explanation}
	Since random \(d\)-regular graph is an \hyperref[def:expander]{expander} with high probability, and \hyperref[def:balanced-separator]{balanced separator} is of size \(\Omega (n)\) for \hyperref[def:expander]{expander}.
\end{explanation}

\subsection{Cut-Matching Game to Certify Treewidth}
Now, we try to verify the \hyperref[def:treewidth]{treewidth}. Verifying a small \hyperref[def:treewidth]{treewidth} is easy, we just ask for a \hyperref[def:tree-decomposition]{tree decomposition} as a certification. In general, given \(G\) and \(k\), checking if \(\operatorname{tw}(G) \leq k\) is \(\NP\)-complete~\cite{arnborg1987complexity}. However, it is fixed-parameter-tractable, i.e., for every fixed \(k\), there is a linear-time algorithm (specifically, \(O(k^{k^3} n)\)), that, given \(G\), decides whether \(\operatorname{tw}(G) \leq k\)~\cite{bodlaender1993linear}. As for an approximation algorithm for computing \hyperref[def:treewidth]{treewidth}, it takes \(O(2^k n)\) time for a \(5\)-approximation~\cite{bodlaender2016c}, while for the best known polynomial-time approximation algorithm is \(O(k \sqrt{\log k} )\)-approximate~\cite{feige2005improved}.

Now, it turns out that instead of directly certifying/computing the \hyperref[def:treewidth]{treewidth}, from the understanding of the connection between the size of \hyperref[def:balanced-separator]{balanced separator} and \hyperref[def:treewidth]{treewidth}, with the fact that \hyperref[def:expander]{expander} has large \hyperref[def:balanced-separator]{balanced separator}, one natural idea is the following:

\begin{intuition}
	We try to ``embed'' an \hyperref[def:expander]{expander} using \hyperref[def:cut-matching-game]{cut-matching game}.
\end{intuition}

Utilizing this idea, the following is known:

\begin{theorem}\label{thm:treewidth-sparsifier}
	Let \(G = (V, E)\) be a graph with \hyperref[def:treewidth]{treewidth} \(\geq k\). Then three is a randomized algorithm that computes a subgraph \(H = (V, E_H)\) such that with high probability,
	\begin{enumerate}[(i)]
		\item \(\operatorname{tw}(H) = \Omega (k / \poly \log k)\), where \(\poly \log k\) is something like \(\log ^2 k\);
		\item \(\deg_H(v) = O(\log ^2 k)\).
	\end{enumerate}
\end{theorem}

\autoref{thm:treewidth-sparsifier} is further improved as follows:

\begin{theorem}[\cite{chekuri2013poly,chekuri2014degree}]\label{thm:treewidth-sparsifier-improved}
	Let \(G = (V, E)\) be a graph with \hyperref[def:treewidth]{treewidth} \(\geq k\). Then three is a randomized algorithm that computes a subgraph \(H = (V, E_H)\) such that with high probability,
	\begin{enumerate}[(i)]
		\item \(\operatorname{tw}(H) = \Omega (k / \poly \log k)\), where \(\poly \log k\) is something like \(\log ^{100} k\);
		\item \(\deg_H(v) = 3\).
	\end{enumerate}
\end{theorem}

\begin{remark}
	The full generality of \autoref{thm:treewidth-sparsifier} and \autoref{thm:treewidth-sparsifier-improved} are stated as topological minor. We omit the exact details.
\end{remark}

\subsection{Algorithm with Treewidth}
The reason why we care about \hyperref[def:treewidth]{treewidth} is that there are several ``templates'' for applications:
\begin{itemize}
	\item if \(G\) has \emph{small} (constant) \hyperref[def:treewidth]{treewidth}, we can then solve the problem via dynamic programming on the \hyperref[def:tree-decomposition]{tree decomposition};
	\item if \(G\) has \emph{large} \hyperref[def:treewidth]{treewidth}, we can then use structure, in particular, obstructions such as grids:
	      \begin{itemize}
		      \item answer is clear from obstruction, or
		      \item ``reduce'' the problem in some fashion and recurse.
	      \end{itemize}
\end{itemize}

Let's just look at the case that when the \hyperref[def:treewidth]{treewidth} is small. In this case, dynamic programming based algorithms for trees extends naturally to bounded \hyperref[def:treewidth]{treewidth} graphs.

\begin{eg}[Problems on planar graph]
	From the \hyperref[eg:planar-separator-theorem]{planar separator theorem}, we can solve many problems in \(2^{\sqrt{n} }\) using dynamic programming for planar graphs.
\end{eg}

One such problem is the \hyperref[prb:MWIS]{maximum (weighted) independent set}:

\begin{problem}[Maximum weighted independent set]\label{prb:MWIS}
Given a graph \(G = (V, E)\) with vertex capacity \(w \colon V \to \mathbb{R} \), the \emph{maximum weighted independent set} (MWIS) problem asks for computing \(\max _{S \subseteq V} w(S)\) such that \(S \subseteq V\) is an independent set.
\end{problem}

\hyperref[prb:MWIS]{MWIS} is \(\NP\)-hard, even for planar graphs, and more generally, is also very hard even to approximate in general graphs. However, we can still solve it efficiently (in polynomial time) in bounded \hyperref[def:treewidth]{treewidth} graphs. To get some intuition, let's first see how to solve it for tree using dynamic programming.

\begin{eg}[MWIS on tree]
	Given a tree \(T\) with vertex capacity \(w\colon V \to \mathbb{R} \), let \(T_v\) be the sub-tree of \(T\) rooted at node \(v \in V_T\), and let \(\OPT(v)\) be the optimum value of \hyperref[prb:MWIS]{MWIS} on \(T_v\). Furthermore, let's define \(\OPT(v, 1)\) be the optimum value of \hyperref[prb:MWIS]{MWIS} on \(T_v\) that includes \(v\), and \(\OPT(v, 0)\) is defined similarly such that it does not include \(v\). Then, we have \(\OPT(v) = \max (\OPT(v, 1), \OPT(v, 0))\).

	After some observation, we see that
	\[
		\begin{dcases}
			\OPT(v, 1) = w(v) + \sum_{u \text{ child of } v} \OPT(u, 0) ; \\
			\OPT(v, 0) = \sum_{u \text{ child of } v} \OPT(u).
		\end{dcases}
	\]
	Using this, we can solve the corresponding dynamic programming problem efficiently.
\end{eg}

Let's now see how to use the above idea to solve \hyperref[prb:MWIS]{MWIS} for graphs with low \hyperref[def:treewidth]{treewidth}.

\begin{eg}[MWIS on tree decomposition]\label{eg:MWIS-tree-decomposition}
	Given a \hyperref[def:tree-decomposition]{tree decomposition} of \hyperref[def:tree-decomposition-width]{width} \(k\) for a graph \(G\), \hyperref[prb:MWIS]{MWIS} can be computed in \(O(k 2^{k+1} n)\) time.
\end{eg}
\begin{explanation}
	Let \(T = (V_T, E_T)\) be the corresponding \hyperref[def:tree-decomposition]{tree decomposition}. For \(t \in V_T\), let \(G_t\) be the subgraph of \(G\) induced by \(S \coloneqq \bigcup_{t \in V(T_t)} B(t) \subseteq V\), i.e., nodes in bags of \(T_t\).

	Now, let \(\OPT(t, S)\) be the value of \hyperref[prb:MWIS]{MWIS} in \(T_t\) among independent sets \(I\) such that \(I \cap X_t = S\). We see that the number of values to compute at each node \(t \in V_T\) is \(\leq 2^{k+1}\), where \(k\) is the \hyperref[def:tree-decomposition-width]{width} of \(T\). Using dynamic programming, we can compute all values from leaves to root in \(O(k 2^{k+1} N)\) time, where \(N\) is the number of nodes in \(T\), i.e., \(\lvert V_T \rvert \).
\end{explanation}

The above yield a \(2^{O(\sqrt{n} )}\) time algorithm for planar graphs. However, it's worth noting that it's possible to get a fast \((1 - \epsilon )\)-approximation algorithm in planar graphs. For that, we need the following.

\begin{theorem}\label{thm:treewidth-planar-graph-color}
	For any planar graph \(G = (V, E)\) and integer \(h \geq 3\), \(V\) can be colored with \(h\) colors such that for any \(i \in [h]\), \(\operatorname{tr}(G - V_i) \leq 3h\) where \(V_i \subseteq V\) is the set of vertices colored with color \(i\).
\end{theorem}

\begin{eg}[MWIS on planar graph]
	Consider \hyperref[prb:MWIS]{MWIS} on planar graphs. Let \(\epsilon \in (0, 1)\) and \(h = 1 / \epsilon \). From \autoref{thm:treewidth-planar-graph-color}, consider removing \(V_i\) from \(G\) and compute the corresponding \hyperref[prb:MWIS]{MWIS} to get \(\OPT_i\) in \(G - V_i\) using the \hyperref[eg:MWIS-tree-decomposition]{previous example}'s approach, which takes \(O(2^{1 / \epsilon } n / \epsilon )\) since \(\operatorname{tr}(G-V_i) \leq 3h = O(1 / \epsilon )\). In total, there are \(h = 1 / \epsilon \) of \(V_i\)'s, hence the total running time is \(O(2^{1 / \epsilon } n / \epsilon ^2)\).

	Then, we simply output the final answer as \(\max _{i\in [h]} \OPT_i\), say it's \(\OPT_{i^{\ast} }\). From the pigeonhole principle, the output \(\OPT_{i^{\ast} }\) for the graph \(G- V_{i^{\ast} }\) is \((1-\epsilon)\)-approximation compared to the original optimum value \(\OPT\) for the whole graph \(G\).
\end{eg}