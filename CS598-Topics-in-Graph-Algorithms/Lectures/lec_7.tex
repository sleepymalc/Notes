\lecture{7}{17 Sep.\ 11:00}{Linear Programming for Sparsest Cut}
\section{Sparsest Cut}
Consider building a network of \(n\) vertices. The best network might be the complete graph, which can do everything and is robust. However, the problem is that the degree is too high (\(n-1\)).

\begin{eg}
	For degree equal to \(2\), the best we can hope for is a cycle.
\end{eg}

The magic happens whenever the degree goes up to \(3\).

\begin{intuition}
	If we can down-weight edges in a complete graph \(K_n\) by \(3 / (n-1)\), then any cut \(S\) has
	\[
		\delta (S)
		= \lvert S \rvert \cdot \lvert V \setminus S \rvert \cdot \frac{3}{n-1}
		\approx c \lvert S \rvert
	\]
	for \(\lvert S \rvert \ll \lvert V \setminus S \rvert \) and some \(c \geq 0 \).
\end{intuition}

This notion can be formalized as the so-called \hyperref[def:expander]{expander}~\cite{hoory2006expander}. We postpone the formal introduction of \hyperref[def:expander]{expander}, and first focus on a closely related problem, the \hyperref[prb:sparsest-cut]{sparsest cut problem}, specifically, the \hyperref[prb:non-uniform-sparsest-cut]{non-uniform} version. It turns out that solving this helps us answer various questions for \hyperref[def:expander]{expanders}.

\subsection{Uniform and Non-Uniform Sparsest Cut Problem}
To introduce the problem, we first define the \hyperref[def:sparsity]{sparsity} for a cut.

\begin{definition}[Sparsity]\label{def:sparsity}
	Given a graph \(G = (V, E)\) with edge capacity \(c \colon E \to \mathbb{R} _{+}\) and a demand graph \(H = (V, F)\) with demand capacity \(D \colon F \to \mathbb{R} _{+}\), for any cut \(S \subseteq V\), its \emph{sparsity} is defined as
	\[
		\frac{c(\delta (S))}{\sum_{i \colon \lvert S \cap \{ s_i, t_i \} \rvert = 1} D_i}.
	\]
\end{definition}

That is, the \hyperref[def:sparsity]{sparsity} of a cut is the ratio of the capacity of the cut and the total demand of the pairs separated by \(S\). Now, we can introduce the \hyperref[prb:non-uniform-sparsest-cut]{non-uniform sparsest cut problem}.

\begin{problem}[Non-uniform sparsest cut]\label{prb:non-uniform-sparsest-cut}
Given a supply graph \(G = (V, E)\) with edge capacity \(c \colon E \to \mathbb{R} _{+}\) and \(k\) pairs of vertices \(\{ (s_i, t_i) \} _{i=1}^{k}\) along with non-negative demand values \(D_1, \dots , D_k\).\footnote{If \(G\) is undirected, then the demand pairs are unordered, i.e., we do not distinguish \((s_i, t_i)\) from \((t_i, s_i)\).} The \emph{non-uniform sparsest cut problem} aims to find a cut \(S\) with minimum \hyperref[def:sparsity]{sparsity}.
\end{problem}

Here, demands forms a demand graph \(H = (V, F)\) with edges \((s_i, t_i)\) and demand capacity \(D\colon F \to \mathbb{R} _{+}\) such that \(D((s_i, t_i)) = D_i\). With this representation, the \hyperref[def:sparsity]{sparsity} of cut \(S\) is simply \(c(\delta _G(S)) / D(\delta _H(S))\) where \(\delta _G(S)\) (respectively, \(\delta _H(S)\)) represents the supply (respectively, demand) edges crossing \(S\).

\begin{intuition}
	We're trying to find the best ``bang per buck''	cut, i.e., how much capacity do we need to remove per amount of demand separated to satisfy the demand?
\end{intuition}

\begin{remark}
	We can define a cut as removing a set of edges, leading to more than two components. In the case of \hyperref[prb:non-uniform-sparsest-cut]{sparsest cut} in undirected graphs, it suffices to restrict attention to cuts of the form \(\delta (S)\) for some \(S \subseteq V\). This is not necessarily true for directed graphs or even in undirected graphs with node-weights or in hypergraphs.
\end{remark}

\begin{problem}[(Uniform) sparsest cut]\label{prb:sparsest-cut}
The \emph{sparsest cut} problem is the same as \autoref{prb:non-uniform-sparsest-cut} with all \(D(u, v) = 1\) for each unordered pair of vertices \((u, v)\).\footnote{That is, \(\{ (s_i, t_i) \} _{i=1}^{k}\) is the set of all unordered pairs of vertices.}
\end{problem}

We see that in the \hyperref[prb:sparsest-cut]{uniform sparsest cut problem}, the \hyperref[def:sparsity]{sparsity} of a cut \(S\) is given by \(c(\delta _G(S)) / \lvert S \rvert \lvert V \setminus S \rvert \). In this case, the demand graph \(H\) is a complete graph with unit demand values on each edge.

Finally, to further motivate the problem, we see that the \hyperref[prb:sparsest-cut]{uniform sparsest cut} helps us directly and indirectly solve the \hyperref[prb:graph-bisection]{graph bisection}, a central problem is graph algorithm:

\begin{problem}[Graph bisection]\label{prb:graph-bisection}
Given a graph \(G = (V, E)\), the \emph{graph bisection problem} aims to find a partition of \(G\) into \((S, V \setminus S)\) such that \(\lvert S \rvert = \lvert V \setminus S \rvert \) while minimizing \(\lvert \delta (S) \rvert \).
\end{problem}

\subsection{Linear Program Relaxation and Maximum Concurrent Flow}
It's not clear how to start solving the \hyperref[prb:sparsest-cut]{uniform sparsest cut problem} as writing a linear program relaxation for which is not obvious, compared to \hyperref[prb:multi-min-cut]{multi-min-cut} and other cut problems where we have explicit terminal pairs that we wish to separate. Hence, to write an integer program for which, we let \(y_i \in \{ 0, 1 \} \) being the indicator variable of whether we want to separate \((s_i, t_i)\). Moreover, let \(x_e \in \{ 0, 1 \} \) for all \(e \in E\) to be the cut indicator variables.

\begin{intuition}
	If we decide to separate \((s_i, t_i)\), then for every path between \(s_i\) and \(t_i\) we should cut at least one edge on the path.
\end{intuition}

Hence, a natural integer program of \hyperref[prb:non-uniform-sparsest-cut]{non-uniform sparsest cut} and its relaxation is given by:
\begin{equation}\label{eq:non-uniform-sparsest-cut-LP-primal}
	\begin{aligned}
		\min~ & \frac{\sum_{e \in E} c(e) x_e}{\sum_{i=1}^{k} D_i y_i}                                                            \\
		      & \sum_{e \in P} x_e \geq y_i                            & \forall P \in \bigcup_{i=1}^{k} \mathcal{P} _{s_i, t_i}; \\
		      & x_e \in \{ 0, 1 \}                                     & \forall e \in E;                                         \\
		      & y_i \in \{ 0, 1 \}                                     & \forall i = 1, \dots , k;                                \\
	\end{aligned}\quad
	\to
	\begin{aligned}
		\min~           & \sum_{e \in E} c(e) x_e                                                                \\
		                & \sum_{i=1}^{k} D_i y_i = 1                                                             \\
		                & \sum_{e \in P} x_e \geq y_i & \forall P \in \bigcup_{i=1}^{k} \mathcal{P} _{s_i, t_i}; \\
		                & x_e \geq 0                  & \forall e \in E ;                                        \\
		\text{(P)}\quad & y_i \geq 0                  & \forall i = 1, \dots , k,                                \\
	\end{aligned}
\end{equation}
where we use the standard trick, i.e., linearization to normalize the denominator to be \(1\), to make the ratio of the integer program into a linear program. With the dual variable \(z_P\) for each path such that it indicates the amount of ``\hyperref[def:flow]{flow}'' sent on the path \(P\), the dual of the above is
\begin{equation}\label{eq:non-uniform-sparsest-cut-LP-dual}
	\begin{aligned}
		\max~           & \lambda                                                                                                                 \\
		                & \sum_{P \in \mathcal{P} _{s_i, t_i}} z_P \geq \lambda D_i    & \forall i = 1, \dots , k;                                \\
		                & \sum_{i=1}^{k} \sum_{\substack{P \in \mathcal{P} _{s_i, t_i}                                                            \\ P \ni e}} z_P \leq c(e) & \forall e \in E;                                         \\
		                & z_P \geq 0                                                   & \forall P \in \bigcup_{i=1}^{k} \mathcal{P} _{s_i, t_i}; \\
		\text{(D)}\quad & \lambda \geq 0,
	\end{aligned}
\end{equation}
which is a multi-commodity \hyperref[def:flow]{flow}. In particular, it solves the \emph{maximum concurrent multi-commodity flow} problem for the given instance, i.e., it finds the largest value of \(\lambda \) such that there is a feasible multi-commodity \hyperref[def:flow]{flow} for the given pairs in which the \hyperref[def:flow]{flow} routed for pair \((s_i, t_i)\) is at least \(\lambda D_i\).

\begin{notation}[Concurrent flow]
	It is called \emph{concurrent flow} since we need to route all demand pairs to the same factor which is in contrast to the \hyperref[eq:multi-min-cut-LP]{dual} of \hyperref[prb:multi-min-cut]{multi-min-cut}, which corresponds to the maximum throughput multi-commodity \hyperref[def:flow]{flow}.\footnote{Recall that in this case, some pairs may have zero \hyperref[def:flow]{flow} while others have a lot of \hyperref[def:flow]{flow}.}
\end{notation}

We note that this dual can be solved efficiently via ellipsoid method since the separation oracle is just the shortest path problem. One can also write a compact linear program via distance variables as
\[
	\begin{aligned}
		\min~ & \sum_{uv \in E} c(uv) d(uv)        \\
		      & \sum_{i=1}^{k} D_i d(s_i, t_i) = 1 \\
		      & \text{\(d\) is a metric on \(V\)}.
	\end{aligned}
\]

In this context, we can understand the \emph{flow-cut gap} of \hyperref[prb:non-uniform-sparsest-cut]{non-uniform sparsest cut} by the following equivalent way of thinking about the problem:

\begin{intuition}[Cut-condition]
	Given a multi-commodify \hyperref[def:flow]{flow} instance on \(G\). A necessary condition to route all the demand pairs is that if \(G\) satisfies the \emph{cut-condition}, i.e., for every \(S \subseteq V\), the capacity \(c(\delta (S))\) is at least the demand separated by \(S\). However, the converse it not necessarily true, i.e., the cut-condition is not sufficient.
\end{intuition}

The cut-condition is sufficient when \(k = 1\) but is not true in general even for \(k = 3\) in undirected graphs. The question is the maximum value of \(\lambda \) such that we can route \(\lambda D_i\) for each pair \(i\). The worst-case integrality gap of the \hyperref[eq:non-uniform-sparsest-cut-LP-primal]{proceeding linear program} is precisely the flow-cut gap.

\subsection{Rounding Linear Program via \(\ell _1\) Embeddings}
It's known that there is an \(O(\log n)\)-approximation algorithm and the flow-cut gap for \hyperref[prb:sparsest-cut]{uniform sparsest cut}, together with a lower bound of \(\Omega (\log n)\) on the flow-cut gap for \hyperref[prb:sparsest-cut]{uniform sparsest cut} via \hyperref[def:expander]{expanders}~\cite{leighton1999multicommodity}. This leads to an \(O(\log ^2 n)\)-approximation for \hyperref[prb:non-uniform-sparsest-cut]{non-uniform sparsest cut}, and it was an open problem to obtain a tight conjectured bound of \(O(\log n)\). It turns out to be possible via the optimal rounding algorithm for the \hyperref[eq:non-uniform-sparsest-cut-LP-primal]{linear program relaxation}. This goes via metric embedding theory~\cite{linial1995geometry,aumann1998log}, hence we need some basics in metric embeddings to point out the connection and rounding.

\begin{note}
	Even though the metric embedding machinery is powerful, it can seem like magic. The more basic ideas for \hyperref[prb:sparsest-cut]{uniform sparsest cut} based on region growing is useful to know~\cite{williamson2011design}.
\end{note}

\begin{remark}[Rounding via multi-min-cut]
	There are also close connections between \hyperref[prb:non-uniform-sparsest-cut]{sparsest cut} and \hyperref[prb:multi-min-cut]{multi-min-cut}. In particular, suppose there is an \(\alpha (k, n)\)-approximation for \hyperref[prb:non-uniform-sparsest-cut]{non-uniform sparsest cut}, then we have an \(O(\alpha (k, n) \ln k)\)-approximation for \hyperref[prb:multi-min-cut]{multi-min-cut}. The converse is also true.\footnote{See the \href{https://courses.grainger.illinois.edu/cs598csc/fa2024/Notes/lec-sparsest-cut.pdf}{note} for a reference.}
\end{remark}

Before we start, consider the following simple setting when \(G\) is a tree \(T = (V, E)\).

\begin{eg}[Tree]
	Given a tree \(G = T = (V, E)\), for each edge \(e \in T\), we can associate a cut \(S_e\) which is one side of the two components in \(T - e\). The capacity of the cut \(\delta (S_e)\) is \(c(e)\). Let \(D(e) = \sum_{i\colon \lvert S_e \cup \{ s_i, t_i \} \rvert = 1} D_i \) be the demand separated by \(e\). The \hyperref[def:sparsity]{sparsity} of the cut \(S_e\) is simply \(c(e) / D_e\), hence finding the \hyperref[prb:sparsest-cut]{sparsest cut} in this case is easy. Interestingly, the \hyperref[eq:non-uniform-sparsest-cut-LP-primal]{linear program relaxation} give an optimum solution on a tree.
\end{eg}
\begin{explanation}
	Let \((x, y)\) be a feasible solution to the \hyperref[eq:non-uniform-sparsest-cut-LP-dual]{dual} of the \hyperref[eq:non-uniform-sparsest-cut-LP-primal]{linear program relaxation} with objective value \(\lambda \). We want to prove that if \(G\) is a tree \(T\), then there is an edge \(e \in T\) such that \(c_e / D_e \leq \lambda \). We note that by considering the compact linear program with distance variables, we have
	\[
		\lambda
		= \frac{\sum_{e \in E} c(e) x_e}{\sum_{i=1}^{k} D_i d_x(s_i, t_i)}
	\]
	where \(d_x(s_i, t_i)\) is the shortest path distance between \(s_i\) and \(t_i\) induced by \(x\). Since there is a unique path \(P_{s_i, t_i}\) from \(s_i\) to \(t_i\) in \(T\) such that \(d_x(s_i, t_i) = \sum_{e \in P_{s_i, t_i}} x_e\), we have
	\[
		\lambda
		= \frac{\sum_{e \in E} c(e) x_e}{\sum_{i=1}^{k} D_i d_x(s_i, t_i)}
		= \frac{\sum_{e \in E} c(e) x_e}{\sum_{i=1}^{k} D_i \sum_{e \in P_{s_i, t_i}} x_e}
		= \frac{\sum_{e \in E} c(e) x_e}{\sum_{e \in E} x_e \sum_{i \colon e \in P_{s_i, t_i}} D_i}
		= \frac{\sum_{e \in E} c(e) x_e}{\sum_{e \in E} D(e) x_e }.
	\]
	Finally, the result follows from \(\sum_{i} a_i / \sum_{i} b_i \geq \min _i a_i / b_i\) for positive \(a_i\) and \(b_i\)'s.
\end{explanation}

\begin{eg}[Ring]
	For a ring graph, the same technique works where we need to remove two edges.
\end{eg}

The reason why the above proof works for trees is because of a more general phenomenon: the shortest path distances are \hyperref[def:l1-metric]{\(\ell _1\) metrics}, or equivalently, \hyperref[def:cut-metric]{cut metrics}.

\subsubsection{Cut, Line, and \(\ell _1\) Metrics, and Metric Embedding}
To explain the above phenomenon, consider a finite metric space \((V, d)\) with following metrics:

\begin{definition}[Cut metric]\label{def:cut-metric}
	Let \((V, d)\) be a finite metric space. The metric \(d\) is a \emph{cut metric} if there is a set \(S \subseteq V\) such that \(d = d_S\), where \(d_S\) associated with the cut \(S\) is defined as
	\[
		d_S(u, v)
		= \begin{dcases}
			1, & \text{ if } \lvert S \cap \{ u, v \}  \rvert = 1 ; \\
			0, & \text{ otherwise} .
		\end{dcases}
	\]
\end{definition}

The \hyperref[def:cut-cone]{cut-cone} consists of non-negative combination of \hyperref[def:cut-metric]{cut metrics}:

\begin{definition}[Cut cone]\label{def:cut-cone}
	Let \((V, d)\) be a finite metric space. The metric \(d\) is in the \emph{cut cone} if there exist non-negative scalars \(y_S\) where \(S \subseteq V\) such that for all \(u, v \in V\),
	\[
		d(u, v)
		= \sum_{S \subseteq V} y_S d_S(u, v).
	\]
\end{definition}

Beside the \hyperref[def:cut-metric]{cut metric}, another useful metric is the \hyperref[def:line-metric]{line metric} and the well-known \hyperref[def:l1-metric]{\(\ell _1\) metric}:

\begin{definition}[Line metric]\label{def:line-metric}
	Let \((V, d)\) be a finite metric space. The metric \(d\) is a \emph{line metric} if there is a mapping \(f \colon V \to \mathbb{R} \) such that for all \(u, v \in V\),
	\[
		d(u, v)
		= \lvert f(u) - f(v) \rvert .
	\]
\end{definition}

\begin{definition}[\(\ell _1\) metric]\label{def:l1-metric}
	Let \((V, d)\) be a finite metric space. The metric \(d\) is an \emph{\(\ell _1\) metric} if there is some integer \(d\) and a mapping \(f \colon V \to \mathbb{R} ^d\) such that for all \(u, v \in V\),
	\[
		d(u, v)
		= \lVert f(u) - f(v) \rVert _1
	\]
\end{definition}

It might not be too surprising that the following holds.

\begin{lemma}\label{lma:l1-metric-iff-non-negative-combination-of-line-metric}
	A metric \(d\) of a metric space \((V, d)\) is an \hyperref[def:l1-metric]{\(\ell _1\) metric} if and only if it is a non-negative combination of \hyperref[def:line-metric]{line metrics} (in the cone of the \hyperref[def:line-metric]{line metrics}).
\end{lemma}
\begin{proof}
	If \(d\) is an \hyperref[def:l1-metric]{\(\ell _1\) metric} then each dimension corresponds to a \hyperref[def:line-metric]{line metric}. Conversely, any non-negative combination of \hyperref[def:line-metric]{line metrics} can be made into an \hyperref[def:l1-metric]{\(\ell _1\) metric} where each \hyperref[def:line-metric]{line metric} becomes a separate dimension (scalar multiplication for a \hyperref[def:line-metric]{line metric} is also a \hyperref[def:line-metric]{line metric}).
\end{proof}

A more interesting observation is that any \hyperref[def:cut-metric]{cut metric} \(d_S\) is a simple \hyperref[def:line-metric]{line metric}: map all vertices in \(S\) to \(0\) and all vertices in \(V \setminus S\) to \(1\). This leads to the following.

\begin{lemma}\label{lma:l1-metric-iff-cut-cone}
	A metric \(d\) is an \hyperref[def:l1-metric]{\(\ell _1\) metric} if and only if \(d\) is in the \hyperref[def:cut-cone]{cut cone}.
\end{lemma}
\begin{proof}
	If \(d\) is in the \hyperref[def:cut-cone]{cut cone}, then it is a non-negative combination of the \hyperref[def:cut-metric]{cut metrics}, hence it is a non-negative combination of \hyperref[def:line-metric]{line metrics} by the above observation, hence an \hyperref[def:l1-metric]{\(\ell _1\) metric}.

	For the converse, it suffices to argue that any \hyperref[def:line-metric]{line metric} is in the \hyperref[def:cut-cone]{cut cone}. Let \(V = \{ v_i \} _{i = 1}^{n}\) and let \(d\) be a \hyperref[def:line-metric]{line metric} on \(V\). Without loss of generality, assume that the coordinates \(x_i\) of the points for each \(v_i\) corresponding to the \hyperref[def:line-metric]{line metric} \(d\) are \(x_1 \leq x_2 \leq \dots \leq x_n\) on the real line. For \(1 \leq i < n\), let \(S_i = \{ v_1, v_2, \dots , v_i \} \). It is not hard to verify that \(\sum_{i=1}^{n-1} \lvert x_{i+1} - x_i \rvert d_{S_i} = d\).
\end{proof}

Now we have introduced all the necessary metrics we will use. Consider a finite metric space \((V, d)\).

\begin{claim}
	Any finite metric space can be viewed as one that is derived from the shortest path metric induced on a graph with some non-negative edge lengths.
\end{claim}

If \(G = (V, E)\) is a simple graph and \(\ell \colon E \to \mathbb{R} _{+}\) are some edge-lengths, the metric induced on \(V\) depends both on the \emph{topology} of \(G\) and the lengths as well, i.e., finite metrics can encode graph structure, hence it can be diverse. When trying to round we may want to work with simpler metric spaces.

\begin{intuition}[Embedding]
	Embed a given metric space \((V, d)\) into a simpler host metric space \((V^{\prime} , d^{\prime} )\) via an embedding \(f \colon V \to V^{\prime} \).
\end{intuition}

\begin{note}
	Even though we may be interested in finite metric spaces, the host metric space can be continuous or infinite such as the \(\mathbb{R} ^h\) for dimension \(h\).
\end{note}

As embedding typically distorts the distances, thus, we want to find embeddings with small \hyperref[def:distortion]{distortion}.

\begin{definition}[Distortion]\label{def:distortion}
	Let \((V, d)\) and \((V^{\prime} , d^{\prime} )\) be two metric spaces and let \(f \colon V \to V^{\prime} \) be an embedding. The \emph{distortion} of \(f\) is given by\footnote{Additive version are also explored, although they are very restrictive due to lack of scale invariance.}
	\[
		\max _{\substack{u, v \in V \\ u \neq v}} \left( \frac{d^{\prime} (f(u), f(v))}{d(u, v)} , \frac{d(u, v)}{d^{\prime} (f(u), f(v))}\right) .
	\]
\end{definition}

Additionally, we're interested in the following kind of embeddings:

\begin{definition*}
	Let \((V, d)\) and \((V^{\prime} , d^{\prime} )\) be two metric spaces and let \(f \colon V \to V^{\prime} \) be an embedding.
	\begin{definition}[Isometric embedding]\label{def:isometric-embedding}
		The embedding \(f\) is \emph{isometric} if for all \(u, v \in V\),
		\[
			d(u, v) = d^{\prime} (f(u), f(v)).
		\]
	\end{definition}
	\begin{definition}[Contraction]\label{def:contraction}
		The embedding \(f\) is a \emph{contraction} if for all \(u, v \in V\),
		\[
			d(u, v) \geq d^{\prime} (f(u), f(v)).
		\]
	\end{definition}
	\begin{definition}[Non-contracting]\label{def:non-contracting}
		The embedding \(f\) is \emph{non-contracting} if for all \(u, v \in V\),
		\[
			d(u, v) \leq d^{\prime} (f(u), f(v)).
		\]
	\end{definition}
\end{definition*}

Of particular importance are embeddings of finite metric spaces into \(\mathbb{R} ^h\), where the distance in the host space is measured under a norm such as \(\ell _p\) norm. The dimension \(h\) is also important in various applications but in some settings like with \hyperref[prb:non-uniform-sparsest-cut]{non-uniform sparsest cut}, it is not. We assume the following:

\begin{theorem}[Bourgain~\cite{bourgain1985lipschitz,linial1995geometry}]\label{thm:Bourgain}
	Any \(n\)-point finite metric space can be embedded into \(\ell _2\) (hence also \(\ell _1\)) with \hyperref[def:distortion]{distortion} \(O(\log n)\). Moreover, the embedding is a \hyperref[def:contraction]{contraction} and can be constructed in randomized polynomial time and embeds points into \(\mathbb{R} ^h\) where \(h = O(\log ^2 n)\).
\end{theorem}

In fact, one can obtain a refined version of \autoref{thm:Bourgain} that is useful for \hyperref[prb:non-uniform-sparsest-cut]{non-uniform sparsest cut}.

\begin{theorem}[Bourgain~\cite{linial1995geometry}]\label{thm:Borugain*}
	Let \((V, d)\) be an \(n\)-point finite metric space and let \(S \subseteq V\) with \(\lvert S \rvert = k\). Then there is a randomized polynomial time algorithm to compute an embedding \(f \colon V \to \mathbb{R} ^{O(\log ^2 n)}\) such that the embedding is a \hyperref[def:contraction]{contraction}\footnote{I.e., \(\lVert f(u) - f(v) \rVert _1 \leq d(u, v)\) for all \(u, v \in V\)} and for every \(u, v \in S\), \(\lVert f(u) - f(v) \rVert _1 \geq c d(u, v) / \log k\) for some universal constant \(c\).
\end{theorem}

By utilizing \autoref{thm:Borugain*} with the previous insight on tree, we can provide a general guarantee.

\begin{prev}
	The integrality gap of the \hyperref[eq:non-uniform-sparsest-cut-LP-primal]{linear program} is \(1\) on trees since the shortest path metric on trees is in the \hyperref[def:cut-cone]{cut cone}, i.e., \(\ell _1\)-embeddable.
\end{prev}

One can prove that if the shortest path metric on a graph \(G\) embeds into \(\ell _1\) with \hyperref[def:distortion]{distortion} \(\alpha \), then the integrality gap of the \hyperref[eq:non-uniform-sparsest-cut-LP-primal]{linear program} is at most \(\alpha \). This will imply an \(O(\log n)\)-integrality gap via \autoref{thm:Borugain*} since any \(n\)-point finite metric space embeds into \(\ell _1\) with \hyperref[def:distortion]{distortion} \(O(\log n)\).