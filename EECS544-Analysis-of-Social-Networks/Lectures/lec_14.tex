\lecture{14}{25 Oct.\ 12:30}{Random Walker and Monte Carlo Estimator}
\section{Page Rank via Markov Chain}
Recall that the basic page rank \(r(n)\) is updated by the equation \(r(n+1) = N^{\top}r(n)\), i.e.,
\[
	r^{\top}(n+1) = r^{\top}(n)N
\]
where \(N\) is a row stochastic matrix. On the other hand, compare to \hyperref[def:Markov-chain]{Markov Chain},
\[
	\Pr(X_n = \cdot)^{\top} = \mu^{\top}P^n = \mu^{\top}P^{n-1}P = \Pr(X_{n-1} = \cdot)^{\top}P
\]
with \(\mu\sim \Pr(X_0 = \cdot)\). We see that the above two equations are the same with
\[
	r(0)\coloneqq \begin{pmatrix}
		1/V    \\
		\vdots \\
		1/V    \\
	\end{pmatrix}.
\]

\begin{remark}
	It turns out that basic page rank is looking at a \hyperref[def:Markov-chain]{Markov chain}! Recall that \(N_{ij}\) is defined as
	\[
		N_{ij} = \begin{dcases}
			\frac{1}{d_i^{out}}, & \text{ if }A_{ij}= 1; \\
			0,                   & \text{ otherwise}.
		\end{dcases}
	\]
	We pick an outgoing neighbor uniformly at random, but no one else.
\end{remark}

This is indeed the idea of a \hyperref[def:random-walker]{random walker}, and we formalize it as follows.
\begin{definition}[Random walker]\label{def:random-walker}
	Given a \hyperref[def:graph]{graph}, a \emph{random walker} will traverse the \hyperref[def:graph]{graph} following some rule with randomness.
\end{definition}

In this case, our \hyperref[def:random-walker]{random walker} will follow the \hyperref[def:simple-random-walk]{simple random walk} rule defined as follows.
\begin{definition}[Simple random walk]\label{def:simple-random-walk}
	Suppose the \hyperref[def:random-walker]{random walker} is now at location (node) \(i\), then regardless of how he came to \(i\), he'll pick the next destination uniformly at random from outgoing neighbors of \(i\).
	\begin{center}
		\incfig{random-walker}
	\end{center}
\end{definition}

\begin{note}
	If \(d_i^{\text{out}} = 0\), then the \hyperref[def:random-walker]{random walker} following \hyperref[def:simple-random-walk]{simple random walk} will get stuck.
\end{note}

Now, just like in the basic page rank cases, we have
\[
	\widetilde{N} (s) = s N + \frac{1-s}{\left\vert \mathcal{V}  \right\vert }\vec{1} \vec{1} ^{\top}
\]
for scaled page rank update equation.

\begin{intuition}
	Compare to basic page rank, scaled page rank can be thought as follows. When a \hyperref[def:random-walker]{random walker} at location \(i\), we toss a biased coin with \(\Pr(\{\Head \}) = s\). If we get heads, then we follow the \hyperref[def:simple-random-walk]{simple random walk rule}. Otherwise, if we get tails, we pick any node on the network at random as the next destination.
\end{intuition}

\section{Personalized Page Rank via Markov Chain}
\begin{prev}
	Recall that we want to associate page rank with \hyperref[def:random-walker]{random walker}.
	\begin{itemize}
		\item Basic page rank: \hyperref[def:random-walker]{random walker} on the \hyperref[def:graph]{graph}.
		\item Scaled page rank: \hyperref[def:random-walker]{random walker} with reset to a uniformly chosen node.
	\end{itemize}
\end{prev}
This is a good view point for the \textbf{population as a whole user} surfing the webs. Across population reset is to a randomly chosen webpage. We can rank webpages across the population.

\begin{problem}
Can we be more specific?
\end{problem}
\begin{answer}
	With the help of reset, yes.
	\begin{remark}
		Personalizing the restart distribution is what allows this.
	\end{remark}
\end{answer}

Algorithmically, we have the following.

\begin{algorithm}[H]\label{algo:personalized-page-rank-scheme}
	\DontPrintSemicolon{}
	\caption{Personalized Page Rank Scheme}
	\KwData{A network \(\mathcal{G} = (\mathcal{V} , \mathcal{E} )\), iteration \(T\), \(v^{\ast} \in \mathcal{V} \)}
	\SetKwFunction{RW}{RandomWalk}
	\BlankLine

	\For(){\(i = 1, \dots, T\)}{
		\RW{\(v\)}\Comment*[r]{Perform a random walk starting at \(v\)}
	}
	\Return{}\;
\end{algorithm}
\begin{figure}[H]
	\centering
	\incfig{personalized-pagerank}
	\caption{Random Walk for personalized page rank}
	\label{fig:personalized-pagerank}
\end{figure}

We denote the personalized page rank at \(v^{\ast} \) as \(\pi_{v^{\ast}}(s)\). Collect \(\pi_{v^{\ast}}(s)\) for all \(v^{\ast}\in\mathcal{V}\), then if we reset uniformly,
\[
	\pi_{\mathcal{V}} (s) = \frac{1}{\left\vert \mathcal{V} \right\vert }\sum\limits_{v^{\ast}\in\mathcal{V}} \pi_{v^{\ast}}(s).
\]

\begin{remark}
	Average of all the personalized page rank, one can get the scaled page rank since averaging with respect to any specific distribution \(\mu\) distributed on \(\mathcal{V}\), we have
	\[
		\pi_\mu(s) = \sum\limits_{v^{\ast}\in \mathcal{V}} \mu(v^{\ast})\pi_{v^{\ast}}(s).
	\]
	We see that if \(\mu\) is uniformly distributed, we get scaled page rank.
\end{remark}

\section{Monte Carlo Algorithm}
In this section, we'll see some simulation-based algorithms which can help us estimate page rank empirically. This kind of simulation-based algorithm which estimate some kind of randomness is called \emph{Monte Carlo algorithm}. We'll analyze the corresponding estimators' quality in terms of their bias, variance, and so on. Let's start with the first one.

\subsection{First Page Rank Estimator}
We now introduce the first simulation-based algorithm to estimate page rank. The very first one we should do is the \hyperref[algo:random-walk-algorithm]{random walk} where we essentially implement \hyperref[def:simple-random-walk]{simple random walk}.

\begin{algorithm}[H]\label{algo:random-walk-algorithm}
	\DontPrintSemicolon{}
	\caption{Random Walk}
	\SetKwData{result}{result}
	\SetKwData{arrival}{arrival}
	\KwData{A network \(\mathcal{G} = (\mathcal{V} , \mathcal{E} )\), steps \(T\), \(s\)}
	\KwResult{A list \arrival, ending node \(v\in \mathcal{V}\)}
	\SetKwFunction{random}{random}
	\SetKwFunction{toss}{TossCoin}
	\SetKw{Continue}{continue}

	\BlankLine

	\For(\Comment*[f]{Initialize}){\(v\in \mathcal{V} \)}{
		\(\arrival(v)\gets 0\)\;
	}
	\(v\gets \)\random{\(\mathcal{V}\)}\Comment*[r]{select a uniformly random node to start}
	\(\arrival(v)\gets 1\)\;
	\;
	\For(){\(i = 1, \dots, T\)}{
	\(\result\gets\)\toss{\(s\)}\Comment*[r]{Toss a biased coin with \(\Pr(\{\Head \}) = s\), \(\Pr(\{\Tail \}) = 1 - s\)}
	\uIf(\Comment*[f]{If heads}){\(\result = \Head\)}{
	\lIf(\Comment*[f]{Go to \(v\)'s neighbor randomly}){\(d_i^{\text{out}}\neq 0\)}{\(v\gets\) \random{\(N_i^{\text{out}}\)}}
	}\Else(\Comment*[f]{If tails}){
		\(v\gets \)\random{\(\mathcal{V}\)}\Comment*[r]{reset to a uniformly random node}
	}
	\(\arrival(v)\gets \arrival(v) + 1\)\Comment*[r]{Record arrival at \(v\)}
	}
	\Return{\(\arrival\), \(v\)}\;
\end{algorithm}
\begin{note}
	The result of \hyperref[algo:random-walk-algorithm]{random walk} records the time the \hyperref[def:random-walker]{random walker} arrives \(v\) for all \(v\in \mathcal{V} \) throughout the simulation.
\end{note}

We can describe \hyperref[algo:random-walk-algorithm]{random walk} as a \hyperref[def:Markov-chain]{Markov chain}. Consider \(\left\{ X_n \right\}_{n\geq 0}\) where \(X_n\) is the location of the \hyperref[def:random-walker]{random walker} at time \(n\). We see that
\[
	\begin{split}
		X_0 & \sim \operatorname{Uniform}(\{1, 2, \dots , \left\vert \mathcal{V} \right\vert\})                                 \\
		X_1 & =\begin{dcases}
			       \operatorname{Uniform}(\{1, 2, \dots , \left\vert \mathcal{V} \right\vert \}), & \text{ with probability }1 - s; \\
			       \operatorname{NextHop}(x_0),                                                   & \text{ with probability }s,     \\
		       \end{dcases} \\
		    & \vdots                                                                                                            \\
		X_t & =\begin{dcases}
			       \operatorname{Uniform}(\{1, 2, \dots , \left\vert \mathcal{V} \right\vert\}), & \text{ with probability }1 - s; \\
			       \operatorname{NextHop}(x_{t-1}),                                              & \text{ with probability }s      \\
		       \end{dcases}
	\end{split}
\]
with \(\operatorname{NextHop}(\cdot)\) being
\[
	\operatorname{NextHop}(i)=\begin{dcases}
		\operatorname{Uniform}(\{N^{\text{out}}_i\}), & \text{ if }d^{\text{out}}_i > 0; \\
		i,                                            & \text{ if }d^{\text{out}}_i = 0.
	\end{dcases}
\]
\begin{note}
	This is \hyperref[def:irreducible]{irreducible} and \hyperref[def:aperiodic]{aperiodic}.
\end{note}

Suppose we run the \hyperref[algo:random-walk-algorithm]{random walk} for \(T\) iterations, we now analyze our first version of \hyperref[algo:random-walk-algorithm]{random walk} by \hyperref[def:Markov-chain]{Markov Chain} with the help of the \hyperref[thm:SLLN]{strong law of large numbers}. A naive estimation of \(\pi_v \) suggests by \hyperref[algo:random-walk-algorithm]{random walk} is
\[
	\hat{\pi}^{\top}_{v}(s)
	= \frac{1}{T}\sum\limits_{n=0}^{T-1} \mathbbm{1}_{\{X_n = v\}}\qquad \forall v\in \mathcal{V}.
\]
When \(T\to \infty \), we have
\[
	\lim_{T\to \infty}\frac{1}{T} \sum\limits_{n=0}^{T-1} \mathbbm{1}_{\{X_n = v\}} = \pi_v(s) \qquad \forall v\in \mathcal{V}
\]
guaranteed by \hyperref[thm:SLLN]{SLLN} almost surely.
\begin{figure}[H]
	\centering
	\incfig{Monte-Carlo-Estimator-1}
	\caption{First Estimator with \(\hat{\pi}_i(s) = 9/T\).}
	\label{fig:Monte-Carlo-Estimator-1}
\end{figure}

But this estimator has two main issues.
\begin{problem}[Speed of convergence]
Not clear how long we should run this for. Namely, we need to be able to say something about
\[
	\mathbb{E}\left[\left\vert \hat{\pi}^{\top}_v(s) - \pi_v(s) \right\vert^2 \right]
\]
in \(T\) to ensure the speed of convergence.
\end{problem}

\begin{problem}[Biased]
The estimator is a \textbf{biased} estimator. Recall that this estimator is unbiased if
\[
	\mathbb{E}\left[ \hat{\pi}_{v}^{\top}(s) \right] = \pi_v(s);
\]
and if it is biased,
\[
	\hat{\pi}_{v}^{\top}(s) - \pi_v(s)\neq 0.
\]
We see that \(\hat{\pi}^{\top}_v(s)\) is indeed biased since
\[
	\mathbb{E}\left[\hat{\pi}^{\top}_v(s)\right] = \frac{1}{T}\sum\limits_{n=0}^{T-1} \mathbb{E}\left[\mathbbm{1}_{\{X_n = v\}} \right]
	= \frac{1}{T}\sum\limits_{n=0}^{T-1} \Pr(X_n = v)
	= \frac{1}{T}\sum\limits_{n=0}^{T-1} (\mu_0 P^n)_v\underset{T\to \infty }{\to }\pi_v(s),
\]
where \(P\) is the \underline{one-step transformation matrix}. This only converges when \(T\to \infty \) by \hyperref[thm:Perron-Frobenius-theorem]{Perron-Frobenius theorem} since the \hyperref[def:aperiodic]{aperiodicity}.
\end{problem}

\subsection{Second Page Rank Estimator}\label{subsec:second-page-rank-estimator}
We now try to improve \hyperref[algo:random-walk-algorithm]{random walk} we just proposed. Recall that
\[
	\widetilde{N}(s) = sN+(1-s)\frac{1}{\left\vert \mathcal{V}  \right\vert }\vec{1}\vec{1}^{\top}.
\]
Notice that \(\pi(s)\) is the left \hyperref[def:eigenvector]{eigenvector} of \(\widetilde{N}(s)\) with \hyperref[def:eigenvalue]{eigenvalue} \(1\), i.e.,
\[
	\pi^{\top}(s)\widetilde{N}(s) = \pi^{\top}(s).
\]
Therefore,
\[
	\pi ^{\top} (s)
	= \pi^{\top}(s)sN+(1 - s)\underbrace{\pi^{\top}(s)\vec{1}}_{ = 1}\frac{1}{\left\vert \mathcal{V}\right\vert}\vec{1}^{\top}
	= \pi^{\top}(s)sN+(1 - s)\frac{1}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top},
\]
where we group \(\pi^{\top}(s)\vec{1} = 1\) because \(\pi(s)\) is a probability vector, which sums to \(1\). Then
\[
	\pi^{\top}(s)(I - sN) = (1 - s)\frac{1}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top}.
\]
Hence,
\[
	\pi^{\top}(s) = (1 - s)\frac{1}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top}(I - sN)^{-1}
	= (1 - s)\frac{1}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top}\left(\sum\limits_{i=0}^{\infty} s^i N^i\right)
	= \sum\limits_{i=0}^{\infty} s^i (1 - s)\frac{1}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top}N^i
\]
where \(N^0\coloneqq I\).
\begin{prev}
	We have \((I - A)^{-1} = I + A + A^2 + \dots\).
\end{prev}

We now analysis the expression
\[
	\pi^{\top}(s) = \sum\limits_{i=0}^{\infty} \underbrace{s^i (1 - s)}_{\tau}\frac{1}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top}N^i.
\]
Firstly, the term \(s^i(1-s) \coloneqq \tau\) is the so-called geometric \hyperref[def:random-variable]{random variable}.
\begin{intuition}[Geometric random variable]\label{int:geometric-random-variable}
	We can think of \(\tau\) as the probability of tossing a series of \hyperref[def:independent]{independent} biased coins until first tail occurs, since we have
	\[
		\Pr(\tau = 0) = 1 - s,\quad
		\Pr(\tau = 1) = s(1 - s), \dots,
		\Pr(\tau = i) = s^i(1 - s).
	\]
\end{intuition}
\begin{remark}
	We see that reset happens at time \(N\), hence we look at time \(N-1\).
\end{remark}

Next, we analyze the term \(\frac{1}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top}N^i\), where
\begin{itemize}
	\item \(u = \frac{1}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top}\) is the uniform distribution on the nodes of the graph.
	\item \(N^i\) represents random walk repeated \(i\) times.
\end{itemize}

\begin{intuition}
	In all, this term represents that we start at a uniformly random node and take \(i\) steps as per random walk. Namely, this term represents the distribution of locations of the random walk after \(i\) steps.
\end{intuition}

Now, the suggested algorithm becomes follows.

\begin{algorithm}[H]\label{algo:Monte-Carlo-algorithm-2}
	\DontPrintSemicolon{}
	\caption{Estimate Page Rank ver.2}
	\SetKwData{arrival}{arrival}
	\KwData{A network \(\mathcal{G} = (\mathcal{V} , \mathcal{E} )\), iteration \(K\), \(s\)}
	\KwResult{A list \arrival}
	\SetKwFunction{RW}{\hyperref[algo:random-walk-algorithm]{RandomWalk}}
	\SetKwFunction{random}{random}
	\SetKwFunction{geo}{\hyperref[int:geometric-random-variable]{GeoRandom}}

	\BlankLine

	\For(\Comment*[f]{Initialize}){\(v\in \mathcal{V} \)}{
		\(\arrival(v)\gets 0\)\;
	}
	\;
	\For(){\(i = 1, \dots, K\)}{
		\(\tau \gets \) \geo{\(s\)}\Comment*[r]{Toss a \hyperref[int:geometric-random-variable]{geometric random variable}}
		\(v\gets \)\random{\(\mathcal{V}\)}\Comment*[r]{select a uniformly random node}
		\(\_, v\gets\)\RW{\(v\), \(\tau \), \(s\)}\Comment*[r]{\hyperref[algo:random-walk-algorithm]{Random walk} starting at \(v\) with \(\tau \) steps}
		\(\arrival(v)\gets \arrival(v) + 1\)\Comment*[r]{Record where the \hyperref[def:random-walker]{random walker} ends up at}
	}
	\Return{\arrival}\;
\end{algorithm}
\begin{note}
	The result of \hyperref[algo:Monte-Carlo-algorithm-2]{the above algorithm} records the time the \hyperref[def:random-walker]{random walker} \textbf{ends up} at \(v\) for all \(v\in \mathcal{V} \) in every \hyperref[algo:random-walk-algorithm]{random walk}.
\end{note}

Denote those \(K\) \hyperref[def:random-variable]{random variables} \(\tau\) as \(\tau_1, \tau_2, \dots , \tau_K\) and the associated \(X_{\tau}\) as \(X_{\tau_1}, X_{\tau_2}, \dots , X_{\tau_K}\). We see that
\[
	\hat{\pi}_v^K(s) = \frac{1}{K}\sum\limits_{k=1}^{K} \mathbbm{1}_{\{X_{\tau_k} = v\}}.
\]
This is an unbiased estimator since
\[
	\mathbb{E}\left[\mathbbm{1}_{\{X_{\tau_k}=v\}}\right] = \Pr(X_{\tau_k} = v) = \pi_v(s).
\]
Also, the variance can be derived as
\[
	\Var\left[\hat{\pi}_v^K(s) \right] = \frac{\pi_v(s)(1 - \pi_v(s))}{K}\leq \frac{1}{4K}.
\]

\begin{remark}
	Since \(n(1-n)\leq 1/4\) for every \(n\in[0, 1]\) with equality if \(x = 1/2\).
\end{remark}

\begin{figure}[H]
	\centering
	\incfig{Monte-Carlo-Estimator-2}
	\caption{Second Estimator with \(X_{\tau-1}\sim\pi(s)\)}
	\label{fig:Monte-Carlo-Estimator-2}
\end{figure}

\begin{note}
	We note that the empirical mean is unbiased while the empirical variance is biased. Let \(X_1, X_2, \dots  ,X_N\) are \hyperref[def:i.i.d.]{i.i.d.} with mean \(\mu\) and variance \(\sigma^2\).
	\begin{itemize}
		\item\(\hat{\mu}_N\) is the empirical mean which is defined as
		      \[
			      \hat{\mu}_N \coloneqq  \frac{1}{N}\sum\limits_{i=1}^{N} X_i
		      \]
		      with
		      \[
			      \mathbb{E}\left[\hat{\mu}_N \right] = \frac{1}{N}\sum\limits_{i=1}^{N} \mathbb{E}\left[X_i \right] = \frac{1}{N}\sum\limits_{i=1}^{N} \mu = \mu,
		      \]
		      which implies this is an unbiased estimator.
		\item Denote \(\hat{\Var}(N)\) as the empirical variance estimator. It's defined as
		      \[
			      \hat{\Var}(N) \coloneqq \frac{1}{N}\sum\limits_{i=1}^{N} (X_i - \hat{\mu}_N)^2.
		      \]
		      This is indeed a biased estimator since
		      \[
			      \mathbb{E}\left[\hat{\Var}(N) \right]  = \left(\frac{N-1}{N}\right) \sigma^2,
		      \]
		      which is slightly less than the true variance. We then have an unbiased estimator defined as
		      \[
			      \overline{\Var}(N) \coloneqq  \frac{1}{N-1}\sum\limits_{i=1}^{N} (X_{i-\hat{\mu}_N})^2
		      \]
		      with \(\mathbb{E}\left[\overline{\Var}(N) \right]  = \sigma^2\).
	\end{itemize}
\end{note}

\subsection{Third Page Rank Estimator}\label{subsec:third-page-rank-estimator}
Recall that
\[
	\pi(s) = \frac{1 - s}{\left\vert \mathcal{V}  \right\vert}\vec{1}^{\top} (I - sN)^{-1}.
\]
Now, we define
\[
	Z \coloneqq (I - sN)^{-1} = \sum\limits_{i=0}^{\infty} s^i N^i
\]
with \(s^0\coloneqq 1\) and \(N^0\coloneqq I\). \(Z\) is a non-negative matrix since \(Z_{ij}\geq 0\). We further define \(\sigma_{ij}\coloneqq (1 - s)Z_{ij}\) with a matrix \(\Sigma \) whose entries are defined as \(\left(\Sigma\right)_{ij}\coloneqq \sigma_{ij}\) and \(\sigma_{i\cdot}\) is the row of \(\Sigma\) corresponding to node \(i\).

\begin{remark}
	We see that \(\lambda _{i\cdot}\) is the page rank corresponding to the reset distribution where we always reset to node \(i\).
\end{remark}

Then, we proposed the following algorithm.

\begin{algorithm}[H]\label{algo:Monte-Carlo-algorithm-3}
	\DontPrintSemicolon{}
	\caption{Estimate Page Rank ver.3}
	\SetKwData{arrival}{arrival}
	\KwData{A network \(\mathcal{G} = (\mathcal{V} , \mathcal{E} )\), iteration \(M\), \(s\)}
	\KwResult{A list \arrival}
	\SetKwFunction{RW}{\hyperref[algo:random-walk-algorithm]{RandomWalk}}
	\SetKwFunction{geo}{\hyperref[int:geometric-random-variable]{GeoRandom}}

	\BlankLine

	\For(\Comment*[f]{Initialize}){\(v\in \mathcal{V} \)}{
		\(\arrival(v)\gets 0\)\;
	}
	\;
	\For(){\(i\in \mathcal{V}\)}{
		\For(){\(j\in 1, \dots, M\)}{
			\(\tau^i \gets \) \geo{\(s\)}\Comment*[r]{Toss a \hyperref[int:geometric-random-variable]{geometric random variable}}
			\(\_, v \gets\)\RW{\(i\), \(\tau ^i\) }\Comment*[r]{\hyperref[algo:random-walk-algorithm]{random walk} starting at \(i\) with steps \(\tau ^i\)}
			\(\arrival(v)\gets \arrival(v) + 1\)\Comment*[r]{Record where the \hyperref[def:random-walker]{random walker} ends up at}
		}
	}
	\Return{\arrival}\;
\end{algorithm}
\begin{note}
	The result of \hyperref[algo:Monte-Carlo-algorithm-3]{the above algorithm} records the time the \hyperref[def:random-walker]{random walker}
	\textbf{ends up} at \(v\) for all \(v\in \mathcal{V} \) in every \hyperref[algo:random-walk-algorithm]{random walk}.
\end{note}

If we only take one walk, namely \(M = 1\), then
\[
	\pi_v(s) = \frac{1}{\left\vert \mathcal{V}  \right\vert }\sum\limits_{i\in\mathcal{V}}\sigma_{iv}(s).
\]

Now, for every node \(i\) in the network, we run \(M\) times with \(\tau^i_1, \tau^i_2, \dots , \tau^i_M\) and \(X_{\tau_1}^i, X_{\tau_2}^i, \dots , X_{\tau_M}^i\) and count the number of arrivals in node \(v\) and find the average. Explicitly, we have
\[
	\hat{\pi}_v(s)
	= \frac{1}{\left\vert \mathcal{V} \right\vert}\sum\limits_{i=1}^{\left\vert \mathcal{V}\right\vert} \frac{1}{M}\sum\limits_{m=1}^{M} \mathbbm{1}_{\left\{X^i_{\tau^i_m} = v\right\}}
	= \frac{1}{\left\vert \mathcal{V}  \right\vert M}\sum\limits_{i=1}^{\left\vert \mathcal{V} \right\vert} \sum\limits_{m=1}^{M} \mathbbm{1}_{\left\{X^i_{\tau^i_m} = v\right\}}.
\]

This is another unbiased estimator for the same reason given in the \hyperref[subsec:second-page-rank-estimator]{second estimator}, but we see that this converges faster and with smaller variance. Specifically, we have
\[
	\Var\left[\hat{\pi}_v(s) \right]  = \frac{1}{\left\vert \mathcal{V}  \right\vert M}\left(\pi_{j} - \frac{\sum_{i=1}^{\left\vert \mathcal{V}  \right\vert} \sigma_{ij}^2}{\left\vert \mathcal{V}  \right\vert}\right)
\]
for all \(v\in \mathcal{V} \). Hence, this is a better estimator.

\begin{figure}[H]
	\centering
	\incfig{Monte-Carlo-Estimator-3}
	\caption{Third Page Rank Estimator with \(X^i_{\tau_j - 1}\sim\sigma_i(s)\)}
	\label{fig:Monte-Carlo-Estimator-3}
\end{figure}

\begin{problem}
Although the estimator found by \hyperref[algo:Monte-Carlo-algorithm-3]{this algorithm} is better than the other two, but it still has its flaws.  The main thing is, it throws away so much data, which means, which means it will converge slower.
\end{problem}