\lecture{15}{27 Oct.\ 12:30}{Final Page Rank Estimator}
\begin{prev}
	Main idea is that to construct a \hyperref[def:Markov-chain]{Markov chain} on nodes of the \hyperref[def:graph]{graph}. At none \(i\), the \hyperref[def:random-walker]{random walker} toss a biased coin and determine where to go.
\end{prev}

\subsection{Forth Page Rank Estimator}
We introduce the final estimator, which make the full use of the data given, which avoids the problem we faced when using \hyperref[subsec:third-page-rank-estimator]{the third estimator}.

Formally, to further utilize the data thrown away by \hyperref[algo:Monte-Carlo-algorithm-3]{the previous algorithm}, we just record more data as follows.

\begin{algorithm}[H]\label{algo:Monte-Carlo-algorithm-4}
	\DontPrintSemicolon{}
	\caption{Estimate Page Rank ver.4}
	\SetKwData{t}{t}
	\KwData{A network \(\mathcal{G} = (\mathcal{V} , \mathcal{E} )\), iteration \(M\), \(s\)}
	\KwResult{A list \(N_t\)}
	\SetKwFunction{RW}{\hyperref[algo:random-walk-algorithm]{RandomWalk}}
	\SetKwFunction{geo}{\hyperref[int:geometric-random-variable]{GeoRandom}}
	\SetKwData{arrival}{arrival}

	\BlankLine

	\For(\Comment*[f]{Initialize}){\(v\in \mathcal{V} \)}{
		\For(){\(t\in 1, \dots M \times \left\vert \mathcal{V}  \right\vert  \)}{
			\(N_t(v)\gets 0\)\;
		}
	}
	\(\t\gets 0\)\;
	\;
	\For(){\(i\in \mathcal{V}\)}{
		\For(){\(j\in 1, \dots, M\)}{
			\(\tau^i \gets \) \geo{\(s\)}\Comment*[r]{Toss a \hyperref[int:geometric-random-variable]{geometric random variable}}
			\(\arrival, v \gets\)\RW{\(i\), \(\tau ^i\) }\Comment*[r]{\hyperref[algo:random-walk-algorithm]{random walk} starting at \(i\) with steps \(\tau ^i\)}
			\For(\Comment*[f]{Record every node the \hyperref[def:random-walker]{random walker} has been to}){\(k\in \mathcal{V} \)}{
				\(N_t(k)\gets N_t(k) + \arrival(k)\)\;
			}
			\(\t\gets \t + 1\)\;
		}
	}
	\Return{\(N_t\)}\;
\end{algorithm}
\begin{intuition}
	We see that this \hyperref[algo:Monte-Carlo-algorithm-4]{algorithm} just record every information about the \hyperref[def:random-walker]{random walker}! We see that it utilize the data in the most efficient way.
\end{intuition}

In this case, we don't need to analyze this estimator, since there is an off-the-shell theorem we can use, \href{https://en.wikipedia.org/wiki/Renewal_theory}{Renewal theorem}.
\begin{theorem}[Renewal Theorem]\label{thm:Renewal-theorem}
	We have
	\[
		\pi_v(s) = \frac{\mathbb{E}\left[\# \text{ of visits to node \(v\) in this reset} \right]}{\mathbb{E}\left[\text{Duration of reset} \right] }
	\]
	for every \(v\), where we have \(\mathbb{E}\left[\text{Duration of reset} \right] =\frac{1}{1-s}\), which implies
	\[
		\hat{\pi}_v(s) = \frac{\frac{1}{T}\sum_{i=1}^{T} N_t(k)}{\frac{1}{1-s}} = \frac{1-s}{T}\sum\limits_{t=1}^{T} N_t(v)
	\]
	where \(N_t(v)\) is the number of time we visited \(v\) in reset duration.
\end{theorem}
\begin{note}
	Note that \(T\) is \(M\times \left\vert \mathcal{V}  \right\vert \) since there are these many reset duration in the \hyperref[algo:Monte-Carlo-algorithm-4]{algorithm}. Also, we see that each reset duration is independent of the other \(T\) rest duration starting at uniformly chosen node.
\end{note}

We see that this is also a non-biased estimator, and also, the \hyperref[def:random-variable]{random variables} sequence \(\{N_t(v)\}_t\) is \hyperref[def:i.i.d.]{i.i.d}.

\begin{remark}
	This estimator does not throw out those useful data, hence the estimator computed by this algorithm converges faster.
\end{remark}

\begin{figure}[H]
	\centering
	\incfig{Monte-Carlo-Estimator-4}
	\caption{Forth Page Rank Estimator which records \(N(v)\) for every node.}
	\label{fig:Monte-Carlo-Estimator-4}
\end{figure}

\chapter{Random Graph}
We are interested in some functions depend on the number of nodes \(n\) in the \hyperref[def:graph]{graph} such that \(\lim\limits_{n\to \infty }f(n)\) can be understood, namely we are interested in so-called \emph{large \hyperref[def:graph]{graph} property} since nearly all networks we have in practice are large. Furthermore, by \autoref{thm:SLLN}, if we assume that the real-work network is formed following some underlying distribution, we can then study some nice random \hyperref[def:graph]{graph} models and use it to help us understand the world.

\section{Order Relationship of Functions}
We first introduce some common notations for asymptomatic comparison.
\begin{definition*}
	Given \(f(n)\) and \(g(n)\), the following notion compare \(f\) and \(g\) \emph{loosely}.
	\begin{definition}[Big-\(O\)]
		\(f(n)\) is said to be \(O(g(n))\) if there is a positive \(k\) such that
		\[
			f(n)\leq k\cdot g(n)
		\]
		for all large enough \(n\).
	\end{definition}

	\begin{definition}[Big-\(\Omega\)]
		\(f(n)\) is said to be \(\Omega(g(n))\) if there is a positive \(k\) such that
		\[
			f(n) \geq k\cdot g(n)
		\]
		for all large enough \(n\).
	\end{definition}

	\begin{definition}[Big-\(\Theta \)]
		\(f(n)\) is said to be \(\Theta(g(n))\) if there are positive \(k_1\leq k_2\) such that
		\[
			k_1\cdot g(n)\leq f(n) \leq k_2\cdot g(n)
		\]
		for all large enough \(n\).
	\end{definition}
\end{definition*}

\begin{definition*}
	Given \(f(n)\) and \(g(n)\), the following notion compare \(f\) and \(g\) \emph{strictly}.
	\begin{definition}[Small-\(o\)]
		\(f(n)\) is said to be \(o(g(n))\) if for every positive \(\epsilon\), there exists an \(N\) large enough such that
		\[
			f(n) \geq \epsilon\cdot g(n)
		\]
		for all \(n\geq N\).
	\end{definition}

	\begin{definition}[Small-\(\omega \)]
		\(f(n)\) is said to be \(\omega(g(n))\) if for every positive \(\epsilon\), there exists an \(N\) large enough such that
		\[
			f(n) \leq \epsilon\cdot g(n)
		\]
		for all \(n\geq N\).
	\end{definition}

	\begin{definition}[Same order]\label{def:same-order}
		\(f(n)\) is said to be of the \emph{same order} as \(g(n)\) asymptotically if
		\[
			\lim_{n\to \infty } \frac{f(n)}{g(n)} = 1.
		\]
		We can write \(f(n)\sim g(n)\) for short in this case.
	\end{definition}
\end{definition*}

\begin{remark}
	If \(f(n)\) or \(g(n)\) are not non-negative, we simply use \(\left\vert f(n) \right\vert \) and \(\left\vert g(n) \right\vert \) and apply the above definitions.
\end{remark}

We first consider the size of the largest connected component, and see how does this relation behave as \(n\to \infty\). Formally, for a function \(f(n)\) which represent the size of a graph, where \(n\) is the number of nodes in the graph.
\begin{eg}[Island]
	If \(f(n) = \log n\), we'll see small islands with \(\frac{n}{\log(n)}\) of them without big continent. This is because
	when \(n\to \infty \), \(\frac{n}{\log(n)}\to 0\).
	\begin{center}
		\incfig{random-graph-logn}
	\end{center}
\end{eg}

\begin{eg}
	If \(f(n) = n^c\) with \(c<1\), we'll see a continent but'll shrink as \(n\to \infty\). This is because that \(\frac{n^c}{n}\to 0\) for \(c<1\) as \(n\to \infty \). Better than island but not that great.
	\begin{center}
		\incfig{random-graph-n^c}
	\end{center}
\end{eg}

\begin{eg}[Giant component]
	If \(f(n) = cn\), there will be a giant component roughly \(c\) percent of nodes in the \hyperref[def:graph]{graph}. Since as \(n\to \infty \), \(\frac{cn}{n}\to c\).
	\begin{center}
		\incfig{random-graph-cn}
	\end{center}
\end{eg}
