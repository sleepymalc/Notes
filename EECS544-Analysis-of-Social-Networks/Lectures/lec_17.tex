\lecture{17}{3 Nov.\ 12:30}{Random Graph}
\section{Real World Graphs}\label{sec:real-world-graphs}
In this section, we compare the random \hyperref[def:graph]{graph} models with the real-world \hyperref[def:graph]{graphs}.
\begin{remark}[Dense and sparse]
	Generally, we say a \hyperref[def:graph]{graph} is sparse when the number of edges is linear with respect to \(V\), namely
	\[
		E\sim \Theta(V).
	\]
	And we say a \hyperref[def:graph]{graph} is dense when the number of edges is quadratic with respect to \(V\), namely
	\[
		E\sim \Theta(V^2),
	\]
	note that this case is not so common. We can also see this by considering the number of triangles. In the connectivity regime, the number of edges is
	\[
		\Theta(V\log V),
	\]
	which is small relative to what's observed in real-world \hyperref[def:graph]{graphs}.
\end{remark}

In practice, a real-world graph will have the following properties:
\begin{enumerate}
	\item The number of edges is linear in \(V\).
	\item \hyperref[def:giant-component]{Giant component} usually entice graph(connectivity)
	\item Lots of triangles
\end{enumerate}

Compare to the real-world \hyperref[def:graph]{graphs}, the \hyperref[sec:Erdos-Renyi-random-graphs-family]{Erdős-Rényi random graphs family} usually have the following properties, for each different settings:
\begin{itemize}
	\item \hyperref[subsec:extremely-sparse-regime]{Extremely sparse regime}:
	      \begin{itemize}
		      \item edges is linear in \(V\)
		      \item \hyperref[def:giant-component]{giant component}
		      \item isolated nodes
		      \item no triangles as \(V\to \infty \)
	      \end{itemize}
	\item \hyperref[subsec:sparse-regime]{Sparse regime}:
	      \begin{itemize}
		      \item edges is \(\sim \Theta(V\log V)\)
		      \item most nodes in the \hyperref[def:giant-component]{giant component}
		      \item can have connectivity
		      \item Many triangles
	      \end{itemize}
	\item \hyperref[subsec:extremely-dense-regime]{Extremely dense regime}:
	      \begin{itemize}
		      \item edges is \(\sim \Omega(V^{1+\epsilon})\) with \(\epsilon>0\)
		      \item fully \hyperref[def:connected]{connected}
		      \item lots of triangles
	      \end{itemize}
\end{itemize}

\section{R-MAT Graphs}\label{sec:R-MAT-graphs}
Besides \hyperref[sec:Erdos-Renyi-random-graphs-family]{Erdős-Rényi random graph family}, we can use different approaches to generate a random \hyperref[def:graph]{graph}. We now introduce so-called \emph{R-MAT graphs}.

\begin{intuition}
	The main idea is to \underline{recursively set an entry of the \hyperref[def:adjacency-matrix]{adjacency matrix}}.
\end{intuition}

We assume that the number of nodes is \(V = 2^k\) for some \(k\). Now, given four numbers
\(a, b, c, d \geq 0\) with \(a+b+c+d = 1\), we let
\[
	\begin{bmatrix}
		a & b \\
		c & d \\
	\end{bmatrix}
\]
be a distribution on \(\{1, 2, 3, 4\}\). Further, we denote
\begin{itemize}
	\item \(E\): Target number of edges to be placed on a \hyperref[def:directed-graph]{directed graph}. Furthermore, we add entries in the
	      \hyperref[def:adjacency-matrix]{adjacency matrix} one edge of a time.
	\item \(A\): \(2^k \times 2^k\) matrix such that
	      \[
		      A = \substack{2^{k - 1}\\ \\ \\ \\ 2^{k-1}} \overset{\ 2^{k - 1}\ \ \ \ \ 2^{k-1}}{\begin{bmatrix}
				       & A_1 &  & A_2 & \\
				       &     &  &     & \\
				       & A_3 &  & A_4 & \\
			      \end{bmatrix}}_{2^k\times 2^k}
	      \]
\end{itemize}

Then the algorithm to generate an R-MAT graph is as follows.

\begin{algorithm}[H]\label{algo:R-MAT-graph-generator-algo}
	\DontPrintSemicolon{}
	\caption{R-MAT Graph Generator}
	\KwData{\((a, b, c, d)\), \(V = 2^k\)}
	\KwResult{\hyperref[def:adjacency-matrix]{Adjacency matrix} \(A\)}
	\SetKwFunction{rand}{rand}
	\SetKwData{r}{r}
	\SetKwData{region}{region}
	\BlankLine

	\(A_{V\times V} = 0_{V \times V}\) \Comment*[r]{Initialize}
	\;
	\For(){\(e= 1, \dots , E\)}{
		\(A^\prime \gets A\)\;
		\For(\Comment*[f]{Iterated backwards, choose \(A_{2^l \times 2^l}\) with size \(2^{l-1} \times  2^{l-1}\)}){\(\ell = k, \dots , 2\)}{
			\(\r\gets \)\rand{\([0, 1]\)} \;
			\uIf(\Comment*[f]{\(1\sim a\)}){\(r\leq a\) }{
				\(\region\gets 1\)\;
			}\uElseIf(\Comment*[f]{\(2\sim b\)}){\(a < r \leq  a + b\)}{
				\(\region\gets 2\)\;
			}\uElseIf(\Comment*[f]{\(3\sim c\)}){\(a + b < r \leq  a + b + c\)}{
				\(\region\gets 3\)\;
			}\Else(\Comment*[f]{\(4\sim d\)}){
				\(\region\gets 4\)\;
			}
			\(A^\prime \gets A^\prime_i\)\Comment*[r]{Choose the region \(i\)}
		}
		\If(){\(A^\prime = \begin{bmatrix}0 \\\end{bmatrix}\)}{
			\(A^\prime = \begin{bmatrix}1 \\ \end{bmatrix}\)\Comment*[r]{Note that this also changes \(A\)'s corresponding entry}
		}
	}
	\;
	\For(\Comment*[f]{Clean of to make \(A\) a \hyperref[def:simple-graph]{simple} \hyperref[def:directed-graph]{directed graph}}){\(i = 1, \dots, V\)}{
		\(A_{ii} = 0\)\label{algo:R-MAT-graph-generator-algo-clean-up}
	}
	\Return{\(A\)}\;
\end{algorithm}

\begin{eg}
	In picture, \hyperref[algo:R-MAT-graph-generator-algo]{R-MAT algorithm} is doing the following:
	\[
		M = \left(
		\begin{array}{c|c}
				\begin{array}{cc}
					 & \\
					 &
				\end{array} & \begin{array}{c|c}
					              \begin{array}{c|c}
						      & \\
						\hline
						\cdot &
					\end{array} &    \\
					              \hline
					                                 &
				              \end{array} \\
				\hline
				\begin{array}{cc}
					 & \\
					 &
				\end{array} & \begin{array}{cc}
					               & \\
					               &
				              \end{array}
			\end{array}
		\right),
	\]
	where we choose \(k = 3\) with the sequence \(2, 1, 3\).
\end{eg}

Note that in \autoref{algo:R-MAT-graph-generator-algo-clean-up}, we delete any entries on the diagonal and produce the \hyperref[def:adjacency-matrix]{adjacency matrix} \(A\) to produce a \hyperref[def:simple-graph]{simple} \hyperref[def:directed-graph]{directed graph}.
\begin{eg}[Clean up]
	Let \(A\) be
	\[
		A = \begin{pmatrix}
			0 & 0 & 0 & 1 \\
			1 & 0 & 0 & 0 \\
			0 & 0 & 1 & 0 \\
			0 & 1 & 0 & 1 \\
		\end{pmatrix},
	\]
	which has self-loop hence not \hyperref[def:simple-graph]{simple}. After cleaning up, we have
	\[
		A^\prime = \begin{pmatrix}
			0 & 0 & 0 & 1 \\
			1 & 0 & 0 & 0 \\
			0 & 0 & 0 & 0 \\
			0 & 1 & 0 & 0 \\
		\end{pmatrix}.
	\]
\end{eg}

\begin{remark}
	We can do some comparison between what we have already seen.
	\begin{itemize}
		\item \hyperref[sec:Erdos-Renyi-random-graphs-family]{Erdős-Rényi Random Graph}: Poisson \hyperref[def:degree]{degree} distribution with \(p = \frac{c}{V}\). Then the tail probability (CCDF) is
		      \[
			      \Pr(\deg (i)\geq k)\cong c e^{-\alpha k},
		      \]
		      which decreases exponentially.
		\item \hyperref[sec:real-world-graphs]{Real-World graphs}. Tail probability is
		      \[
			      \Pr(\deg(i) \geq k) = \frac{c}{k^{\alpha - 1}},
		      \]
		      which is polynomial decrease if \(\alpha\in(2, 3)\). More specifically,
		      \begin{itemize}
			      \item \(\alpha>2\):
			            \begin{itemize}
				            \item mean \hyperref[def:degree]{degree} exists, such that
				                  \[
					                  \text{mean \hyperref[def:degree]{degree} }\cong \sum\limits_{k=1}^{\infty} k \frac{\widetilde{c}}{k^{\alpha}} = \sum\limits_{k=1}^{\infty} \frac{\widetilde{c}}{k^{\alpha - 1}}.
				                  \]
				                  We see that \(\alpha - 1 > 1\) is needed for above to converge to a finite number.
				            \item mean squared of \hyperref[def:degree]{degree} exists, such that
				                  \[
					                  \text{mean squared of \hyperref[def:degree]{degree} }\cong \sum\limits_{k=1}^{\infty} k^2 \frac{\widetilde{c}}{k^{\alpha}} = \sum\limits_{k=1}^{\infty} \frac{\widetilde{c}}{k^{\alpha-2}}.
				                  \]
			            \end{itemize}
			      \item \(\alpha<3\): This implies \(\alpha - 2 < 1\), hence above will just diverge to \(+\infty\).
		      \end{itemize}
		\item In practice, the variance of \hyperref[def:degree]{degrees} is exponentially high.
	\end{itemize}
\end{remark}

\section{Preferential Attachment Directed Graph}
Yet, we have another algorithm to produce a random \hyperref[def:graph]{graph}. Let the (\hyperref[def:undirected-graph]{undirected}) \hyperref[def:degree]{degree} distribution on \(V\) being
\[
	d_1, d_2, \dots , d_V
\]
with \(\sum\limits_{i=1}^{V} d_i\) being even. Then by considering a histogram, for \(k = \{0, 1, \dots , V - 1\}\), we have
\[
	P_k^{\alpha} = \frac{1}{V}\sum\limits_{i=1}^{V} \mathbbm{1}_{\{d_i = k\}} = \frac{\#\text{ of nodes with \hyperref[def:degree]{degree} }k}{V}.
\]

We make the following assumptions for constructing a preferential attachment \hyperref[def:graph]{graph}.
\begin{itemize}
	\item Nodes arrive in sequence \(1, 2, \dots , V\)
	\item Each node has an \hyperref[def:out-degree]{out-degree}. Out going edges that need to be paired with nodes earlier in sequence such that
	      \[
		      d_{i}^{\text{out}} \leq i - 1.
	      \]
\end{itemize}

Now, with \(p\in(0, 1)\), the following algorithm will generate a preferential attachment \hyperref[def:graph]{graph}.

\begin{algorithm}[H]\label{algo:preferential-attachment-graph-generator-algo}
	\DontPrintSemicolon{}
	\caption{Preferential Attachment Graph Generator}
	\KwData{\(p\in (0, 1)\), \(V\), \(\{d_i\}_{i=1}^V\)}
	\KwResult{\(\mathcal{G} \)}
	\SetKwFunction{TC}{TossCoin}
	\SetKwFunction{rand}{rand}
	\SetKwFunction{randW}{randWithWeight}
	\SetKwData{result}{result}
	\SetKwData{up}{unpaired}
	\SetKwData{region}{region}
	\BlankLine

	\(\mathcal{V} \gets \{1, 2, \dots , V\}\)\;
	\(\mathcal{E} \gets \varnothing\)\;
	\(\mathcal{G} \gets (\mathcal{V} , \mathcal{E} )\) \Comment*[r]{Initialize}
	\;
	\For(\Comment*[f]{Assume nodes arrive in order}){\(i = 1, \dots  , V\)}{
	\(\up\gets \{1, 2, \dots , i-1 \}\)\Comment*[r]{Unpaired nodes earlier to \(i\)}
	\For(){\(e = 1, \dots, d^{\text{out}}_i\)}{
	\(\result\gets\) \TC{\(p\) }\Comment*[r]{\(\Pr(\{\Head \})=p \), \(\Pr(\{\Tail \})\eqqcolon q = 1 - p\)}
	\uIf(\Comment*[f]{If heads}){\(\result = \Head\)}{
		\(j\gets\)\rand{\(\up\) }
	}\Else(\Comment*[f]{If tails}){
	\(j\gets \)\randW{\(\up\), \(\{d_k^{\text{in}}\}_{k\in \up}\) }\footnote{In this case, we choose one of the unpaired nodes in \textbf{proportion} to their incoming edges.}\;
	}
	\;
	\(\mathcal{E} \gets \mathcal{E} + \{i \to j\}\)\Comment*[r]{Add the edge \(i\to t\) to \(\mathcal{G} \)}
	\(\up\gets \up \setminus \{j\}\)\Comment*[r]{\(j\) is now paired with \(i\)}
	}
	}

	\Return{\(\mathcal{G} \)}\;
\end{algorithm}

\begin{remark}
	Node \(1 \) always has \hyperref[def:out-degree]{out-degree} \(0\).
\end{remark}

\section{Analysis of Preferential Attachment Directed Graph}
Now, let us analyze the situation such that if \(d_{i}^{\text{out}} = 1\) for all \(i = 2, 3, \dots \) with \(t = 2, 3, \dots , V\). Define
\[
	X_i(t)\coloneqq \#\text{ of incoming edges for node \(i\) after node \(t\) is added to the \hyperref[def:graph]{graph}}.
\]
We then see
\[
	\sum\limits_{i=1}^{V} X_i(t) = t - 1 = \sum\limits_{i=1}^{t - 1} X_{i}(t)
\]
since \(X_i(t) = 0\) for all \(i\) such that \(t\leq i\leq V\). Look at the change of \(X_{i}(t)\) with respect to \(X_i(t - 1)\), we further have
\[
	X_{j}(t) = \begin{dcases}
		X_{j}(t - 1) = 0, & \text{ if }t\leq j\leq V;                                                 \\
		X_{j}(t - 1),     & \text{ w.p. } p \frac{t - 2}{t - 1}+q \frac{t - 1 - X_{j}(t - 1)}{t - 1}; \\
		X_{j}(t - 1)+1    & \text{ w.p. } p \frac{1}{t - 1} + q \frac{X_{j}(t - 1)}{t - 1}.           \\
	\end{dcases}
\]

\begin{remark}
	This is a vector valued \hyperref[def:stochastic-process]{random process}.
\end{remark}

In order to simply the analysis, we take a look at the expectation. We have \(x_{j}(t) = \mathbb{E}\left[X_{j}(t) \right] \), which is just
\[
	x_{j}(t) = \begin{dcases}
		x_{j}(t - 1) = 0,                                                & \text{ if }t\leq j\leq V; \\
		x_{j}(t - 1) + p \frac{1}{t - 1} + q \frac{x_{j}(t - 1)}{t - 1}, & \text{ otherwise}.
	\end{dcases}
\]
Then, we have
\[
	\underbrace{x_{j}(t) - x_{j}(t - 1)}_{\text{discrete derivative}} = \begin{dcases}
		0,                                              & \text{ if }t\leq j\leq V; \\
		\frac{p}{t - 1} + q \frac{x_{j}(t - 1)}{t - 1}, & \text{ otherwise}.
	\end{dcases}
\]

Let \(V\to \infty \) and \(\widetilde{x}_i(t) = \frac{x_{i}(t)}{V}\cong \frac{X_{i}(t)}{V}\) will be well-approximated by a differential equation
\[
	\frac{\mathrm{d}\widetilde{x}_j(t)}{\mathrm{d}t} = \frac{p}{t} + q \frac{\widetilde{x}_j(t)}{t} = \frac{p + q \widetilde{x}_j(t)}{t}.
\]
Rearranging, we have
\[
	\frac{1}{p+q \widetilde{x}_j(t)}\frac{\mathrm{d}\widetilde{x}_j(t)}{\mathrm{d}t} = \frac{1}{t}.
\]
Then since
\[
	\frac{\mathrm{d}}{\mathrm{d}t}\left(\ln(p + q \widetilde{x}_j(t))\right) = \frac{1}{p + q \widetilde{x}_j(t)}\cdot q \frac{\mathrm{d}\widetilde{x}_j(t)}{\mathrm{d}t},
\]
hence we have
\[
	\frac{\mathrm{d}}{\mathrm{d}t}\left(\ln(p + q \widetilde{x}_j(t))\right) = \frac{q}{t},
\]

In all, we have
\[
	\ln(p + q \widetilde{x}_j(t)) = q \ln(t) + qc.
\]
Take exponential on both sides, we have \(p + q \widetilde{x}_j(t) = t^q e^{cq}\).  Solving for \(\widetilde{x}_j\), we see that
\[
	\widetilde{x}_j(t) = \frac{t^q e^{cq} - p}{q}.
\]

We now need the initial value. In this case, we have \(x_{j}(t) = 0\), \(\forall\ t\leq j\leq V\), which implies \(\widetilde{x}_j(j) = 0\), and hence
\[
	\begin{split}
		\widetilde{x}_j(j) = 0
		 & \iff j^q e^{cq} = p                                                                                   \\
		 & \implies e^{cq} = \frac{p}{j^q}                                                                       \\
		 & \implies \widetilde{x}_j(t) = \frac{p}{q}\left(\left(\frac{t}{j}\right)^q - 1\right)\forall\ t\geq j.
	\end{split}
\]

From this, we see that the fraction of nodes with \hyperref[def:degree]{degree} greater than \(k\) is
\[
	\frac{1}{\left(1 + \frac{q}{p}\frac{k}{V}\right)^{\frac{1}{q}}} \cong \frac{c}{k^{\alpha}}
\]
where \(\alpha = \frac{1}{q} = \frac{1}{1 - p}\).

\begin{note}[Power-tail law]
	This is what we called \emph{power law tail}.
\end{note}

\begin{remark}
	This process is biasing towards incoming degrees.
\end{remark}

\begin{note}[Formal Analysis]
	A more formal setup is the following. Let \(\left\{X_j(t)\right\}_{j = 1, \dots , V}\) be a \hyperref[def:Markov-chain]{Markov chain}. Considering two aggregations:
	\begin{enumerate}
		\item For \(i = 0, 1, \dots \), we determine the fraction of nodes with \hyperref[def:in-degree]{in-degree} at least \(i\) at time \(t\). Then
		      \[
			      y_{i}(t) = \frac{1}{V}\sum\limits_{k=1}^{V} \sum\limits_{j=1}^{V} \mathbbm{1}_{\{ X_{j}(t) = k \}} = \frac{1}{V}\sum\limits_{j=1}^{V} \mathbbm{1}_{\{ X_{j}(t)\geq i \}}.
		      \]
		\item For \(s\in[0, 1]\), define \(t = \left\lfloor sV\right\rfloor \). Now, for \(i = 0, 1, \dots \), set \(\widetilde{y}^V_t(s) = y_{i}(\left\lfloor sV\right\rfloor )\). Then the formal comparison of \(\widetilde{y}^V_i(s)\) can be made with an appropriate O.D.E. solution \(\left\{y_{i}(s)\right\}_{i = 0, 1, \dots }\) for \(s\in [0, 1]\). This is the formal \href{https://en.wikipedia.org/wiki/Mean-field_theory}{mean-field} analysis that is not in the scope of the course. Hence, we give a flavor of the results using a hand-wavy argument.
	\end{enumerate}
\end{note}