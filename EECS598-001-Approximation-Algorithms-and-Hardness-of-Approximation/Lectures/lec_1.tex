\chapter{Introduction}
\lecture{1}{29 Aug.\ 10:30}{Overview and Set Cover}
\section{Computational Problem}
We're interested in the following optimization problem: Given a problem with an input, we want to either maximize or minimize some objectives. This suggests the following definition.

\begin{definition}[Computational problem]\label{def:computational-problem}
	A \emph{computational problem} \(P\) is a function from input \(I\) to \((X, f)\), where \(X\) is the feasible set of \(I\) and \(f\) is the objective function.
\end{definition}

We see that by replacing \(f\) with \(-f\), we can unify the notion and only consider either minimization or maximization, but we will not bother to do this.

\begin{eg}[\(s\)-\(t\) shortest path]\label{eg:s-t-path}
	The \emph{\(s\)-\(t\) shortest path} problem \(P\) can be formalized as follows. Given input \(I\), it defines
	\begin{itemize}
		\item Input: Graph \(\mathcal{G} =(\mathcal{V} , \mathcal{E} )\) and two vertices \(s, t\in \mathcal{V} \).
		\item Feasible set: \(X=\left\{ \text{set of all (simple) paths \(s\) to \(t\)} \right\}\).
		\item objective function: \(f\colon X\to \mathbb{R} \) where \(f(x) = \operatorname{length}(x)\) (\# of edges of \(x\)).
	\end{itemize}
	The output of \(P\) should be some \(x\in X\) (i.e., some valid \(s\)-\(t\) paths) such that it minimizes \(f(x)\).
\end{eg}

We see that the \hyperref[def:computational-problem]{computational problem} we focus on is an optimization problem, and more specifically, we're interested in \hyperref[def:combinatorial-optimization]{combinatorial optimization}.

\begin{definition}[Combinatorial optimization]\label{def:combinatorial-optimization}
	A \emph{combinatorial optimization} problem is a \hyperref[def:computational-problem]{problem} where the feasible set \(X\) is a finite set.
\end{definition}

\begin{eg}[\(s\)-\(t\) shortest path]
	The \hyperref[eg:s-t-path]{\(s\)-\(t\) shortest path} problem is a \hyperref[def:combinatorial-optimization]{combinatorial optimization} problem since given a graph \(\mathcal{G} \) with \(n = \left\vert \mathcal{V}  \right\vert \), \(m = \left\vert \mathcal{E}  \right\vert \), there are at most \(n!\) different paths, i.e., \(\left\vert X \right\vert \leq n! < \infty\).
\end{eg}

\begin{note}
	We'll also look into some continuous optimization problem, where \(X\) is now infinite (or even uncountable). For example, find \(x\in \mathbb{R} \) that minimizes \(f(x)=x^{2} +2x + 1\). In this case, \(X = \mathbb{R} \) which is uncountable (hence infinite).
\end{note}

\section{Efficient Algorithms}
Given a \hyperref[def:computational-problem]{problem} \(P\), we want to solve it fast with \hyperref[def:algorithm]{algorithms}. Before we characterize the speed of an \hyperref[def:algorithm]{algorithm}, we should first define what exactly an algorithm is.

\begin{definition}[Algorithm]\label{def:algorithm}
	Given a \hyperref[def:computational-problem]{problem} \(P\) and input \(I\) (which defines \(X\) and \(f\)), an \emph{algorithm} \(A\) outputs solution \(y = A(I)\) such that \(y\in X\) and \(y = \argmax_{x\in X} f(x)\) or \(\argmin_{x\in X}\), depending on \(I\).
\end{definition}

\begin{definition}[Efficient]\label{def:efficient}
	We say that an \hyperref[def:algorithm]{algorithm} \(A\) is \emph{efficient} if it runs in \textbf{polynomial time}.
\end{definition}

\begin{remark}[Runtime parametrization]
	The \emph{runtime} of an \hyperref[def:algorithm]{algorithm} \(A\) should be parametrized by the size of input \(I\). Formally, given input \(I\) represented in \(s\) bits, runtime of \(A\) on \(I\) should be \(\poly(s)\) for \(A\) to be \hyperref[def:efficient]{efficient}.
\end{remark}

\begin{note}
	In most cases, there are \(1\) or \(2\) parameters that essentially define the size of input.

	\begin{eg}[Graph]
		A natural representation of a graph with \(n\) vertices and \(m\) edges are
		\begin{enumerate}[(a)]
			\item Adjacency matrix: \(n ^{2} \) numbers.
			\item Adjacency list: \(O(m+n)\) numbers.
		\end{enumerate}
	\end{eg}

	\begin{eg}[Set system]
		A \hyperref[def:set-system]{set system} with \(n\) elements and \(m\) sets has a natural representation which uses \(O(nm)\) numbers.
	\end{eg}
\end{note}

\begin{eg}
	If an input \(I\) can be represented by \(s\) bits, then the runtime of an \hyperref[def:algorithm]{algorithm} can be \(O(s \log s)\), \(O(s ^{2} )\), or \(O(s^{100} )\), which are considered as \hyperref[def:efficient]{efficient}. On the other hand, something like \(2^s\) or \(s!\) are not.
\end{eg}

Hence, our goal is to get \(\poly((n, m))\)-time \hyperref[def:algorithm]{algorithm}!

\section{Approximation Algorithms}
We first note that many interesting \hyperref[def:combinatorial-optimization]{combinatorial optimization problems} are \(\NP\)-hard, hence it's impossible to find optimum in polynomial time unless \(\P\) is \(\NP\). This suggests one problem: \emph{How well can we do in polynomial time?}

In normal cases, we may assume that objective function value is always positive, i.e., \(f\colon X \to \mathbb{R} ^* \cup \left\{ 0 \right\} \). Then, we have the following definition which characterize the \emph{slackness}.

\begin{definition}[Approximation algorithm]\label{def:approximation-algorithm}
	Given an \hyperref[def:algorithm]{algorithm} \(A\), we say \(A\) is an \emph{\(\alpha\)-approximation algorithm} for a \hyperref[def:computational-problem]{problem} \(P\) if for every input \(I\) of \(P\),
	\begin{itemize}
		\item Min: \(f(A(I)) \leq \alpha \cdot \OPT (I)\) for \(\alpha \geq 1\)
		\item Max: \(f(A(I)) \geq \alpha \cdot \OPT (I)\) for \(\alpha \leq 1\)
	\end{itemize}
	where we define \(\OPT(I)\) as \(\max _{x\in X}f(x)\) for maximization, \(\min _{x\in X}f(x)\) if minimization.
\end{definition}

We see that \(\alpha \) characterizes the slackness allowed for our \hyperref[def:algorithm]{algorithm} \(A\). Now, we're ready to look at some interesting \hyperref[def:combinatorial-optimization]{problems}. Broadly, there are around \(10\) classes of them which are actively studied:

\begin{itemize}
	\item We'll see cover, clustering, network design, and constraint satisfaction problems.
	\item We'll not see: graph cuts, Packing, Scheduling, String, etc.
\end{itemize}

The above list is growing! For example, applications of continuous optimization in \hyperref[def:combinatorial-optimization]{combinatorial optimization} is getting attention recently. Also, there are around \(8\) techniques developed, e.g., greedy, local search, LP rounding, SDP rounding, primal-dual, cuts and metrics, etc.

\section{Hardness}

For most problems we saw, we can even say that getting an \(\alpha\)-approximation is \(\NP\)-hard for some \(\alpha > 1\). This bound is sometimes tight, but not always, and we'll focus on this part in the second half of this course.

\chapter{Covering}\label{ch:covering}
\section{Set Cover}
Before we jump into any problem formulations, we define a fundamental object in combinatorial optimization, the \hyperref[def:set-system]{set system}.

\begin{definition}[Set system]\label{def:set-system}
	Given a ground set \(\Omega\) (often called \emph{universe}), the \emph{set system} is an order pair \((\Omega , \mathcal{S} )\) where \(\mathcal{S} \) is a collection of subsets of \(\Omega\).
\end{definition}

\begin{note}
	For a \hyperref[def:set-system]{set system} \((\Omega , \mathcal{S} )\), we often let \(m\coloneqq \left\vert \mathcal{S}  \right\vert \) and \(n\coloneqq \left\vert \Omega  \right\vert \).
\end{note}

\begin{definition}[Degree]\label{def:degree}
	Given a \hyperref[def:set-system]{set system} \((\Omega , \mathcal{S} )\), the \emph{degree of \(x\in \Omega\)}, \(\deg(x)\), is defined as
	\[
		\deg(x) \coloneqq \left\vert \left\{ S\in \mathcal{S} \mid x\in S \right\}  \right\vert.
	\]
\end{definition}

\begin{remark}[Bipartite representation]
	Naturally, for a \hyperref[def:set-system]{set system}, we have a bipartite representation.
	\begin{figure}[H]
		\centering
		\incfig{set-system-bip}
		\caption{Bipartite representation of a \hyperref[def:set-system]{set system}.}
		\label{fig:set-system-bip}
	\end{figure}

	Denote \(d \coloneqq \max _{e\in U}\deg(e)\leq m\) and \(k \coloneqq \max _{i\in [m]}\left\vert S_i \right\vert \leq n\), which is just the maximum vertex degree on two sides of the bipartite graph representation of this \hyperref[def:set-system]{set system}.
\end{remark}

Finally, we have the following.

\begin{definition}[Covering]\label{def:covering}
	A \emph{covering} \(\mathcal{S} ^\prime \subseteq \mathcal{S} \) of \((\Omega , \mathcal{S} )\) is a (sub)collection of subsets such that \(\bigcup_{S\in \mathcal{S} ^\prime }S = \Omega\).
\end{definition}

Let's first consider the classical problem called \hyperref[prb:set-cover]{set cover}.

\begin{problem}[Set cover]\label{prb:set-cover}
Given a finite \hyperref[def:set-system]{set system} \((U, \mathcal{S} )\) where \(\mathcal{S} \coloneqq \left\{ S_i\subseteq U\right\}_{i=1}^m \) along with a weight function \(w\colon \mathcal{S} \to \mathbb{R} ^+\), find a \hyperref[def:covering]{covering} \(\mathcal{S} ^\prime \) while minimizing \(\sum_{S\in \mathcal{S} ^\prime }w(S)\).
\end{problem}

Assuming there always exists at least one \hyperref[def:covering]{covering}, we can in fact get two types of non-comparable approximation ratio in terms of \(k\) and \(d\). Specifically, we get \(\log k\) and \(d\)-approximation ratio via either greedy, LP rounding or dual-methods.

\section{Greedy Method}
We first see the algorithm when \(w(S) = 1\) for all \(S\in \mathcal{S} \).

\begin{algorithm}[H]\label{algo:set-cover-greedy}
	\DontPrintSemicolon{}
	\caption{\hyperref[prb:set-cover]{Set cover} -- Greedy}
	\KwData{A \hyperref[def:set-system]{set system} \((U, \mathcal{S})\)}
	\KwResult{A \hyperref[def:covering]{covering} \(\mathcal{S} ^\prime \)}
	\BlankLine

	\(\mathcal{S} ^\prime \gets \varnothing \), \(i\gets 0\)\;
	\While(\Comment*[f]{\(O(n)\)}){\(U \neq \varnothing \)}{
		Choose \(S_i\) with maximum \(\left\vert U \cap S_i \right\vert\)\Comment*[r]{\(O(mn)\)}
		\For(){\(e\in U \cap S_i\) }{
			\(y_e \gets w(S_{i}) / \left\vert U \cap S_i \right\vert \)\Comment*[r]{Average costs}
		}
		\(\mathcal{S} ^\prime \gets \mathcal{S} ^\prime \cup \left\{ S_i \right\} \)\;
		\(U\gets U\setminus S_i\)\;\label{line5:algo:set-cover-greedy}
		\(i\gets i+1\)\;
	}
	\Return{\(\mathcal{S} ^\prime \)}\;
\end{algorithm}

We focus on the case that \(w(S) = 1\) for all \(S\).

\begin{remark}
	It's clear that \autoref{algo:set-cover-greedy} is a polynomial time algorithm, also, the output \(\mathcal{S} ^\prime \) is always a valid \hyperref[def:covering]{covering}.
\end{remark}

\begin{theorem}\label{thm:set-cover-k-approx}
	\autoref{algo:set-cover-greedy} is an \(H_k\)-approximation\footnote{\(H_k\) is the so-called \emph{harmonic number}, which is defined as \(\sum_{i=1}^{k} 1 / i \leq \ln k + 1\).} algorithm.
\end{theorem}
\begin{proof}
	Denote the \(\OPT\) as \(\mathcal{S} ^{\ast} \coloneqq \left\{ S_1^{\ast} , \dots , S_{\ell}^{\ast} \right\}\), and first note that the average cost \(y_e\) essentially maintains \(\sum_{e\in U} y_e = \left\vert \mathcal{S} ^\prime  \right\vert\), hence we just need to bound \(y_e\) w.r.t.\ \(S^{\ast}\). To do this, for any \(S^{\ast} \in \mathcal{S} ^{\ast} \), say \(S_1^{\ast} \eqqcolon \left\{ e_1, \dots , e_k  \right\} \) where we number \(e_i\) in terms of the order of which being deleted, i.e., \(e_1\) is deleted first from \(U\) (\autoref{line5:algo:set-cover-greedy}), etc.

	\begin{note}
		\(S_1^{\ast} \) can have less than \(k\) element, but in that case similar argument will follow. Also, if some elements are deleted at the same time, we just order them arbitrarily.
	\end{note}

	Then, we have the following claim.

	\begin{claim}\label{clm:set-cover-cost}
		For all \(e_i\), \(y_{e_i} \leq  \frac{1}{k - i + 1}\).
	\end{claim}
	\begin{explanation}
		Consider the iteration when \(e_i\) was picked by \(S^\prime\), i.e., \(\left\vert U \cap S^\prime  \right\vert \geq \left\vert U \cap S_1 ^{\ast} \right\vert \geq k - i + 1\), then by definition (\autoref{line5:algo:set-cover-greedy}) we have \(y_{e_i} = \frac{1}{\vert U \cap S^\prime  \vert } \leq \frac{1}{\vert U \cap  S_1 ^{\ast} \vert} \leq \frac{1}{k - i + 1}\).
	\end{explanation}

	We immediately see that whenever the optimal solution pays \(1\) (for choosing \(S_1^{\ast}\) for instance), \autoref{algo:set-cover-greedy} pays at most \(H_k\) since \(\sum_{e_i\in S_1^{\ast} }y_{e_i} \leq \sum_{i=1}^{k} \frac{1}{k-i+1} = H_k\), or more formally,
	\[
		\left\vert \mathcal{S} ^\prime  \right\vert = \sum_{e\in U}y_e \leq \sum_{S_i^{\ast} \in \mathcal{S} ^{\ast} }\underbrace{\sum_{e\in S_i^{\ast} }y_e}_{\leq H_k} \leq \ell \cdot H_k = H_k\cdot \left\vert \OPT \right\vert,
	\]
	which finishes the proof.
\end{proof}

In all, observe that \(H_k \leq \ln k + 1\), we see that \autoref{algo:set-cover-greedy} is a \((\ln k)\)-approximation algorithm. Also, the weighted version can be easily derived by replacing \(1\) with the corresponding weight.