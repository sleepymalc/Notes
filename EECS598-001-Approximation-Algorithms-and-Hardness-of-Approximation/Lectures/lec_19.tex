\chapter{Hardness of Approximation}
\lecture{19}{7 Nov.\ 10:30}{Complexity Theory for Approximation Algorithm}
Recall how we define the \hyperref[def:combinatorial-optimization]{combinatorial optimization}.

\begin{prev}
	Given a set of all possible inputs \(\mathcal{I} \) for a \hyperref[def:combinatorial-optimization]{combinatorial optimization} problem \(P\), the goal is to find \(x\in X_I\) to maximize/minimize \(f_I (x)\) where \(f_I \colon X_I \to \mathbb{R} ^+\) is an objective function, and \(X_I\) is a set of feasible solutions.
\end{prev}

Now, we're going to discuss the complexity of doing approximation problem. But since the classical complexity theory is under the context of decision problems, we now try to generalize it.

\section{Approximation Complexity}
In this section, we'll consider \hyperref[def:combinatorial-optimization]{maximization problems} primarily, however, the same definition can be adapted to \hyperref[def:combinatorial-optimization]{minimization problems} naturally. First consider the decision problems: given a \hyperref[def:combinatorial-optimization]{maximization problem} \(P\) with goal being finding an objective in \(\mathbb{R} \), we have the following decision version.

\begin{definition}[Decision-\(P\)]\label{def:decision-P}
	Given a \hyperref[def:combinatorial-optimization]{maximization problem} \(P\), the \emph{decision-\(P\)} is the decision version of \(P\), where given an input \(I\in \mathcal{I} \), \(c\in \mathbb{R} ^+\), finds an algorithm which output \(\True\) if \(\OPT_I \geq c\), \(\False\) otherwise.
\end{definition}

And we have the following characterization of \(P\) and \hyperref[def:decision-P]{decision-\(P\)} in terms of complexity class.

\begin{definition}
	A \hyperref[def:combinatorial-optimization]{maximization problem} \(P\) is \(\NP\) if \hyperref[def:decision-P]{decision-\(P\)} is \(\NP\).
\end{definition}

\subsection{The Gap Problem}
Adapting this to the approximation version, we have the following generalization.

\begin{definition}[Gap]\label{def:Gap}
	Given a \hyperref[def:combinatorial-optimization]{maximization problem} \(P\) with \(\alpha \leq 1\), the \emph{\(\alpha \)-Gap} \(P\) is the \hyperref[def:decision-P]{decision version} of \(\alpha\)-approximating \(P\), where given an input \(I\in \mathcal{I} \) and \(c\in \mathbb{R} ^+\), finds an algorithm which outputs \(\True\) if \(\OPT_I \geq c\), \(\False\) if \(\OPT_I < \alpha c\), and anything else (don't care) otherwise.
\end{definition}

\begin{intuition}
	Since we allow some approximation, we don't care about the ``\hyperref[def:Gap]{gap}'', i.e., \(\OPT_I \in [\alpha c, c)\) .
\end{intuition}

Since we see that we may ignore some outputs, we divide the output into two different sets.

\begin{notation}
	Let \(\mathcal{I} \) be a set of all inputs, \(Y\) be a set of all \(\True\) inputs, \(N\) be a set of all \(\False\) inputs.\footnote{We have \(Y \cap N = \varnothing \), while not necessarily have \(N \cup Y = \mathcal{I} \).} Then given an input \(I\in \mathcal{I} \), output \(\True\) if \(I\in Y\), \(\False\) if \(I\in N\), anything otherwise.
\end{notation}

\begin{remark}
	If there is an \(\alpha \)-approximation algorithm for \(P\), then there is an algorithm for \hyperref[def:Gap]{\(\alpha\)-Gap} \(P\).
\end{remark}
\begin{explanation}
	Given \(I\) and \(c\), we run the \(\alpha \)-approximation algorithm for \(P\) to get a solution with value \(c^\prime \). Notice that we necessarily have
	\[
		\alpha \OPT_I \leq c^\prime \leq \OPT_I,
	\]
	hence we can design a new algorithm which outputs \(\True\) if \(c^\prime \geq c \cdot \alpha \), \(\False\) otherwise. This is a correct algorithm for \hyperref[def:Gap]{\(\alpha \)-Gap \(P\)} since
	\begin{itemize}
		\item If \(\OPT_I \geq c\), then \(c^\prime \geq \alpha \OPT_I \geq \alpha c\), which is the \(\True\) case.
		\item If \(\OPT_I < \alpha c\), then \(c^\prime \leq \OPT_I < \alpha c\), which is the \(\False\) case.
	\end{itemize}

	Conversely, it there is no polynomial time algorithm for \hyperref[def:Gap]{\(\alpha \)-Gap \(P\)}, there is no \(\alpha \)-approximation algorithm for \(P\).
\end{explanation}

Again, we have the following characterization of \(P\) and \hyperref[def:Gap]{\(\alpha \)-Gap \(P\)} in terms of complexity class.

\begin{definition}
	An \(\alpha \)-approximating problem \(P\) is \(\NP\) if the \hyperref[def:Gap]{\(\alpha \)-Gap \(P\)} is \(\NP\).
\end{definition}

\subsection{Approximation Reduction}
Finally, we briefly review the classical reduction for \hyperref[def:decision-P]{decision} problem.

\begin{prev}[Reduction]
	Given two problems \(P_1\), \(P_2\), a \emph{reduction} from \(P_1\) to \(P_2\) is a polynomial time algorithm \(R\) such that given an input \(I_1\in \mathcal{L}_1 \), output \(I_2\in \mathcal{L} _2\) satisfying both \hyperref[def:completeness]{completeness} and \hyperref[def:soundness]{soundness}.
	\begin{prev}[Completeness]
		A \hyperref[def:reduction]{reduction} from \(P_1\) to \(P_2\) satisfies \emph{completeness} if it transforms an accepted input for \(P_1\) to an accepted input for \(P_2\), i.e., if \(I_1\in Y_1\), then \(I_2\in Y_2\).
	\end{prev}
	\begin{prev}[Soundness]
		A \hyperref[def:reduction]{reduction} from \(P_1\) to \(P_2\) satisfies \emph{soundness} if it transforms a rejected input for \(P_1\) to a rejected input for \(P_2\), i.e., if \(I_1\in N_1\), then \(I_2\in N_2\).
	\end{prev}
\end{prev}

The reason why we care about \hyperref[def:reduction]{reduction} is that given a reduction \(R\) from \(P_1\) to \(P_2\), if there exists an algorithm for \(P_2\), then we have an algorithm for \(P_1\). by the following.

\begin{algorithm}[H]\label{algo:reduction}
	\DontPrintSemicolon{}
	\caption{\hyperref[def:reduction]{Reduction}}
	\KwData{Algorithm \(\mathcal{A} _2\) for \(P_2\), \hyperref[def:reduction]{reduction} \(R\) from \(P_1\) to \(P_2\), input \(I_1 \in \mathcal{L} _1\) for \(P_1\)}
	\KwResult{Decision of \(I_1\)}
	\SetKwFunction{Reduction}{\(R\)}
	\SetKwFunction{Alg}{\(\mathcal{A} _2\)}
	\BlankLine
	\(I_2\gets\)\Reduction{\(I_2\)}\;
	\Return{\Alg{\(I_2\)}}
\end{algorithm}

Similarly, since we care about approximation algorithm, we can define the approximation version of the reduction from \hyperref[def:Gap]{\(\alpha \)-Gap} \(P_1\) to \hyperref[def:Gap]{\(\beta \)-Gap} \(P_2\) as follows.

\begin{definition}[Reduction]\label{def:reduction}
	Given two \hyperref[def:combinatorial-optimization]{maximization problems} \(P_1\), \(P_2\), a \emph{reduction} from \hyperref[def:Gap]{\(\alpha \)-Gap} \(P_1\) to \hyperref[def:Gap]{\(\beta \)-Gap} \(P_2\) is a polynomial time algorithm \(R\) such that given an input \(I_1\in \mathcal{L}_1 \) and \(c_1\), output \(I_2\in \mathcal{L} _2\) and \(c_2\) satisfying both \hyperref[def:completeness]{completeness} and \hyperref[def:soundness]{soundness}.
	\begin{definition}[Completeness]\label{def:completeness}
		A \hyperref[def:reduction]{reduction} from \(P_1\) to \(P_2\) satisfies \emph{completeness} if it transforms an accepted input for \(P_1\) to an accepted input for \(P_2\), i.e., if \(\OPT_{I_1} \geq c_1\), then \(\OPT_{I_2} \geq c_2\).
	\end{definition}
	\begin{definition}[Soundness]\label{def:soundness}
		A \hyperref[def:reduction]{reduction} from \(P_1\) to \(P_2\) satisfies \emph{soundness} if it transforms a rejected input for \(P_1\) to a rejected input for \(P_2\), i.e., if \(\OPT_{I_1} < \alpha c_1\), then \(\OPT_{I_2} < \beta c_2\).
	\end{definition}
\end{definition}

\begin{remark}
	The term \hyperref[def:completeness]{completeness} and \hyperref[def:soundness]{soundness} comes from logic.
\end{remark}
\begin{explanation}
	More intuitively, for a proof system, \hyperref[def:completeness]{completeness} states that every true statement has a proof, while \hyperref[def:soundness]{soundness} states that every false statement can't have a proof, i.e., we can't prove anything that is wrong.
\end{explanation}

And again, given a \hyperref[def:reduction]{reduction} \(R\) if there is a polynomial time algorithm for \hyperref[def:Gap]{\(\beta \)-Gap} \(P_2\), then we have a polynomial time algorithm for \hyperref[def:Gap]{\(\alpha \)-Gap} \(P_1\); on the other hand, if there is no polynomial time algorithm for \hyperref[def:Gap]{\(\alpha \)-Gap} \(P_1\), then there is no polynomial time algorithm for \hyperref[def:Gap]{\(\beta \)-Gap} \(P_2\), so there is no \(\beta \)-approximation algorithm for \(P_2\).

\section{Probabilistically Checkable Proofs}
\subsection{Constraint Satisfaction Problem}
We first study one of the most important problems in theoretical computer science, the \hyperref[prb:CSP]{CSP} problem. This is important since it's the \hyperref[def:reduction]{reduction} for many important problems, and form the discussion, if we have a good algorithm for \hyperref[prb:CSP]{CSP}, we automatically get lots of other problems solved.

\begin{problem}[CSP]\label{prb:CSP}
Given an input \((x_1, \dots , x_n) = X\), \(C_1, \dots  , C_m\) where \(C_i = (a_i, b_{i_1}, \dots , b_{i_k})\) be the set of clauses where \(a_i\in {\ell }\), \(b_{i_j}\in [n]\), the \emph{constraint satisfaction problem of \(\Sigma, \Phi \)}\footnote{\(\Sigma \) is the alphabet set and \(\Phi = \left\{ \phi _1, \dots , \phi _{\ell }  \right\} \) is a family of constraints where \(\phi _i \colon \Sigma ^k \to \left\{ 0, 1 \right\} \).} is to find \(\sigma \colon X\to \Sigma \) maximizing the number of satisfied clauses, i.e., \(\sigma _{a_i}(x_{b_1},\dots , x_{b_k} )=1\).
\end{problem}

There's an important distinction between problem description and problem instance. That is, the \hyperref[prb:CSP]{CSP} with respect to \(\Sigma, \Phi \) is the problem description of a class of problems, and after given some variables \(X\) and clauses \(C_i\), it becomes a problem instance, which can be solved.

\begin{notation}[Problem description]
	The problem description of \hyperref[prb:CSP]{CSP} with respect to \(\Sigma \) and \(\Phi \) is denoted as \(\CSP(\Sigma , \Phi )\).
\end{notation}

Notice that we can equivalently maximize the fraction instead of maximize the number of satisfied clauses, i.e., the objective is now \(\#\text{satisfied clauses} / m \). It's because it's convenient to normalize the objective to be in \([0, 1]\).

\begin{note}
	Notice that to represent \(\phi _i \colon \Sigma ^k \to \left\{ 0, 1 \right\} \), it's often more convenient just to denote it as \(\phi _i ^{-1} (\left\{ 1 \right\} )\), i.e., the set of accepted string in \(\Sigma ^k\) w.r.t.\ \(\phi _i\).
\end{note}

\begin{eg}[Max-cut as CSP]
	\hyperref[prb:max-cut]{Max cut} is equivalent to \(\CSP(\Sigma , \Phi )\) where \(\Sigma = \left\{ 0, 1 \right\} \), \(\Phi = \left\{ \phi _1 \right\} \) with \(\phi _1 = \left\{ 01, 10 \right\}\).
\end{eg}
\begin{explanation}
	If we model \hyperref[prb:max-cut]{max cut} in this way, given an instance of \hyperref[prb:max-cut]{max cut}, i.e., given a graph \(\mathcal{G} =(\mathcal{V} , \mathcal{E} )\) with \(n\) nodes, \(C_i = (1, u, v)\) for \((u, v)\in \mathcal{E} \). The first entry is \(1\) since there are only one constraint to check whether a node is in the cut or not, and we create \(C_i\) for every edge \((u, v)\).
\end{explanation}

\begin{eg}[Max-2SAT as CSP]
	MAX-2SAT is equivalent to \(\CSP(\Sigma , \Phi )\) where \(\Sigma = \left\{ 0, 1 \right\} \), \(\Phi =\left\{ \phi _1, \dots , \phi _4  \right\} \) with
	\[
		\begin{split}
			\phi _1 = \left\{ 01, 10, 11 \right\} \iff (x_i \lor x_j),            & \quad
			\phi _2 = \left\{ 01, 10, 00 \right\} \iff (\overline{x}_i \lor \overline{x}_j), \\
			\phi _3 = \left\{ 01, 00, 11 \right\} \iff (\overline{x}_i \lor x_j), & \quad
			\phi _4 = \left\{ 00, 10, 11 \right\} \iff (x_i \lor \overline{x}_j).
		\end{split}
	\]
\end{eg}
\subsection{The Probabilistic Checkable Proofs Theorem}
As mentioned, there's lots of \hyperref[def:reduction]{reduction} can be done between fundamental problems considered in TCS to \hyperref[prb:CSP]{CSP}, including one of the most important results in hardness, the \hyperref[thm:PCP]{PCP theorem}. In order to do this, we need a more fine-grained version of \autoref{def:Gap}.

\begin{definition}[\((c, s)\)-Gap]\label{def:c-s-Gap}
	Given a maximization \hyperref[def:combinatorial-optimization]{problem} \(P\) with \(0 < s \leq c \leq 1\), the \emph{\((c, s)\)-Gap} \(P\) is the \hyperref[def:decision-P]{decision version} of \(\alpha\)-approximating \(P\), where given an input \(I\in \mathcal{I} \) and \(c\in \mathbb{R} ^+\), finds an algorithm which outputs \(\True\) if \(\OPT_I \geq c\), \(\False\) if \(\OPT_I < s\), and anything else (don't care) otherwise.
\end{definition}

\begin{note}
	We implicitly assume that \hyperref[def:c-s-Gap]{\((c, s)\)-Gap} \(P\) is only defined for \(P\) being a \hyperref[prb:CSP]{CSP}, or can be \hyperref[def:reduction]{reduced} to \hyperref[prb:CSP]{CSP}.
\end{note}

\begin{remark}
	We see that by setting \(s = \alpha \cdot c\), we recover \autoref{def:Gap} from \autoref{def:c-s-Gap}.
\end{remark}

Then, we have the following.

\begin{theorem}[Cook-Levin theorem~\cite{10.1145/800157.805047}]\label{thm:Cook-Levin}
	The \hyperref[def:c-s-Gap]{\((1, 1)\)-Gap} \hyperref[prb:max-3SAT]{3SAT} is \(\NP\)-hard.
\end{theorem}

\begin{theorem}[Karp~\cite{karp1972reducibility}]\label{thm:Karp}
	For all fixed \(\epsilon > 0\), \hyperref[def:c-s-Gap]{\((1-\epsilon , 1-\epsilon )\)-Gap} \hyperref[prb:max-cut]{max cut} is \(\NP\)-hard.
\end{theorem}

\begin{note}
	The \hyperref[def:c-s-Gap]{\((1, 1)\)-Gap} \hyperref[prb:max-cut]{max cut} is \(\P\).
\end{note}
\begin{explanation}
	Recall that if we transform \hyperref[prb:max-cut]{max cut} into \hyperref[prb:CSP]{CSP}, the optimal value is always \(1\),\footnote{i.e., we're not comparing to the optimal value of one instance of \(\mathcal{G} \).} i.e., every edge is cut edge, so in this case the graph must be bipartite. This can be easily check.
\end{explanation}

\begin{theorem}[PCP theorem~\cite{Feige1991ApproximatingCI, 10.1145/278298.278306}]\label{thm:PCP}
	There exists an \(\epsilon > 0\) such that \hyperref[def:c-s-Gap]{\((1, 1-\epsilon )\)-Gap} \hyperref[prb:max-3SAT]{3SAT} is \(\NP\)-hard.
\end{theorem}

To understand \href{thm:PCP}, we need to understand the class \hyperref[def:PCP]{PCP}. First, recall the definition of \(\NP\).

\begin{prev}[\(\NP\)]
	A language \(L \subseteq \left\{ 0, 1 \right\} ^{\ast}\) is in \(\NP\) if there exists a Turing machine \(V\) runs in \(\poly(\vert x \vert )\) such that given \(x\),
	\begin{itemize}
		\item \(x\in L\), then \(\exists y\) such that \(V(x, y) = 1\);
		\item \(x \notin L\), then \(\forall y\) such that \(V(x, y) = 0\).
	\end{itemize}
\end{prev}

\begin{definition}[\(\PCP\)]\label{def:PCP}
	The class \emph{probabilistically checkable proofs}, or \(\PCP_{c, s}(r(n), q(n))\),\footnote{We implicitly assume that \(r\) and \(q\) depends on the length of the input \(\vert x \vert = n\).} is defined as \(L\in \PCP_{c, s}(r, q)\) if there exists a poly-time randomized Turing machine \(V\) which can only flip \(r\) coins\footnote{It only accepts random string \(R\) with length \(r\), i.e., is \(R \in \left\{ 0, 1 \right\} ^r\).} and given an input \(x\), \(V\) can look at \(x\) on \(q\) position \(Q_1, \dots  , Q_q\) by \(\phi _R\colon \left\{ 0, 1 \right\} ^q \to \left[ 0, 1 \right] \) where
	\begin{itemize}
		\item \(x\in L\), then \(\exists y\) such that \(\Pr_{R}(\phi (y_{Q_1}, \dots , y_{Q_q} ) = 1) \geq c\);
		\item \(x\notin L\), then \(\forall y\) such that \(\Pr_{R}(\phi (y_{Q_1}, \dots , y_{Q_q} ) = 1) < s\).
	\end{itemize}
\end{definition}

In \autoref{def:PCP}, the randomized Turing machine \(V\) decides both the position (\(Q_1, \dots , Q_q \)) we're allowed to access, and also a function \(\phi _R\) which only looks at \(x_{Q_1}, \dots , x_{Q_q}\), acting as a decider for \(V\).

\begin{note}
	Everything is decided before looking at any input.
\end{note}

Just like \hyperref[thm:Cook-Levin]{Cook-Levin theorem} is the mother of all \emph{exact hardness}, \hyperref[thm:PCP]{PCP theorem} is the mother of all \emph{hardness of approximation}.