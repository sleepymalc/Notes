\lecture{7}{21 Sep.\ 10:30}{\(k\)-Median and LMP Approximation}
\section{\(k\)-Median}

Let's look at another \hyperref[prb:clustering]{clustering} problem.

\begin{problem}[\(k\)-median]\label{prb:k-median}
Given a \hyperref[def:metric]{metric space} \((X, d)\) and \(P, Q\subseteq X\) with \(k\in \mathbb{N} \), find \(Q^\prime \subseteq Q\) with \(\left\vert Q^\prime  \right\vert = k\) which minimizes \(\sum_{i\in P} \min _{j\in Q^\prime } d(i, j)\).
\end{problem}

The natural linear programming for \autoref{prb:k-median} is the following. Consider \(\left\{ x_{ij}  \right\}_{i\in P, j\in Q} \) and \(\left\{ y_j \right\}_{j\in Q} \), then
\[
	\begin{aligned}
		\min~ & \sum_{ij} x_{ij} d(i, j)                                                \\
		      & \sum_{j} x_{ij} \geq 1   & \forall i\in P         &  & (\alpha _i)      \\
		      & x_{ij} \leq y_j          & \forall i\in P, j\in Q &  & (\beta _{ij})    \\
		      & \sum_{j} y_j \leq k      &                        &  & (f)\footnotemark \\
		      & x, y\geq 0
	\end{aligned}
\]
\footnotetext{Notice that compare to \autoref{prb:facility-location}, \(f\) here is a variable but not a given facility cost! The reason why we do this will be clear soon.}

\begin{intuition}
	We interpret \(x_{ij} \) as follows: if \(x_{ij} = 1\), then \(i\) belongs to \(j\). And \(y_j=1\) if \(j\) is the actual median we choose (i.e., in \(Q^\prime \)). As for constraints, both \(\sum_{j} x_{ij} \geq 1\) and \(\sum_{j} y_j \leq k\) are clear, while for \(x_{ij} \leq y_j\), we see that it can't be the case that \(x_{ij} = 1\) while \(y_j = 0\), i.e., we can't have the case that \(x_{ij} \) belongs to \(j\) while \(j\) isn't even in \(Q^\prime \).
\end{intuition}

The dual is then
\[
	\begin{aligned}
		\max~ & \sum_{i} \alpha _i - kf                                    \\
		      & \sum_{i} \beta _{ij} \leq d(i, j) & \forall i\in P, j\in Q \\
		      & \sum_{i} \beta _{ij} \leq f       & \forall j\in Q         \\
		      & \alpha , \beta \geq 0
	\end{aligned}
\]

\begin{note}
	Notice that this is exactly the dual as \autoref{prb:facility-location}, except that we now have an additional \(-kf\) term in the objective function. Although \(f\) is not included in the statement of \autoref{prb:k-median}, by denoting one of the dual variable \(f\), we get a similar formulation compare to \autoref{prb:facility-location}.
\end{note}

Due to the similarity between \autoref{prb:k-median} and \autoref{prb:facility-location}, we can try to use \autoref{algo:facility-location-greedy-III} which solves \autoref{prb:facility-location} with \(2\)-\hyperref[def:LMP]{LMP} guarantee. But note that in \autoref{prb:facility-location}, we need to specify \(f\). Suppose we guessed \(f\), and we run a \(\gamma\)-\hyperref[def:LMP]{LMP approximation} algorithm and somehow get \(k^\prime = k\). Then we have
\[
	\frac{\conn(\ALG)}{\gamma } \leq \sum_{i} \alpha _i - kf \leq \OPT_{\text{\(k\)-med.}},
\]
i.e., this is a \(\gamma \)-approximation algorithm. So now, the task is to guess \(f\) such that the algorithm gives exactly \(k\) centers.

\subsection{Bipoint Solution}
Turns out that we don't have ideas about the relation between \(k\) and \(f\), the only thing we know is that if \(f\to \infty \), \(k\) decreases, other than that it behaves quite arbitrary.

\begin{remark}
	The relation between \(k\) and \(f\) indeed highly depends on what algorithm we use. But at least for \autoref{algo:facility-location-greedy-III}, nobody knows anything in this case.
\end{remark}

Given this fact, just randomly guess one \(f\) doesn't work. A new idea is then to maintain two solutions (or interval) \([f^2, f^1]\) such that \(f^2 \leq f^1\),\footnote{We start from \(f^2 = 0\) and \(f^1 = \infty \), where we set \(f^1\) arbitrary large.} where
\begin{itemize}
	\item at \(f^2\), the algorithm opens \(k^2 \geq k\) facilities;
	\item at \(f^1\), the algorithm opens \(k^1 \leq k\) facilities.
\end{itemize}
Then, a naive approach is to use binary search and get \(f^2 \leq f^1\) such that
\[
	\left\vert f^1 - f^2 \right\vert \leq \frac{\epsilon \OPT}{n}.
\]
Notice that the whole point of doing binary search is because we assume that if \(k^2 \geq k\) at \(f^2\) and \(k_1 \leq k\) at \(f^1\), then we can find an \(f^{\ast} \in [f^2, f^1]\) such that we get exactly \(k^{\ast} = k\) at \(f^{\ast} \).

\begin{remark}[Caveat of achieving \(k\)]
	This is probably not the case for \autoref{algo:facility-location-greedy} (\(2\)-\hyperref[def:LMP]{LMP}) since the decision is quite sequential; but if we use \autoref{algo:facility-location-PD} (\(2\)-\hyperref[def:LMP]{LMP}), since there are lots of \hyperref[prb:max-strongly-independent-set]{maximal independent sets}, so by doing \textbf{a lot more work}, we can actually achieve this.
\end{remark}

Now, assume that we have continuity of the relation between \(k\) and \(f\) by carefully designing our (\(\gamma\)-\hyperref[def:LMP]{LMP}) algorithm, then \(\exists a\in [0, 1]\) and \(b \coloneqq 1 - a\) such that \(k \coloneqq ak^1 + bk^2\) where \(k^1 \leq k \leq k^2\). Denote \(C^i\) as the connection cost \(\conn(f^i)\) of \(f^i\) such that \(C^1 \geq C^2\), we have
\[
	\begin{dcases}
		C^1 + \gamma k^1 f^1 \leq \gamma \sum_{i} \alpha _i^1, & \quad (\times a) \\
		C^2 + \gamma kr2 f^2 \leq \gamma \sum_{i} \alpha _i^2, & \quad (\times b)
	\end{dcases}
\]
hence,
\[
	aC^1 + bC^2
	\leq \gamma \left( a \sum_{i} \alpha _i^1 + b \sum_{i} \alpha _i^2 - ak^1 f^1 - bk^2 f^2 \right)
	\leq \gamma \underbrace{\left( \sum_{i} \alpha _i - kf \right)}_{\leq \OPT_{\text{\(k\)-med.}}} + \underbrace{\vphantom{\left( \sum_{i} \alpha _i - kf \right)}\gamma k \left\vert f^1 - f^2 \right\vert}_{\leq \epsilon \OPT_{\text{\(k\)-med.}}},
\]
where we set \(\alpha \coloneqq a \alpha ^1 + b \alpha ^2\) and \(f \coloneqq \max (f^1, f^2)\).

\begin{note}
	To make sure \(\sum_{i} \alpha _i - kf \leq  \OPT_{\text{\(k\)-med.}}\), we need to check that \((\alpha , f)\) is dual-feasible for \autoref{prb:k-median}.
\end{note}
\begin{explanation}
	The feasibility comes from the fact that the first two constraints of \autoref{prb:k-median} are linear, so they're automatically satisfied. The only non-trivial constraint is \(\sum_{i} \beta _{ij} \leq f\), but since we choose \(f\) to be the maximum, it'll be more satisfied.
\end{explanation}

\begin{definition}[Bipoint solution]\label{def:bipoint-solution}
	Given \(F^1\), \(F^2\) with \(\left\vert F^1 \right\vert = k^1\), \(\left\vert F^2 \right\vert = k^2\) and \(k=ak^1 + bk^2\) for \(a, b\in [0, 1]\) and \(a + b = 1\), the \emph{bipoint solution}, denoted as \(aF^1 + b F^2\), satisfies
	\[
		a C^1 + b C^2 \leq \gamma \cdot \OPT_{\text{\(k\)-med.}}.
	\]
\end{definition}


\subsection{Bipoint Rounding}
From \autoref{def:bipoint-solution}, it's natural to do the so-called \hyperref[def:bipoint-rounding]{bipoint rounding}.
\begin{definition}[\(\delta \)-bipoint rounding]\label{def:bipoint-rounding}
	Given solutions \(F^1\) and \(F^2\), a solution \(F\) with \(\left\vert F \right\vert = k\) such that
	\[
		\conn(F) \leq \delta \cdot (a C^1 + b C^2) = \delta \cdot \conn(aF^1 + bF^2)
	\]
	is the so-called \emph{\(\delta\)-bipoint rounding solution}.
\end{definition}
\begin{note}
	If we have a \(\delta\)-\hyperref[def:bipoint-rounding]{bipoint rounding} of a \(\gamma \)-\hyperref[def:LMP]{LMP} algorithm solution, then we automatically have an approximation ratio of \(\delta \cdot \gamma \) for this \hyperref[def:bipoint-rounding]{bipoint rounding} solution.
\end{note}

Back to \autoref{prb:k-median}, we see that we can actually get a \(2\)-\hyperref[def:bipoint-rounding]{bipoint rounding} as follows. Consider we create a bipartite graph with \(Q^1, Q^2 \subseteq Q\) being two sides of the graph. Then for each \(i\in P\), \(i\) is connected to the closest facility in \(Q^1\), and also another closest facility in \(Q^2\), so we can create an edge between these two facilities.

\begin{center}
	\incfig{bipoint-rounding}
\end{center}

Now, for a fixed \(i\in P\), let \(d_j \coloneqq d(i, Q^j )\) for \(j = 1, 2\), we want to compare our designed final cost to \(a C^1 + b C^2\), so for this fixed \(i\), we want to make sure \(i\) \emph{pays} not much more than \(a d_1 + b d_2\).

\begin{intuition}
	We see that a natural rounding algorithm is the following: for an \(i \in P\), if its closest facility in \(Q^1\) is opened while its closest facility in \(Q^2\) is not opened, we may just direct \(i\) to the opened one in \(Q^1\), same for the other case. Now, if both facilities are opened, then we direct \(i\) to the facility in \(F^1\) with probability \(a\), while to the facility in \(Q^2\) with probability \(1 - a = b\).
\end{intuition}

\begin{remark}
	The problem of the above algorithm is that we don't have control about the total number of the final open facilities: it can be the case that at the end we open every facility in \(Q^2\), which is \(k^2\), not \(k\). So we need to sometimes direct \(i\) to other facilities (in \(Q^1\)) that is not closest to which.
\end{remark}

For \(j\in Q^1\), let \(\pi (j)\) be the closest facility in \(Q^2\) to \(j\), and let \(Q^{\ast} \) be the image of such a map \(\pi \), i.e., \(Q^{\ast} = \left\{ j^\prime \in Q^2 \colon j^\prime =\pi (j) \text{ for some }j\in Q^1  \right\} \).

\begin{note}
	We may assume \(\left\vert Q^{\ast} \right\vert = k^1\).
\end{note}
\begin{explanation}
	Clearly, \(\left\vert Q^{\ast} \right\vert \leq k^1\). And if \(\left\vert Q^{\ast}  \right\vert < k^1\), we add arbitrary centers so that \(\left\vert Q^{\ast}  \right\vert = k^1\).
	\begin{center}
		\incfig{bipoint-rounding-II}
	\end{center}
	For example, the initial image size above is only \(4\), we need to add \(2\) more arbitrary centers into \(Q^{\ast}\).
\end{explanation}

To open the facilities as what we want, consider the following rounding algorithm.

\begin{algorithm}[H]\label{algo:k-median-bipoint-rounding}
	\DontPrintSemicolon{}
	\caption{\hyperref[prb:k-median]{\(k\)-Median} -- \(2\)-\hyperref[def:bipoint-rounding]{Bipoint Rounding}}
	\KwData{A set of clients \(P\subseteq X\), a set of (possible) facilities \(Q\subseteq X\), \(a\in (0, 1)\), \(\epsilon \in (0, 1)\), \(k\in \mathbb{N} \)}
	\KwResult{A set of opened facilities \(Q^\prime \subseteq Q\) with \(\left\vert Q ^\prime \right\vert = k \)}
	\SetKwFunction{rand}{rand}
	\SetKwFunction{bisearch}{binary-search}
	\SetKw{Break}{break}
	\BlankLine

	\((Q^1, Q^2)\gets\)\bisearch{\(P\), \(Q\), \(\epsilon\)}\Comment*[r]{achieve \(\left\vert f^1 - f^2 \right\vert \leq \epsilon \OPT / n\)}
	\;
	\(Q^\prime \gets \varnothing \), \(k^1\gets \left\vert Q^1 \right\vert\), \(k^2\gets \left\vert Q^2 \right\vert\), \(Q^{\ast}\gets \left\{ j^\prime \in Q^2 \colon j^\prime =\pi (j) \text{ for some }j\in Q^1  \right\}\)\;
	\;
	\For(\label{algo:k-median-bipoint-rounding-for}){\(j\in Q^1\)}{
		\uIf(\Comment*[f]{open \(Q^1\) w.p.\ \(a\)}){\rand{\(0\), \(1\)}\(\leq a\)}{
			\(Q^\prime \gets Q^\prime \cup \left\{ j \right\} \)\;
		}\Else(\label{algo:k-median-bipoint-rounding-else}\Comment*[f]{open \(Q^{\ast}\) w.p.\ \(1-a\)}){
			\(Q^\prime \gets Q^\prime \cup \left\{ \pi (j) \right\} \)\;
		}
	}
	\;
	\Comment{still need to open \(k - k^1\) more}
	\(Q^\prime \gets Q^\prime \cup \left\{ \text{\((k - k^1)\) random \(j\in Q^2 \setminus Q^{\ast}\) }  \right\} \)\label{algo:k-median-bipoint-rounding-random}\;
	\Return{\(Q^\prime \)}\;
\end{algorithm}

\begin{remark}
	\autoref{algo:k-median-bipoint-rounding} is a randomized algorithm which will \textbf{always} open \(k\) facilities. The randomness comes from the cost, i.e., we can analyze its cost in expectation.
\end{remark}

\begin{intuition}
	\autoref{algo:k-median-bipoint-rounding} is kind of \emph{mimicking} what we want, since
	\begin{itemize}
		\item \(j\in Q^1\), \(\Pr(\text{\(j\) open} )= a\)
		\item \(j\in Q^{\ast}\), \(\Pr(\text{\(j\) open} ) = 1 - a = b\)
		\item \(j\in Q^2 \setminus Q^{\ast}\), \(\Pr(\text{\(j\) open} )= \frac{k-k^1}{k^2 - k^1}= b\)
	\end{itemize}
\end{intuition}

\begin{theorem}
	\autoref{algo:k-median-bipoint-rounding} is a \(2\)-\hyperref[def:bipoint-rounding]{bipoint} algorithm (in expectation).
\end{theorem}
\begin{proof}
	Let's analyze a bit careful. Fixing an \(i\in P\), and denote its closest facility in \(Q^1\) as \(j^1\), and the closest facility in \(Q^2\) as \(j^2\). If \(j^1\) is not opened, then we know \(\pi (j^1)\) is opened for sure in \autoref{algo:k-median-bipoint-rounding-else}. We see that
	\begin{itemize}
		\item If \(j^2\) is in \(Q^{\ast} \), then we know \(i\) will be direct to either \(j^1\) or \(j^2\) in \autoref{algo:k-median-bipoint-rounding-for}, i.e., \(i\) is perfectly happy since it can go to one of the closest facility.
		\item The tricky case is when \(j^2\) is not in \(Q^{\ast}\).
		      \begin{itemize}
			      \item If \(j^1\) is opened, \(i\) can still go to \(j^1\) without problem.
			      \item If \(j^1\) is also not opened, we know that \(\pi (j^1)\) will be opened in \autoref{algo:k-median-bipoint-rounding-else}. In this worst case, we just direct \(i\) to \(\pi (j^1)\) and the distance will be \(i\to j^1 \to \pi (j^1) \), which is bounded by \(d_1 + d(j^1, \pi (j^1))\). But observe that \(d(j^1, \pi (j^1)) \leq d_1 + d_2\), so we have \(2d_1 + d_2\).
		      \end{itemize}
	\end{itemize}

	\begin{center}
		\incfig{bipoint-rounding-III}
	\end{center}

	In all, we have the following.\footnote{This is a slightly worse result since we force \(i\) to go to \(j^2\) if \(j^2\) is opened, but actually, \(i\) can go to \(j^1\) if \(j^1\) is opened too with shorter distance. But this still gives us a good enough bound.}
	\begin{table}[H]
		\centering
		\begin{tabular}{c|c|c}
			\toprule
			                               & Distance       & Probability                  \\
			\midrule
			\(j^2\) open                   & \(d_2\)        & \(b\)                        \\
			\(j^2\) not open, \(j^1\) open & \(d_1\)        & \(\geq (a-b)^+ \eqqcolon M\) \\
			none of \(j^1, j^2\) open      & \(2d_1 + d_2\) & \(\leq 1 - b - M\)           \\
			\bottomrule
		\end{tabular}
	\end{table}

	Then, the expected cost is just\footnote{Since the final case is always worse than the second case, it is legal to assume that the second case has the minimum probability and the final has the maximum for the expectation bound to hold.}
	\[
		\mathbb{E}\left[\text{\(i\)'s connection cost}\right] \leq b d_2 + Md_1 + (1 - b - M) (2d_1 + d_2),
	\]
	and we now have two cases.
	\begin{itemize}
		\item If \(b \geq a\), then \(b \geq 1 / 2\), \(M = 0\) and
		      \[
			      \mathbb{E}\left[\text{\(i\)'s connected cost}\right] \leq b \cdot d_2 + (1 - b) (2 d_1 + d_2) = 2ad_1 + d_2 \leq 2(ad_1 + bd_2).
		      \]
		\item If \(a > b\), then \(a > 1 / 2\), \(M = a - b\) and
		      \[
			      \mathbb{E}\left[\text{\(i\)'s connected cost}\right] \leq b\cdot d_2 + (a - b) d_1 + b(2d_1 + d_2) = d_1(a+b)+d_2(b+b) \leq 2(ad_1 + bd_2).
		      \]
	\end{itemize}

	This shows \autoref{algo:k-median-bipoint-rounding} is a \(2\)-\hyperref[def:bipoint-rounding]{bipoint} algorithm in expectation, proving the result.
\end{proof}

\begin{remark}[SOTA]
	The SOTA result specifically for \autoref{prb:k-median} is summarized as follows.
	% file:///Users/pbb/Developer/quiver/src/index.html?q=WzAsOCxbMCwwLCJcXHRleHR7XFxoeXBlcnJlZlthbGdvOmZhY2lsaXR5LWxvY2F0aW9uLVBEXXtQcmltYWwtRHVhbH0gXFwoM1xcKS1cXGh5cGVycmVmW2RlZjpMTVBde0xNUH19Il0sWzAsMiwiXFx0ZXh0e0R1YWwgRml0dGluZ35cXGNpdGV7aHR0cHM6Ly9kb2kub3JnLzEwLjQ4NTUwL2FyeGl2LjIyMDcuMDUxNTB9IFxcKDEuOVxcbGRvdHMgOSBcXCktXFxoeXBlcnJlZltkZWY6TE1QXXtMTVB9fSJdLFsxLDEsIlxcdGV4dHtcXGh5cGVycmVmW2FsZ286ay1tZWRpYW4tYmlwb2ludC1yb3VuZGluZ117XFwoMlxcKS1iaXBvaW50IHJvdW5kaW5nfX0iXSxbMSwyLCJcXHRleHR7XFwoMS4zXFxsZG90cyAzXFwpLWJpcG9pbnQgcm91bmRpbmd9XFxmb290bm90ZXtUaGlzIHdpbGwgcmV0dXJuIFxcKGsrY1xcKSBjZW50ZXJzLCB3aGVyZSBcXChjXFwpIGlzIGFuIGFic29sdXRlIGNvbnN0YW50LiBUaGVyZSdzIGEgd2F5IHRvIHRyYW5zZm9ybSB0aGlzIHNvbHV0aW9uIGJhY2sgdG8gXFwoa1xcKSBjZW50ZXJzIHdpdGhvdXQgbG9vc2luZyBhbnkgcGVyZm9ybWFuY2UufSJdLFsyLDAsIlxcdGV4dHtcXCgzXFwpLWFwcHJveGltYXRpb259Il0sWzIsMiwiXFx0ZXh0e1xcKDIuNjdcXCktYXBwcm94aW1hdGlvbn0iXSxbMCwxLCJcXHRleHR7XFxoeXBlcnJlZlthbGdvOmZhY2lsaXR5LWxvY2F0aW9uLWdyZWVkeS1JSUlde0dyZWVkeX0gXFwoMlxcKS1cXGh5cGVycmVmW2RlZjpMTVBde0xNUH19Il0sWzIsMSwiXFx0ZXh0e1xcKDRcXCktYXBwcm94aW1hdGlvbn0iXSxbMSwzXSxbMyw1XSxbMCw0LCJcXHRleHR7Q29udmVyc2lvbn0iXSxbNiwyXSxbMiw3XV0=
	\[\begin{tikzcd}
			{\text{\hyperref[algo:facility-location-PD]{Primal-Dual} \(3\)-\hyperref[def:LMP]{LMP}}} && {\text{\(3\)-approximation}} \\
			{\text{\hyperref[algo:facility-location-greedy-III]{Greedy} \(2\)-\hyperref[def:LMP]{LMP}}} & {\text{\hyperref[algo:k-median-bipoint-rounding]{\(2\)-bipoint rounding}}} & {\text{\(4\)-approximation}} \\
			{\text{Dual Fitting~\cite{https://doi.org/10.48550/arxiv.2207.05150} \(1.9\dots 9 \)-\hyperref[def:LMP]{LMP}}} & {\text{\(1.3\dots 3\)-bipoint rounding}\footnote{This will return \(k+c\) centers, where \(c\) is an absolute constant. There's a way to transform this solution back to \(k\) centers without loosing any performance.}} & {\text{\(2.67\)-approximation}}
			\arrow[from=3-1, to=3-2]
			\arrow[from=3-2, to=3-3]
			\arrow["{\text{Conversion}}", from=1-1, to=1-3]
			\arrow[from=2-1, to=2-2]
			\arrow[from=2-2, to=2-3]
		\end{tikzcd}\]

	But we'll see that by changing the problem a bit, like consider squaring the distance in the objective of \autoref{prb:k-median} (which is the \(k\)-mean problem), we can get \(9\)-approximation by \hyperref[algo:facility-location-PD]{Primal-Dual}, while the lower path doesn't tell us anything, which is so fragile.
\end{remark}

\begin{note}[Derandomized]
	It's possible to derandomized \autoref{algo:k-median-bipoint-rounding}.
\end{note}