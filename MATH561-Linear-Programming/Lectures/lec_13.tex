\lecture{13}{18 Oct. 08:00}{Duality}
We now formally prove \hyperref[thm:strictly-complementarity]{strictly complementarity theorem}.

\begin{proof}[Proof of \autoref{thm:strictly-complementarity}]
	First prove for one fixed \(j\). Consider
	\[
		\begin{aligned}
			\max~        & x_{j}            \\
			             & c^{\top}x \leq v \\
			             & Ax = b           \\
			(P_{j})\quad & x\geq 0,
		\end{aligned}
	\]
	where
	\[
		\begin{aligned}
			 & c^{\top}x \\
			 & Ax = b    \\
			 & x\geq 0
		\end{aligned}
	\]
	is trying to model the set of \hyperref[def:optimal-solution]{optimal solutions} to \((\mathrm{P})\), and \(P_{j}\) is trying to find an \hyperref[def:optimal-solution]{optimal solution} of \((\mathrm{P})\) with \(x_{j}>0\).

	We see that there are three cases.
	\begin{enumerate}
		\item \(P_{j}\) has an \hyperref[def:optimal-solution]{optimal solution}. \(\hat{x}\) with \(\hat{x}_j>0\). Take \(\hat{x}\) \hyperref[def:optimal-solution]{optimal} for \(P_{j}\) \(\implies\) \(\hat{x}\) \hyperref[def:optimal-solution]{optimal} for \((\mathrm{P})\). Take a \(\hat{y}\) \hyperref[def:optimal-solution]{optimal} for \((\mathrm{D})\).
		\item \(P_{j}\) is unbounded. Take any \hyperref[def:feasible-solution]{feasible solutions} \(\hat{x}\) of \(P_{j}\) with \(\hat{x}_j>0\).
		\item The \hyperref[def:optimal-solution]{optimal} value of \(P_{j}\) is zero. Then consider the \hyperref[def:dual]{dual} of \(P_{j}\), denoted by \(D_{j}\) with the \hyperref[def:dual]{dual} variables \(w\in\mathbb{R},\ y\in\mathbb{R}^m\). We then have
		      \[
			      \begin{aligned}
				      \min~        & wv + y^{\top}b                        \\
				                   & wc^{\top}+y^{\top}A \geq e_{j}^{\top} \\
				      (D_{j})\quad & w\geq 0,\ y \text{ unres.}
			      \end{aligned}
		      \]
		      Suppose \(\hat{w}\) and \(\hat{y}\) is \hyperref[def:optimal-solution]{optimal} for \(D_{j}\).
		      \begin{enumerate}
			      \item[Case 1.] \(\hat{w}>0\): Then
				      \[
					      \begin{split}
						      &-c^{\top} + \left( \frac{\hat{y}^{\top}}{-\hat{w}} \right)A\underset{\leq }{\cancel{\geq}} \frac{1}{-\hat{w}} e_{j}^{\top}\\
						      \implies&\underbrace{\left( \frac{\hat{y}^{\top}}{-\hat{w}} \right)}_{\hat{\hat{y}}} A\leq c^{\top} -  \frac{1}{\hat{w}} e_{j}^{\top}\\
						      \implies&\hat{\hat{y}}^{\top}A\leq c^{\top} - \frac{1}{\hat{w}_j}e_{j}^{\top}\\
						      \implies&\hat{\hat{y}}^{\top}A\leq c^{\top} \text{ with a little \hyperref[def:slack-variable]{slack} in the \(j^{th}\) constraint.}\\
						      \implies&\hat{\hat{y}}^{\top}A_{\cdot j}\leq c_{j} - \frac{1}{\hat{w}}<c_{j}, \forall j.
					      \end{split}
				      \]
				      Note that the \hyperref[def:optimal-solution]{optimal} value of \(D_{j}\) is zero since the \hyperref[def:optimal-solution]{optimal} value of \(P_{j}\) is zero. Then
				      \[
					      \begin{split}
						      &\hat{w} v + \hat{y}^{\top}b = 0\\
						      \implies& -v + \left( \frac{\hat{y}^{\top}}{-\hat{w}} \right) b = 0\\
						      \implies& \hat{\hat{y}}^{\top}b = v\\
						      \implies& \hat{\hat{y}} \text{ is optimal for }D.
					      \end{split}
				      \]
			      \item[Case 2.] \(\hat{w} = 0\): Then
				      \[
					      \hat{y}^{\top}A\geq e_{j}^{\top}.
				      \]
				      Let \(\widetilde{y}\) be an \hyperref[def:optimal-solution]{optimal solution} of \((\mathrm{D})\). Now consider \(\widetilde{y} - \hat{y}\), we have
				      \[
					      \left(\widetilde{y} - \hat{y}\right)^{\top}A = \underbrace{\widetilde{y}^{\top}A}_{\leq c^{\top}} - \underbrace{\hat{y}^{\top}A}_{\geq e_{j}^{\top}} \leq c^{\top} - e_{j}^{\top},
				      \]
				      we see that \(\left( \widetilde{y} - \hat{y} \right) \) is \hyperref[def:feasible-solution]{feasible} for \((\mathrm{D})\) with \hyperref[def:slack-variable]{slackness} in the right-hand side in the \(j^{th}\) constraint.

				      Then the objective value of \(\widetilde{y}-\hat{y}\) of \((\mathrm{D})\) is
				      \[
					      \left( \widetilde{y}-\hat{y} \right)^{\top}b = \widetilde{y}^{\top}b - \hat{y}^{\top}b = v - \hat{y}^{\top}b = v
				      \]
				      since \(\hat{y}^{\top}b\) is the \hyperref[def:optimal-solution]{optimal} value of \(D_{j}\), which is zero.
		      \end{enumerate}
	\end{enumerate}

	Notice that this is just for a fixed \(j\)!
	\begin{table}[H]
		\centering
		\begin{tabular}{c|ccccc|ccccc}
			\toprule
			\(j\)      & \multicolumn{5}{c|}{\(\hat{x}^{\top}\)}   & \multicolumn{5}{c}{\(c^{\top} - \hat{y}^{\top}A\)}                                                                                                                                                                                                                                    \\
			\midrule
			1          & \(\ddots\)                                &                                                    &                   & 0            &            & \(\ddots\)                                             &            &                                                                                  &            &            \\
			\(\vdots\) &                                           & \(\ddots\)                                         &                   & \(\vdots\)   &            &                                                        & \(\ddots\) &                                                                                  &            &            \\
			\(j\)      & \multicolumn{2}{c}{\(\to \hat{x}^{(j)}\)} & \(0/+\)                                            & \(\vdots\)        &              &            &                                                        & \(+/0\)    & \multicolumn{2}{c}{\footnotesize{\(\leftarrow c^{\top}-\hat{y}^{(j)^{\top}}A\)}}                           \\
			\(\vdots\) &                                           &                                                    &                   & \(\ddots\)   &            &                                                        &            &                                                                                  & \(\ddots\) &            \\
			\(n\)      &                                           &                                                    &                   & 0            & \(\ddots\) &                                                        &            &                                                                                  & \(+\)      & \(\ddots\) \\
			\hline
			           &                                           &                                                    & \(\hat{\hat{x}}\) & \(\uparrow\) &            & \multicolumn{5}{c}{\(c^{\top}-\hat{\hat{y}}^{\top}A\)}                                                                                                                           \\
			           &                                           &                                                    &                   & 0            &            &                                                        &            &                                                                                  & \(+\)      &            \\
			\bottomrule
		\end{tabular}
	\end{table}
	\begin{intuition}
		We average out for all \(j\), then we have
		\[
			\hat{\hat{x}} \coloneqq \sum\limits_{j=1}^{n} \frac{1}{n}\hat{x}^{(j)},\quad \hat{\hat{y}} \coloneqq \sum\limits_{j=1}^{n} \frac{1}{n}\hat{y}^{(j)}
		\]
	\end{intuition}

	We check that \(\hat{\hat{x}}\) and \(\hat{\hat{y}}\) are \hyperref[def:feasible-solution]{feasible}. Since
	\[
		A\hat{\hat{x}} = A\left( \frac{1}{n}\sum\limits_{j=1}^{n} \hat{x}^{(j)} \right) = \frac{1}{n}\sum\limits_{j=1}^{n} \underbrace{\hat{Ax}^{(j)}}_{b} = b.
	\]
\end{proof}

\begin{eg}
	For multicommodity flow problem, we see that
	\[
		\begin{aligned}
			\min~ & \sum\limits_{(i, j)\in \mathcal{A}} c_{ij}x_{ij}                                                                                                                                                     \\
			      & \underbrace{\sum\limits_{j\colon (i, j)\in\mathcal{A}} x_{ij}}_{\text{flow out of }i} - \underbrace{\sum\limits_{j\colon (j, i)\in\mathcal{A}} x_{ji}}_{\text{flow into }i}  = b_i, i \in\mathcal{N} \\
			      & x_{ij}\geq 0 \leq u_{ij} \text{ for }(i, j)\in\mathcal{A}
		\end{aligned}
	\]
\end{eg}
\begin{explanation}
	Write it in the matrix form, we have
	\[
		\begin{aligned}
			\min~ & c^{\top}x                  \\
			      & Ax = b                     \\
			      & 0\leq \underline{x\leq u},
		\end{aligned}
	\]
	write it in another way, we have
	\[
		\begin{aligned}
			\min~ & c^{\top}x \\
			      & Ax = b    \\
			      & Ix\leq u  \\
			      & x\geq 0
		\end{aligned}
	\]
	with the dual variables \(y\) and \(\Pi\), we have the \hyperref[def:dual]{dual}
	\[
		\begin{aligned}
			\max~ & y^{\top}b + \Pi^{\top}u            \\
			      & y^{\top}A+\Pi^{\top}I\leq c^{\top} \\
			      & y\text{ unres.},\ \Pi\leq 0.
		\end{aligned}
	\]

	The \(A\) looks like
	\[
		A_{(m\times n)} = \substack{\text{Nodes }\\\mathcal{N}}\overset{\mathrm{arc}(i, j)}{
			\begin{pmatrix}
				 &  & \ddots &        & 0      &        &        &  & \\
				 &  &        & \ddots & \vdots &        &        &  & \\
				 &  & \ldots & \ldots & +1     & \ldots & \ldots &  & \\
				 &  &        &        & \vdots &        &        &  & \\
				 &  & \ldots & \ldots & -1     & \ldots & \ldots &  & \\
				 &  &        &        & \vdots & \ddots &        &  & \\
				 &  &        &        & 0      &        & \ddots &  & \\
			\end{pmatrix}}\substack{i\\ \\ \\ \\ \\ j}
	\]

	Then we see the \hyperref[def:dual]{dual} is just
	\[
		\begin{alignedat}{3}
			\max~ & \sum\limits_{i\in\mathcal{N}}y_{i}b_{i}+\sum\limits_{(i, j)\in\mathcal{A}} \Pi_{ij}u_{ij}                                        \\
			& y_{i} - y_{j}+\Pi_{ij}\leq c_{ij}                                                         && \text{ for all }(i, j)\in\mathcal{A} \\
			& \Pi_{ij}\leq 0                                                                            && \text{ for all }(i, j)\in\mathcal{A}.
		\end{alignedat}
	\]
\end{explanation}