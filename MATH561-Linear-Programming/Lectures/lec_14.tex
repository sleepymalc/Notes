\chapter{Sensitivity Analysis}
\lecture{14}{25 Oct. 08:00}{Sensitivity Analysis}
As usual, we start with the \hyperref[def:primal]{primal} and the \hyperref[def:dual]{dual}
\[
	\begin{aligned}
		\min~             & c^{\top}x \\
		                  & Ax = b    \\
		(\mathrm{P})\quad & x\geq  0,
	\end{aligned}\quad \begin{aligned}
		\max ~            & y^{\top}b               \\
		                  & y^{\top}A\leq c^{\top}. \\
		(\mathrm{D})\quad &
	\end{aligned}
\]
with an \hyperref[def:optimal-solution]{optimal} \hyperref[def:basic-partition]{basic partition} \(\beta, \eta\) such that
\[
	\overline{x} \coloneqq \begin{dcases}
		\overline{x}_{\beta} \coloneqq A_\beta^{-1} b \geq  \vec{0} \\
		\overline{x}_{\eta} \coloneqq \vec{0}
	\end{dcases}, \qquad \overline{y}^{\top} \coloneqq c_{\beta}^{\top}A^{-1}_{\beta}.
\]

\begin{prev}
	The \hyperref[def:dual]{dual} feasibility is
	\[
		\overline{c}_{\eta} \coloneqq c_{\eta} - c_{\beta}^{\top}A_{\beta}^{-1}A_{\eta} = c_{\eta} - \overline{y}^{\top}A_{\eta}\geq \vec{0}
	\]
	from \autoref{lma:lec7-2}.
\end{prev}

\section{Local Analysis}
\subsection{Right-Hand Side Changes}
We let
\[
	b\to b+\Delta_i e_i = \begin{pmatrix}
		b_1            \\
		b_2            \\
		\vdots         \\
		b_{i}+\Delta_i \\
		\vdots         \\
		b_m            \\
	\end{pmatrix},
\]
then
\[
	A_{\beta}^{-1}(b+\Delta_{i}e_{i}) = A_{\beta}^{-1}b +\Delta_{i}\underbrace{A_{\beta}^{-1}e_{i}}_{h^{i}},
\]
where \(h_{i}\) is the \(i^{th}\) column of \(A^{-1}_{\beta}\). So now we have
\[
	\overline{x}_{\beta} + \Delta_{i}h^i \geq \vec{0},
\]
where we need \(\beta, \eta\) to still be an \hyperref[def:optimal-solution]{optimal} \hyperref[def:basic-partition]{partition}.

\subsection{Objective Value}
Now, the objective value is
\[
	c_{\beta}^{\top}(\overline{x}_{\beta}+\Delta_{i}A_{\beta}^{-1}e_{i})+ c_{\eta}^{\top}\vec{0}
	= \underbrace{c_{\beta}^{\top}\overline{x}_{\beta}}_{\substack{\text{old obj.}\\\text{value}}}+\Delta_{i}\underbrace{c_{\beta}^{\top}A_{\beta}^{-1}}_{\overline{y}^{\top}}e_{i}
	= c_{\beta}^{\top}\overline{x}_{\beta}+\Delta_{i}\overline{y}^{\top}_i.
\]

\subsection{Analysis}
Let \(f\) be
\[
	\begin{aligned}
		f(b)\coloneqq \min~ & c^{\top}x \\
		                    & Ax = b    \\
		(P_b)\quad          & x\geq 0
	\end{aligned}
\]
where \(f\colon \mathbb{R}^{m}\to \mathbb{R}\). We see that since the \hyperref[def:optimal-solution]{optimal} objective value is equal for the \hyperref[def:dual]{dual} of \(P_b\), then \(f(b) = y^{\top}b\). Then
\[
	\frac{\partial f}{\partial b_{i}} = \overline{y}_i
\]
if \(\overline{x}_{\beta}>\vec{0}\).

\begin{problem*}
	For what values of \(\Delta_{i}\) is
	\[
		\overline{x}_{\beta} + \Delta_{i}h^i \geq \vec{0}?
	\]
\end{problem*}
\begin{answer}
	Firstly, we see that we need
	\[
		\overline{x}_{\beta_K}+\Delta_{i}h^i_K \geq 0 \text{ for }K = 1, \dots , m.
	\]

	Equivalently,
	\[
		\Delta_{i}h^i_K \geq -\overline{x}_{\beta_K},
	\]
	hence
	\[
		\begin{dcases}
			\Delta_{i}\geq \frac{-\overline{x}_{\beta_K}}{h^i_K}, & \text{ if }h^i_K>0, \\
			\Delta_{i}\leq \frac{-\overline{x}_{\beta_K}}{h^i_K}, & \text{ if }h^i_K<0.
		\end{dcases}
	\]

	We define \(L_{i}, U_{i}\) such that
	\[
		L_{i}\leq \Delta_{i}\leq U_{i}
	\]
	where
	\[
		L_{i} \coloneqq \max_{K\colon h^i_K > 0}\{-\overline{x}_{\beta_K}/h^i_K\},\qquad U_{i} \coloneqq \min_{K\colon h^i_K < 0}\{-\overline{x}_{\beta_K}/h^i_K\}.
	\]
\end{answer}
\begin{remark}
	Noting that if \(h^i_K\leq 0\) for all \(K\), then we define \(L_{i} \coloneqq -\infty \). Similarly, if \(h^i_K\geq 0\) for all \(K\), we define \(U_i \coloneqq \infty \).
\end{remark}

\section{Global Analysis}
We start with a theorem.

\begin{theorem}\label{thm:lec14-1}
	The domain of \(f\) is a \hyperref[def:convex-set]{convex set}.
\end{theorem}
\begin{proof}
	Assume that the \hyperref[def:dual]{dual} of \(P_b\) is \hyperref[def:feasible-solution]{feasible}, where we denote the \hyperref[def:dual]{dual} as \(D_b\):
	\[
		\begin{aligned}
			\max~      & y^{\top}b               \\
			(D_b)\quad & y^{\top}A\leq c^{\top}.
		\end{aligned}
	\]

	Now, the domain is the set of \(b\) such that \(P_b\) is \hyperref[def:feasible-solution]{feasible}. Mathematically,
	\[
		S\coloneqq \left\{ b\colon Ax = b, x\geq 0 \text{ are feasible.}\right\}\subseteq \mathbb{R}^m.
	\]
	Suppose \(b^1, b^2\in S\). We want to check
	\[
		\lambda b^{1}+(1-\lambda) b^2 \in S\text{ for }0<\lambda<1.
	\]
	Notice that there is an \(x^1\) such that
	\[
		Ax^1 = b^1, x^1\geq \vec{0}
	\]
	and there is an \(x^2\) such that
	\[
		Ax^2 = b^2, x^2\geq \vec{0}.
	\]

	Firstly, we check that \(\lambda x^{1}+(1 - \lambda)x^2\) is non-negative. This is clear since all components are non-negative. Then we check
	\[
		A(\lambda x^{1}+(1 - \lambda)x^2) = \lambda b^1 + (1 - \lambda)b^2.
	\]
	This is clear since
	\[
		A(\lambda x^{1}+(1 - \lambda)x^2) = \lambda Ax^1 + (1 - \lambda)Ax^2 = \lambda b^1 + (1-\lambda)b^2.
	\]
\end{proof}

We now introduce the \hyperref[def:convex-function]{convexity of a function}.

\begin{definition}[Convex function]\label{def:convex-function}
	A function \(f\colon S \to \mathbb{R} \) is \emph{convex} if the domain \(S\) is \hyperref[def:convex-set]{convex} and for all \(x^1, x^2\in S\) and \(0 < \lambda < 1\),
	\[
		f(\lambda x^1+(1 - \lambda)x^2)\leq \lambda f(x^1)+(1-\lambda)f(x^2).
	\]
\end{definition}

\begin{figure}[H]
	\centering
	\incfig{convex-function}
	\caption{\hyperref[def:convex-function]{Convex function}.}
	\label{fig:convex-functiona}
\end{figure}

\subsection{Affine Function}
Before we go further, we need to have several definitions.
\begin{definition}[Affine function]\label{def:affine-function}
	A function \(f\colon \mathbb{R}^{m}\to \mathbb{R}\) is \emph{affine} if
	\[
		f(u_1, u_2, \dots , u_m) = a_0 + \sum\limits_{i=1}^{m} a_{i}u_{i}
	\]
	where \(a_i \in\mathbb{R}\) for \(i = 0, \dots , m\).
\end{definition}

\begin{remark}
	If \(a_0 = 0\), then \(f\) is a linear function.
\end{remark}

\begin{definition}[Convex piece-wise linear function]\label{def:convex-piece-wise-linear-function}
	A function \(f\colon \mathbb{R}^{m}\to \mathbb{R}\) is \emph{convex piece-wise linear} if \(f\) is the point-wise maximum of \hyperref[def:affine-function]{affine functions}.
\end{definition}

\begin{figure}[H]
	\centering
	\incfig{convex-piecewise-linear-function}
	\caption{\hyperref[def:convex-piece-wise-linear-function]{Convex piece-wise linear function}.}
	\label{fig:convex-piecewise-linear-function}
\end{figure}
Now, suppose \(f_{i}\colon \mathbb{R}^m \to \mathbb{R}\) for \(i = 1, \dots , K\) and assume that each is \hyperref[def:affine-function]{affine}.
Then define
\[
	f(x)\coloneqq \max_{1\leq i\leq K} \left\{ f_i(x) \right\}.
\]

\begin{theorem}
	The point-wise maximum of \hyperref[def:affine-function]{affine function} is a \hyperref[def:convex-function]{convex function}.
\end{theorem}
\begin{proof}
	We see that
	\[
		\begin{split}
			f(\lambda x^1 + (1 - \lambda)x^2) &=\max_{1\leq i\leq K}\left\{ f_i(\lambda x^1 + (1 - \lambda)x^2) \right\}\\
			&=\max_{1\leq i\leq K}\left\{ \lambda f_{i}(x^1) + (1 - \lambda)f_{i}(x^2) \right\}\\
			&\geq \max_{1\leq i\leq K}\left\{ \lambda f_{i}(x^1)\right\} + \max_{1\leq i\leq K}\left\{(1 - \lambda)f_{i}(x^2) \right\}\\
			&=\lambda\max_{1\leq i\leq K}\left\{f_{i}(x^1)\right\} + (1 - \lambda)\max_{1\leq i\leq K}\left\{f_{i}(x^2) \right\}
			= \lambda f(x^1)+(1 - \lambda)f(x^2),
		\end{split}
	\]
	where the second equality follows from
	\[
		\max_{1\leq i\leq K}\left\{ a_{i0}+\sum\limits_{l=1}^{m} a_{il}(\lambda u^1_l + (1 - \lambda)u^2_l) \right\}
		= \max_{1\leq i\leq K}\left\{ \lambda a_{i0}+(1 - \lambda)a_{i0} + \sum\limits_{l=1}^{m} a_{il}(\lambda u^1_l + (1 - \lambda)u^2_l) \right\}.
	\]
\end{proof}