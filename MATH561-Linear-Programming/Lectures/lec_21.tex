\lecture{21}{22 Nov. 08:00}{Cutting-Stock Problem}
It's now a good timing to introduce an application of what we have been discussing, namely the \hyperref[prob:cutting-stock-problem]{cutting-stock problem}. We'll see that it naturally utilize the idea of column generation. It's an integer programming problem, though contrarily, it can be nicely approximated by \textbf{rounding down}.

\begin{problem}[Cutting-stock problem]\label{prob:cutting-stock-problem}
Consider we have rolls of paper of width \(W\), with the demand widths being \(w_1, w_2, \ldots , w_m < W\) and demands being \((\mathrm{D})\), which is usually pretty big. The goal is to use as few stock rolls as possible.
\end{problem}
\begin{answer}
	One may try to define
	\[
		x_{ij}\coloneqq \# \text{ of rolls of width \(w_{i}\) to cut from stock roll \(j\)}.
	\]
	But we immediately see that the number of variables is huge for an integer programming, hence this doesn't work. Instead, we denote a \emph{pattern} as a vector \(a\) being
	\[
		a \coloneqq \begin{pmatrix}
			a_1    \\
			a_2    \\
			\vdots \\
			a_m    \\
		\end{pmatrix},
	\]
	where \(a_{i} = \#\text{ of pieces of width \(w_{i}\) to cut using this pattern}\). Then the constraints for a pattern is
	\[
		\begin{aligned}
			 & \sum\limits_{i=1}^{m} w_{i}a_{i}\leq W                   \\
			 & \mathbb{N}\ni a_{i}\geq 0 \text{ for }i = 1, \ldots , m.
		\end{aligned}
	\]
	Moreover, denotes \((\mathrm{D})\) as
	\[
		d\coloneqq \begin{pmatrix}
			d_1    \\
			d_2    \\
			\vdots \\
			d_m    \\
		\end{pmatrix},
	\]
	then we formulate the cutting-stock problem as
	\[
		\begin{aligned}
			z\coloneqq \min~    & \sum\limits_{j} x_{j}                  \\
			                    & \sum\limits_{j} A_{\cdot j}x_{j}\geq d \\
			(\mathrm{CSP})\quad & x_{j}\geq 0 \text{ integer for all }j,
		\end{aligned}
	\]
	where
	\[
		A_{\cdot j} = \begin{pmatrix}
			a_{1j} \\
			a_{2j} \\
			\vdots \\
			a_{mj} \\
		\end{pmatrix}.
	\]

	Turning \(\mathrm{CSP}\) into a \hyperref[def:standard-form]{standard form} problem and \textbf{drop} the integer constraint, we have
	\[
		\begin{aligned}
			\min~                           & \sum\limits_{j} x_{j}                                                         \\
			                                & \sum\limits_{j} A_{\cdot j}x_{j} - t = d                                      \\
			(\mathrm{\underline{CSP}})\quad & x_{j}\geq 0 \text{ for all }j,\ t_{i}\geq 0\text{ for all }i = 1, \ldots , m.
		\end{aligned}
	\]

	\begin{note}
		\(\mathrm{\underline{CSP}}\) gives a lower bound on \(\mathrm{CSP}\). Moreover, the constraint of \(x_{j}\in\mathbb{N}\)
		is now gone.
	\end{note}

	We now want to solve \(\mathrm{\underline{CSP}}\) exactly to get \hyperref[def:optimal-solution]{optimum} \(\overline{x}, \overline{t}\) with value \(\underline{z} = \sum\limits_{i=1}^{m} \overline{x}_{i}\).

	Firstly, if we round up \(\overline{x}\) to \(\left\lceil \overline{x} \right\rceil \), then it is \hyperref[def:feasible-solution]{feasible} for \(\mathrm{CSP}\). We immediately see
	\[
		\sum\limits_{i=1}^{m} \overline{x}_{i} = \underline{z} \leq z \leq \sum\limits_{i=1}^{m} \left\lceil \overline{x}_{i} \right\rceil.
	\]
	Since we also have
	\[
		\left\lceil \sum\limits_{i=1}^{m} \overline{x}_{i} \right\rceil \leq z,
	\]
	hence we see that the rounding up solution \(\left\lceil \overline{x} \right\rceil \) is within \(m-1\) of \hyperref[def:optimal-solution]{optimum}.

	Now we consider how to solve \(\mathrm{\underline{CSP}}\) exactly. Denotes the \hyperref[def:dual]{dual} variables of \(\mathrm{\underline{CSP}}\) being \(y\) such that
	\[
		y = \begin{pmatrix}
			y_1    \\
			\vdots \\
			y_{m}  \\
		\end{pmatrix}.
	\]
	Suppose \(\overline{y}\) is a \hyperref[def:dual-basic-solution]{basic dual solution}. Then the \hyperref[def:reduced-cost]{reduced cost} of a variable is:
	\begin{itemize}
		\item \(t_{i}\):
		      \[
			      0 - \overline{y}^{\top}(-e_{i}) = \overline{y}_{i}.
		      \]
		      Hence, if \(\overline{y}_{i}<0\), \(t\) can enter the \hyperref[def:basis]{basis}.
		\item \(x_{j}\):
		      \[
			      1 - \overline{y}^{\top}A_{\cdot j} = 1 - \sum\limits_{i=1}^{m} \overline{y}_{i}a_{ij}.
		      \]
		      If this is \(<0\), then \(x_{j}\) can enter the \hyperref[def:basis]{basis}. To drive some quantity negative, we simply set up a minimization problem. Specifically, we set up a linear program such that
		      \[
			      \begin{aligned}
				      \min~ & 1 - \sum\limits_{i=1}^{m} \overline{y}_{i}a_{ij} \\
				            & \sum\limits_{i=1}^{m} w_{i}a_{ij}\leq W          \\
				            & a_{ij}\geq 0 \text{ integers}.
			      \end{aligned}
		      \]
		      Equivalently,
		      \[
			      \begin{aligned}
				      1 - \max~ & \sum\limits_{i=1}^{m} \overline{y}_{i}a_{i}         \\
				                & \sum\limits_{i=1}^{m} w_{i}a_{i}\leq W              \\
				                & a_{i}\geq 0 \text{ integers for }i = 1, \ldots , m.
			      \end{aligned}
		      \]
		      This is known as the \emph{\href{https://en.wikipedia.org/wiki/Knapsack_problem}{Knapsack problem}}. Now, let \(f(S)\) being the \hyperref[def:optimal-solution]{optimal} value for knapsack of capacity \(S\) such that \(S = 0, 1, \ldots , W\). We see that
		      \[
			      \begin{split}
				      f(0) &= 0\\
				      &\vdots\\
				      f(S) &= \max_{i\colon w_{i}\leq S}\left\{ \overline{y}_{i} + f(S - w_{i})\right\}\\
				      &\vdots\\
				      f(W) &= \text{ solution}.\\
			      \end{split}
		      \]
		      The running time is \(\Theta(Wm)\).\footnote{Note that we assume \(W\) and \(w_{i}\) to be integers.}

		      Notice that the above only gives \(f(W)\), which is the objective value, but without information for variables. We can retrieve the information by keeping tracking of the argument of maximum in each step, namely we record
		      \[
			      \begin{split}
				      i^{\ast}_{0 }\to f(0) &= 0\\
				      &\vdots\\
				      i^{\ast}_S \to f(S) &= \max_{i\colon w_{i}\leq S}\left\{ \overline{y}_{i} + f(S - w_{i})\right\}\\
				      &\vdots\\
				      i^{\ast}_W \to f(W) &= \text{ solution}.\\
			      \end{split}
		      \]
		      Then we simply \emph{back-track} every \(i^{\ast}\) from \(i^{\ast}_W\), and then the next one is simply \(i^{\ast}_{W - w_{i^{\ast}_W}}\), and so on.
	\end{itemize}
\end{answer}