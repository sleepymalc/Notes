\chapter{Introduction}
\lecture{1}{21 Jan.\ 9:30}{Overview}

In this course, we will consider undirected, unweighted, and finite graph \(G = (V, E)\). Given a graph \(G = (V, E)\), for any \(x, y \in V\), we define \(\omega _{xy} \coloneqq \mathbbm{1}_{(x, y) \in E} \) as the indicator of \((x, y)\) in \(E\).

\section[Structure]{Structure\protect\footnote{Later (after this section), we will not reference back to definitions defined here due to their elementary nature.}}
One of the fundamental structures in a graph is the \hyperref[def:connected-component]{connected component}, where we now define.

\begin{definition}[Connected]\label{def:connected}
	Given a graph \(G = (V, E)\), we say \(x, y \in V\) is \emph{connected}, denoted as \(x \leftrightarrow y\), if there exists a path \(x = v_1, \dots, v_k = y\) such that \(\omega _{v_i v_{i+1}} = 1\) for all \(1 \leq i \leq k-1\).
\end{definition}

It's easy to see that \(\leftrightarrow \) is an equivalent relation, hence, one can define the so-called \hyperref[def:connected-component]{connected component}, which is an equivalent class of \(G\) with \(\leftrightarrow \).

\begin{definition}[Connected component]\label{def:connected-component}
	Given a graph \(G\), a \emph{connected component} \(\mathcal{C} \subseteq V\) is a maximal\footnote{Note the wording: it's not equivalent to maximum.} size subset of \(V\) such that for all \(x, y \in \mathcal{C} \), \(x \leftrightarrow y\).
\end{definition}

\begin{notation}
	For a particular vertex \(v \in V\), we define \(\mathcal{C} (v, G) \coloneqq \{ u \mid u \leftrightarrow v \text{ in } G\} \) as the \hyperref[def:connected-component]{connected component} containing \(v\). If \(G\) is realized, we simply write \(\mathcal{C} (v)\).
\end{notation}

\hyperref[def:connected-component]{Connected component} is an example of \emph{structure}. We list some common structures below:

\begin{definition}[Triangle]\label{def:triangle}
	A \emph{triangle} \((v_1, v_2, v_3)\) in a graph \(G = (V, E)\) is such that \((v_1, v_2), (v_2, v_3)\), and \((v_3, v_1)\) are in \(E\).
\end{definition}

\begin{definition}[Cycle]\label{def:cycle}
	A \emph{\(n\)-cycle} \((v_1, \dots , v_n)\) in a graph \(G = (V, E)\) is such that \((v_i, v_{i+1})\) and \((v_k, v_0)\) are in \(E\).
\end{definition}

\begin{definition}[Clique]\label{def:clique}
	A \emph{\(n\)-clique} \(K_n \subseteq V\) in a graph \(G = (V, E)\) is such that for every \(v_i, v_j \in K_n\), \((v_i, v_j) \in E\).
\end{definition}

\begin{eg}
	It's clear that a \hyperref[def:triangle]{triangle} is just a \hyperref[def:cycle]{\(3\)-cycle} while also a \hyperref[def:clique]{\(3\)-clique}.
\end{eg}

A central problem we will be asking is the following:

\begin{problem*}[Subgraph count]
	Whether a graph contains a certain structure; if yes, how many?
\end{problem*}

\section{Random Graph and Random Graph Process}
We are interested in certain graph models where when the number of vertices grows, some structures emerge. The most famous (and simple) random graph model is the \hyperref[def:Erdős-Rényi-random-graph]{Erdős-Rényi random graph} model.

\begin{definition}[Erdős-Rényi random graph]\label{def:Erdős-Rényi-random-graph}
	The \emph{Erdős-Rényi random graph} model, denoted as \(G(n, p)\) or \(\operatorname{ER}(n, p) \), is a random graph generated on \(n\) vertices such that any two vertices are connected with probability \(p \in (0, 1)\) independently.
\end{definition}

\begin{note}
	There are lots of independence and symmetry, leading to closed forms for many calculations.
\end{note}

To get a less restrictive model, one can also consider inhomogeneous model, where we let \(p_{xy}\) differ for different pairs of \((x , y) \in V \times V\). On the other hand,  to relax edge independence, the so-called \emph{exponential random graph model} exists.

\begin{remark}
	These model all have light-tail. There are also models with heavy tail behavior, e.g., random graph with specified degree distribution, and preferential attachment model.
\end{remark}

It's natural to view these random graph model by a random sequence of graphs, which we call graph process. People are interested in several optimization problems of such a graph process.

\begin{eg}[Optimization on graph process]
	Given a graph process, what's the (expected) number of the largest cycle, or what's the minimum spanning tree, or some maximum weight problem.
\end{eg}

On the other hand, we can also consider another layer of randomness, where we are given a fixed graph, and consider stochastic processes on this graph.

\begin{eg}
	Infection model on a social network, or a growth process.
\end{eg}

Some other more advanced topics include Gibbs measures, spin model (Ising model and its generalization Potts model), and spin glass model.

\chapter{Erdős-Rényi Random Graph}
In this chapter, we first look at the simplest random graph model, the \hyperref[def:Erdős-Rényi-random-graph]{Erdős-Rényi random graph}.

\begin{prev}[Erdős-Rényi random graph]
	Let \(V = [n] \coloneqq \{ 1, \dots , n \} \) and \(p \in [0, 1]\). For every \(1 \leq i < j \leq n\), we let \(\omega _{ij} \overset{\text{i.i.d.} }{\sim } \operatorname{Ber}(p) \), which induces \(E \coloneqq \{ (i, j) \mid \omega _{ij} = 1, 1 \leq i < j \leq n\} \).
\end{prev}

Due to the independence and the simplicity, we get several immediate results.

\begin{claim}
	The number of edges converges in distribution to a standard normal, in particular,
	\[
		\frac{\lvert E \rvert - \binom{n}{2}p}{\sqrt{\binom{n}{2}p (1 - p)} }
		\overset{D}{\to } \mathcal{N} (0, 1),
	\]
	if and only if \(\binom{n}{2}p (1 - p) \to \infty \). As a corollary, we have \(\lvert E \rvert / \binom{n}{2}p \approx 1\).
\end{claim}
\begin{explanation}
	We see that \(\lvert E \rvert = \sum_{1 \leq i < j \leq n} \omega _{ij} \sim \operatorname{Bin}(\binom{n}{2}, p) \), hence, \(\mathbb{E}_{}[\lvert E \rvert ] = \binom{n}{2}p = n(n-1) p / 2\). Then, the result follows directly from the central limit theorem.
\end{explanation}

Now it's a good time to bring up another random graph model, \(\overline{\operatorname{ER}} (n, m)\), where we sample a graph with \(n\) vertices and \(m\) edges uniformly. This is actually the original \hyperref[def:Erdős-Rényi-random-graph]{Erdős-Rényi random graph} model.

\begin{remark}
	If \(m \approx \binom{n}{2}p\), the results often transfer between \(\operatorname{ER}(n, p)\) and \(\overline{\operatorname{ER}} (n, m)\).
\end{remark}

\section{Density and Phase Transition}
\subsection{Dense and Sparse Graph\protect\footnote{Again, since sparse/dense are so elementary, we will not reference back to definitions defined here.}}
We now introduce the concept of \emph{dense} and \emph{sparse} graph, which is decided by the parameter \(\lvert E \rvert / \binom{n}{2} \).

\begin{definition*}
	Consider a graph \(G = (V, E)\) with \(\lvert V \rvert = n\) and \(\lvert E \rvert = m\).
	\begin{definition}[Dense graph]\label{def:dense-graph}
		\(G\) is \emph{dense} if there exists a constant \(\epsilon > 0\) such that \(m / \binom{n}{2} > \epsilon \).
	\end{definition}

	\begin{definition}[Sparse graph]\label{def:sparse-graph}
		\(G\) is \emph{sparse} if the average degree is constant, i.e., \(m = O(n)\).
	\end{definition}
\end{definition*}

Let's first observe an interesting property for the \hyperref[def:Erdős-Rényi-random-graph]{Erdős-Rényi random graph} model. Note that the typical degree of the \hyperref[def:Erdős-Rényi-random-graph]{Erdős-Rényi random graph} is some constant since for \(\operatorname{ER}(n, p)\),
\[
	\frac{1}{\lvert V \rvert } \sum_{v \in V} \deg (v)
	= \frac{2 \lvert E \rvert }{\lvert V \rvert }
	\approx \frac{2 n (n-1)}{2} \frac{p}{n}
	= (n-1) p.
\]

\begin{note}
	Regime hence depends on \(\lambda \coloneqq np\) for some \(\lambda \). When \(\lambda \in (0, \infty )\), we are in the \hyperref[def:sparse-graph]{sparse} regime.
\end{note}

In particular, when \(\lambda \in (0, \infty )\), the degree of a particular vertex follows \(\operatorname{Bin}(n-1, p) = \operatorname{Bin}(n-1, \lambda / n) \).

\begin{claim}
	If \(\lambda \in (0, \infty )\), \(\operatorname{Bin}(n-1, \lambda / n) \overset{D}{\to} \operatorname{Pois}(\lambda ) \) as \(n \to \infty \).
\end{claim}
\begin{explanation}
	We see this in a straightforward way: for any \(k\), \(X \sim \operatorname{Bin}(n-1, \lambda / n) \) has a pmf
	\[
		\Pr_{}(X = k)
		= \binom{n-1}{k} \cdot \left( \frac{\lambda}{n} \right) ^{k} \cdot \left( 1 - \frac{\lambda}{n} \right) ^{n-1-k}
		\to \frac{\lambda ^k}{k!} e^{-\lambda },
	\]
	which is the pmf of \(\operatorname{Pois}(\lambda ) \). Hence, by definition, \(\operatorname{Bin}(n - 1, \lambda / n) \overset{D}{\to} \operatorname{Pois}(\lambda ) \). Another proof is based on the total variational distance \(d_{\mathrm{TV} }\).

	\begin{prev}[Total variational distance]
		For the discrete case, given two discrete probability distributions \(p, r\) with a finite support \(\Omega \),
		\[
			d_{\mathrm{TV} }((p_k)_{k \in \Omega }, (r_k)_{k \in \Omega })
			\coloneqq \frac{1}{2} \sum_{k \in \Omega } \lvert p_k - r_k \rvert.
		\]
	\end{prev}

	Now, consider the empirical degree distribution defined as \(d^{(n)} \coloneqq \frac{1}{n} \sum_{v} \delta _{\deg(v)}\). We see that
	\[
		d_{\mathrm{TV} } \left( d^{(n)} , \operatorname{Pois}(\lambda ) \right)
		= \frac{1}{2} \sum_{k=0}^{n} \left\lvert \frac{\lvert \{ v \mid \deg(v) = k \}  \rvert }{n} - \frac{e^{-\lambda } \lambda ^k}{k!} \right\rvert ,
	\]
	and by Jensen's inequality,
	\[
		\mathbb{E}_{}\left[d_{\mathrm{TV} } \left( p^{(n)} , \operatorname{Pois}(\lambda ) \right) \right]
		\leq \frac{1}{2} \sum_{k=0}^{n} \sqrt{ \mathbb{E}_{}\left[ \left( \frac{\lvert \{ v \mid \deg(v) = k \}  \rvert }{n} - \frac{e^{-\lambda } \lambda ^k}{k!} \right) ^2 \right] }
		\approx \sqrt{\frac{p_k}{n}}
		= O\left( \frac{1}{\sqrt{n} } \right) ,
	\]
	where \(p_k = e^{-\lambda } \lambda ^k / k!\).
\end{explanation}

The above gives a distance-one neighborhood characterization of \(\operatorname{ER}(n, p)\). However, this actually gives a higher-level picture on larger neighborhoods, in particular, the \hyperref[def:connected-component]{connected component}.

\begin{notation}
	Given a graph \(G\), let \(\mathcal{C} _{\max _i}\) denotes the \(i^{\text{th} }\) largest \hyperref[def:connected-component]{connected component} in \(G\). For convenient, we use \(\mathcal{C} _{\max }\) to denote \(\mathcal{C} _{\max _1}\) when it's clear from the context.
\end{notation}

\subsection{Phase Transition of Component Size}
Our goal in this section is to prove the following theorem about the components size in \(\operatorname{ER}(n, \lambda / n) \):

\begin{theorem}\label{thm:Erdős-Rényi-phase-transition}
	Consider the \hyperref[def:Erdős-Rényi-random-graph]{Erdős-Rényi random graph} model \(\operatorname{ER}(n, \lambda / n)\) for some \(\lambda > 0\).
	\begin{enumerate}[(a)]
		\item\label{thm:Erdős-Rényi-phase-transition-a} If \(\lambda < 1\), the graph is disconnected with high probability such that \(\lvert \mathcal{C} _{\max _1} \rvert = \Theta _p (\log n)\). In particular, if \(a (\lambda - 1 - \log \lambda ) > 1\), as \(n \to \infty \), we have \(\Pr_{}\left( \lvert \mathcal{C} _{\max _1} \rvert \geq  a \log n \right) \to 0\).
		\item\label{thm:Erdős-Rényi-phase-transition-b} If \(\lambda > 1\), \(\frac{1}{n} \lvert \mathcal{C} _{\max _1} \rvert \) converges to a constant, i.e., there exists a giant \hyperref[def:connected-component]{component}. Moreover, \(\mathcal{C} _{\max _2}\) has size of \(O(\log n)\).
		\item\label{thm:Erdős-Rényi-phase-transition-c} At \(\lambda = 1\), the random vector \(\frac{1}{n^{2 / 3}} (\lvert \mathcal{C} _{\max _1} \rvert , \lvert \mathcal{C} _{\max _2} \rvert , \dots )\) converges in distribution to a non-trivial limit.
	\end{enumerate}
\end{theorem}

\autoref{thm:Erdős-Rényi-phase-transition} says that in the \hyperref[def:sparse-graph]{sparse} regime, there is a phase transition at \(\lambda = 1\). When \(\lambda < 1\), there will not exist large \hyperref[def:connected-component]{component}; if \(\lambda > 1\), the largest \hyperref[def:connected-component]{component} is of constant fractional of the entire graph, and at \(\lambda = 1\), it's something in between.

\begin{intuition}
	Consider the extremely \hyperref[def:sparse-graph]{sparse} regime where \(\lambda < 1\). We give a heuristic argument of why there can't exist a large \hyperref[def:connected-component]{component}. The neighborhood structure of some vertex \(v\), which should be tree-like, at least locally. This is because, for any \(k \geq 2\), the expected number of cycles of length \(k\) in this structure is
	\[
		\binom{n}{k} \cdot k! \cdot \left( \frac{\lambda}{n} \right) ^k
		\approx n^k \cdot \frac{\lambda ^k}{n^k}
		= \lambda ^k,
	\]
	which implies that when \(\lambda < 1\),
	\begin{equation}\label{eq:Erdős-Rényi-graph-extremely-sparse-cycle-count}
		\sum_{k=2}^{n} \mathbb{E}_{}[\# \text{cycle of length } k]
		\leq \frac{1}{1 - \lambda }.
	\end{equation}
	Hence, in this regime, for a random vertex \(v\), up to any finite distance \(k\), we will only see few cycles.
	\begin{center}
		\incfig{Erdős-Rényi-1-neighborhood}
	\end{center}
\end{intuition}

Formally, by viewing the neighborhood structure as a branching process, one can bound its size.