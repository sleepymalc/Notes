\lecture{7}{11 Feb.\ 9:30}{}
\begin{prev}
	When \(\lambda = pn > 1\), we see giant component. When \(\lambda > \log n / k\), we start to see cluster of size \(k\) vanish since
	\[
		\begin{split}
			\mathbb{E}_{}[\text{\# clusters of size \(k\)}]
			 & \leq \mathbb{E}_{}[\text{\# spanning trees of size \(k\) which is a cluster} ]                                                \\
			 & \leq k^{k-2} \cdot \binom{n}{k} \cdot \left( \frac{\lambda }{n} \right) ^{k-1} \left( 1 - \frac{\lambda}{n} \right) ^{k(n-k)} \\
			 & \leq k^{k-2} \cdot \frac{n^k}{e^{-k} k^{k + 1 / 2}} \cdot \frac{\lambda ^{k-1}}{n^{k-1}} \cdot e^{- \lambda k(n-k) / n}
			= \frac{n}{\lambda k^{5 / 2}} \left( e \lambda e^{- \frac{\lambda (n-k)}{n}} \right) ^k.
		\end{split}
	\]
	When \(k\) is fixed and \(\lambda k^2 \ll n\), the above is bounded by \(\frac{n}{\lambda } (e \lambda e^{-\lambda })^k\). Hence, when \(\lambda = (1 + \epsilon ) \log n / k\) for some \(\epsilon > 0\), this bound goes to \(0\) as \(n \to \infty \).
\end{prev}

\begin{theorem}
	Let \(G \sim \operatorname{ER}(n, \lambda / n)\) with \(\lambda = \log n + c\) for some constant \(c > 0\). Let \(Z \coloneqq N_{\mathcal{T} _1}\) be the number of isolating vertices in \(G\). Then, as \(n \to \infty \), \(Z \overset{D}{\to} \operatorname{Pois}(e^{-c})\), and in particular, \(\Pr_{}\left(\text{no isolated vertices} \right) \to e^{-e^{-c}}\).
\end{theorem}
\begin{proof}
	We see that
	\[
		\mathbb{E}_{}[N_1]
		= n \Pr_{}\left(\text{vertex \(1\) is isolated} \right)
		= n (1 - p)^{n-1}
		= n \left( 1 - \frac{\log n + c}{n} \right) ^{n-1}
		\approx n e^{-\log n - c + o(1)}
		\to e^{-c}.
	\]
	One can use \hyperref[thm:Stein-Chen-method]{Stein-Chen method} or moment method. Fix \(k \geq 2\). Then,
	\[
		\begin{split}
			\mathbb{E}_{}[(N_1)_k]
			 & = \mathbb{E}_{}\left[\sum_{i_1, \dots , i_k \in [n]} \mathbbm{1}_{i_1, \dots , i_k \text{ are isolated} } \right] \\
			 & = (n)_k \cdot \Pr_{}\left(\text{vertices } 1, 2, \dots , k \text{ are isolated} \right)                           \\
			 & \approx n^k (1-p)^{k (n-k)} (1 + o(1))
			\approx \left( n(1-p)^n \right) ^k
			\to (e^{-c})^k.
		\end{split}
	\]
	Since this is true for all \(k\), from \autoref{lma:Poisson-moment-convergence}, we're done.
\end{proof}


\begin{theorem}
	Let \(G \sim \operatorname{ER}(n, \lambda / n)\) with \(\lambda = \log n + c\) for some constant \(c > 0\). Then, as \(n \to \infty \),
	\[
		\Pr_{}\left(G \text{ is connected} \right)
		\to e^{-e^{-c}}.
	\]
\end{theorem}
\begin{proof}
	Since \(\{ \text{connected} \} \subseteq \{ \text{no isolated vertices}  \} \), we see that
	\[
		\begin{split}
			0
			 & \leq \Pr_{}\left(\text{no isolated vertices} \right) - \Pr_{}(\text{connected} )                                                                                                                                                                                                                     \\
			 & = \Pr_{}(\exists \text{a cluster of size } k \in \{ 2, 3, \dots , \lceil n / 2 \rceil  \} )                                                                                                                                                                                                          \\
			 & \leq \sum_{k=2}^{n / 2} \mathbb{E}_{}[\text{\# clusters of size } k]                                                                                                                                                                                                                                 \\
			 & \leq \sum_{k=2}^{n / 2} \frac{n}{\lambda k^{5 / 2}} \left( e \lambda e^{-\frac{\lambda (n-k)}{n}} \right) ^k                                                                                                                                                                                         \\
			 & = \underbrace{\sum_{k=2}^{10} \frac{n}{\lambda k^{5 / 2}} \left( e \lambda e^{-\frac{\lambda (n-k)}{n}} \right) ^k}_{\to 0} + \underbrace{\sum_{k=11}^{n / 2} \frac{n}{\lambda k^{5 / 2}} \left( e \lambda e^{-\frac{\lambda (n-k)}{n}} \right) ^k}_{\leq n (e \lambda e^{-\lambda / 2})^{10} \to 0}
			\to 0.
		\end{split}
	\]
\end{proof}

\begin{prev}
	Now, fix \(k \geq 1\), recall that \(N_k \coloneqq \sum_{v=1}^{n} \mathbbm{1}_{\deg (v) = k} \). Then,
	\[
		\mathbb{E}_{}[N_k]
		= n \cdot \Pr_{}(\deg (1) = k)
		= n \cdot \binom{n-1}{k} \cdot \left( \frac{\lambda}{n} \right) ^k \left( 1 - \frac{\lambda}{n} \right) ^{n-1-k}
		\approx n \frac{n^k}{k!} \frac{\lambda ^k}{n^k} \cdot e^{-\lambda (1 - \frac{k+1}{n})}.
	\]
	We want \(n \lambda ^k e^{-\lambda } = \Theta (1)\), hence \(\lambda = \log n + \log \log^k n + c\). When we choose this \(\lambda \), \(\mathbb{E}_{}[N_k] \to e^{-c}\).
\end{prev}

\begin{theorem}
	For \(G \sim \operatorname{ER}(n, \lambda / n) \) with \(\lambda = \log n + k \log \log n + c\) for some fixed \(c \in \mathbb{R} \). Then, \(N_k \overset{D}{\to} \operatorname{Pois}(e^{-c}) \) for any fixed \(k \geq 0\).
\end{theorem}

\begin{definition}[Hamiltonian cycle]\label{def:Hamiltonian-cycle}
	A \emph{Hamiltonian cycle} in a graph \(G = (V, E)\) is an \hyperref[def:cycle]{\(n\)-cycle} with \(n = \lvert V \rvert \), i.e., a \hyperref[def:cycle]{cycle} passing through all vertices.
\end{definition}

It's obvious that \(\{ \text{existence of \hyperref[def:Hamiltonian-cycle]{Hamiltonian cycle}} \} \subseteq \{ \deg (v) \geq 2 \text{ for all } v \in [n] \} = \{ N_0 = N_1 = 0\} \).

\begin{theorem}
	For \(G \sim \operatorname{ER}(n, \lambda / n) \) with \(\lambda = \log n + \log \log n + c\) for some fixed \(c \in \mathbb{R} \),
	\[
		\Pr_{}(G\text{ is \hyperref[def:Hamiltonian-cycle]{Hamiltonian}} )
		\to e^{-e^{-c}}.
	\]
\end{theorem}

When \(p\) increases, small clusters will vanish, small degree will vanish, ``more structure'' will appear.

Fix a triangle graph \(H = \triangle\). Then, we're interested in \(N_H\), the number of copies of \(H\) in the graph \(G\). We see that
\[
	N_{\triangle}
	= \sum_{1 \leq i < j < k \leq n} \mathbbm{1}_{(i, j), (j, k), (k, i) \in E}.
\]
It's easy to see that
\[
	\mathbb{E}_{}[N_{\triangle}]
	= \binom{n}{3} p^3
	\approx \frac{(np)^3}{6}
\]
What about the variance?

\subsection{Chaos Decomposition}
Recall that \(\omega _{ij} = \mathbbm{1}_{(i, j) \in E} \overset{\text{i.i.d.} }{\sim } \operatorname{Ber}(p) \) for all \(i < j\). Hence,
\[
	N_{\triangle}
	= \sum_{i < j < k \in [n]} \omega _{ij} \omega _{jk} \omega _{ki}
	= \sum_{i < j < k \in [n]} ( \overline{\omega }_{ij} + p ) ( \overline{\omega }_{jk} + p ) ( \overline{\omega }_{ki} + p ) ,
\]
where \(\overline{\omega } \coloneqq \omega - p\). It's easy to see that
\[
	N_{\triangle}
	= \underbrace{\sum_{i < j < k} \overline{\omega }_{ij}\overline{\omega }_{jk}\overline{\omega }_{ki}}_{A_3}
	+ \underbrace{p \sum_{i, j < k} \overline{\omega }_{ij} \overline{\omega }_{ik} }_{A_2}
	+ \underbrace{(n-2) p^2 \sum_{i < j} \overline{\omega }_{ij} }_{A_1}
	+ \binom{n}{2}p^3.
\]
We see that
\[
	\begin{dcases}
		\mathbb{E}_{}[A_3] = 0, & \Var_{}[A_3] = \binom{n}{3} p^3 (1 - p)^3 \approx n^3 p^3 = (np)^3;                            \\
		\mathbb{E}_{}[A_2] = 0, & \Var_{}[A_2] = p^2 \cdot n \binom{n-1}{2}\cdot p^2 (1 - p)^2 \approx n^3 p^4 = (np)^3 \cdot p; \\
		\mathbb{E}_{}[A_1] = 0, & \Var_{}[A_1] = (n-2)^2 p^4 \cdot \binom{n}{2} p (1 - p) \approx n^4 p^5 = (np)^3 \cdot np^2.
	\end{dcases}
\]

\begin{theorem}
	Let \(np^2 \to \infty \), and \(n^2 p(1 - p) \to \infty \), then
	\[
		\frac{N_{\triangle} - \binom{n}{3}p^3}{\sqrt{\Var_{}[A_1] } }
		\overset{D}{\to} \mathcal{N} (0, 1).
	\]
\end{theorem}
\begin{proof}
	Since we have
	\[
		\frac{\overline{N} _{\triangle}}{\sqrt{\Var_{}[A_1] } }
		= \frac{A_3}{\sqrt{\Var_{}[A_1] } } + \frac{A_2}{\sqrt{\Var_{}[A_1] } } + \frac{A_1}{\sqrt{\Var_{}[A_1] } }.
	\]
	For the first term, we see that it is \(\mathbb{E}_{}[A_3] ^2 = \frac{\Var_{}[A_3] }{\Var_{}[A_1] } \to 0\), same for the second term. However, for the last term, we have
	\[
		\frac{A_1}{\sqrt{\Var_{}[A_1] } }
		= \frac{\sum_{i < j} \overline{\omega }_{ij} }{\sqrt{\binom{n}{2} p (1 - p)} },
	\]
	where \(\sum_{i < j} \overline{\omega }_{ij} \sim \operatorname{Bin}(\binom{n}{2}, p) - \binom{n}{2}p \), and CLT applies.
\end{proof}

\begin{exercise}
	Find the CLT threshold for \(N_H\) for a fixed connected graph \(H\).
\end{exercise}

\begin{theorem}
	For \(\operatorname{ER}(n, \lambda / n) \) with \(\lambda > 1\). \(\operatorname{diam}(\mathcal{C} _{\max _1}) \approx c_\lambda \log n\).
\end{theorem}