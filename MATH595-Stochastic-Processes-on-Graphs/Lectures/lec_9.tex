\lecture{9}{18 Feb.\ 9:30}{Minimum Spanning Tree in Erdős-Rényi Graph}
\section{Minimum Spanning Tree}
Let \(\mathcal{T} _n\) be the set of all spanning trees on \([n]\), with \(\lvert \mathcal{T} _n \rvert = n^{n-2}\) from \hyperref[thm:Cayley-formula]{Cayley's formula}.

\begin{note}
	All spanning trees \(\tau \in \mathcal{T} _n\) has \((n-1)\) many edges.
\end{note}

We are now interested in the \hyperref[prb:MST]{minimum spanning tree} problem:

\begin{problem}[Minimum spanning tree]\label{prb:MST}
Given a connected graph \(G=(V, E)\) with edge capacity \(\omega \colon E \to \mathbb{R} _{+} \), the \emph{minimum spanning tree} (MST) problem aims to find the min-cost spanning tree \(\tau ^{\ast} \in \mathcal{T} _n\) with cost \(W_n \coloneqq \min _{\tau \in \mathcal{T} _n} W(\tau ) = W(\tau ^{\ast} )\), where \(W(\tau ) \coloneqq \sum_{e \in \tau } \omega _e \).
\end{problem}

\hyperref[prb:MST]{MST} is a well-studied problem in combinatorial optimization. We will focus on one famous algorithm called the Kruskal's algorithm, which works by sequentially (w.r.t. the edge weight) add edges with the largest weight as long as no cycle is created.

\subsection{Thresholding View of Erdős-Rényi Graph}
In particular, consider the complete graph \(G_n = K_n\) on \([n]\) nodes. Given a cdf \(F\), let \(\omega _e = \omega _{ij} = \omega _{ji} \overset{\text{i.i.d.} }{\sim } F\) on \([0, \infty )\) with \(e = (i, j)\) for all \(1 \leq i < j \leq n\). Without loss of generality, let \(0 \in \operatorname{supp}(F) \), and we may assume that \(F\) is continuous and \(F(x) / x^d \to 1\) as \(x \searrow 0^+\) for some \(d > 0\).

\begin{intuition}
	This means that the cdf generating the edge weight follows a polynomial decay.
\end{intuition}

\begin{eg}
	For \(\mathcal{U} (0, 1)\) and \(\operatorname{Exp}(1) \), the assumption holds for \(d=1\).
\end{eg}

\begin{theorem}
	Let \(G_n\) be the complete graph with \(n\) vertices and edge weight generated from \(F(x) / x^d \to 1\) as \(x \searrow 0^+\) for some \(d > 0\). The following convergences hold as \(n \to \infty \).
	\begin{itemize}
		\item When \(d = 1\),	\(\mathbb{E}_{}[W_n] \to \zeta (3) = \sum_{k=1}^{\infty} 1 / k^3\) and \(\sqrt{n} (W_n - \zeta (3)) \overset{D}{\to} \mathcal{N} (0, \sigma ^2)\) for some \(\sigma ^2 > 0\).
		\item For general \(d > 0\), we have
		      \[
			      n^{1 / d - 1} \mathbb{E}_{}[W_n]
			      \to \sum_{k=1}^{\infty} \frac{1}{dk^3} \frac{\Gamma (k + 1 / d - 1)}{\Gamma () k^{1 / d - }}
			      \eqqcolon c_d,
		      \]
		      and \(\frac{1}{\sqrt{n} } (n^{1 / d} W_n - n c_d) \overset{D}{\to} \mathcal{N} (0, \sigma _d^2)\) for some \(\sigma _d^2 > 0\).
	\end{itemize}
\end{theorem}
\begin{proof}
	Consider the \hyperref[prb:MST]{MST} \(\tau _n^{\ast} \) found by the Kruskal's algorithm. Focus on the case of \(d = 1\) such that \(F(x) = x\) for all \(0 \leq x \leq 1\). Then, we see that
	\[
		W_n
		= \sum_{e \in \tau _n^{\ast} } \omega _e
		= \sum_{e \in \tau _n^{\ast} } \int_{0}^{1} \mathbbm{1}_{x \leq \omega _e} \,\mathrm{d}x
		= \int_{0}^{1} \sum_{e \in \tau _n^{\ast} } (1 - \mathbbm{1}_{x > \omega _e} ) \,\mathrm{d}x
		= \int_{0}^{1} \left( n-1 - \sum_{e \in \tau _n^{\ast} } \mathbbm{1}_{\omega _e < x} \right) \,\mathrm{d}x .
	\]
	We see that:
	\begin{itemize}
		\item all edges \(e\) with weight \(\omega _e \leq x\) gives the edge set of \(\operatorname{ER}(n, x) \);
		\item \(n - \sum_{\mathcal{C} \colon \tau \text{ spans } \mathcal{C} } \lvert E(\tau ) \rvert \) is the total number of components.
	\end{itemize}
	Hence, by letting \(C_n(x)\) denotes the number of components in \(\operatorname{ER}(n, x) \), we further have
	\[
		W_n
		= \int_{0}^{1} \left( \text{\#components in } \operatorname{ER}(n, x) - 1 \right) \,\mathrm{d}x
		= \int_{0}^{1} (C_n(x) - 1) \,\mathrm{d}x .
	\]
	\begin{note}
		For general \(F\), we have \(W_n = \int_{0}^{1} (C_n(x) - 1) \,\mathrm{d}F^{-1} (x) \).
	\end{note}

	For some \(A > 1\) chosen later, consider decomposing the integral into
	\[
		W_n
		= \int_{0}^{A \log n / n} C_n(x) \,\mathrm{d}x
		- \underbrace{\int_{0}^{A \log n / n} 1 \,\mathrm{d}x}_{A \log n / n}
		+ \int_{A \log n / n}^{1} (C_n(x) - 1) \,\mathrm{d}x .
	\]
	The last integral can be bounded with \autoref{thm:Erdős-Rényi-connectivity-threshold} as
	\[
		\begin{split}
			\int_{A \log n / n}^{1} (C_n(x) - 1) \,\mathrm{d}x
			 & = \int_{A \log n / n}^{1} \mathbb{E}_{}[\lvert C_n(x) - 1 \rvert ]  \,\mathrm{d}x                                  \\
			 & \leq \int_{A \log n / n}^{1} n \Pr_{}\left(\operatorname{ER}(n, x) \text{ is not connected} \right)  \,\mathrm{d}x
			\leq n^{2 - A}.
		\end{split}
	\]
	Take \(A = 3\), and let \(x = \lambda / n\), the first integral becomes
	\[
		\int_{0}^{3 \log n / n} C_n(x) \,\mathrm{d}x
		= \int_{0}^{3 \log n} \frac{1}{n} C_n(\lambda / n) \,\mathrm{d}\lambda .
	\]
	Since we care about the number of components, in this regime (i.e., \(\lambda \in (0, 3 \log n)\)), we know that only one giant component can exist, which doesn't matter since it contributes only \(1 / n\), we're really interested in the number of small components.

	Next, consider decomposing \(C_n(\lambda / n)\) into the number of components that are trees plus the number of components that are non-trees. The second quantity, in expectation, is bounded by \(O(1+ 1 / n)\). While for the first quantity, we see that
	\[
		\sum_{k=1}^{n} \underbrace{( \text{\#tree-component of size \(k\)} )}_{N_n(k, \lambda )} + O_p(1),
	\]
	where \(N_n(k, \lambda )\) is the number of tree-components of size \(k\) in \(\operatorname{ER}(n, \lambda / n) \). Overall, we have
	\[
		W_n
		= \Theta \left( \frac{\log n}{n} \right) + \sum_{k=1}^{n} \int_{0}^{3 \log n} \frac{1}{n} N_n(k, \lambda ) \,\mathrm{d}\lambda.
	\]
	From \hyperref[thm:Cayley-formula]{Cayley's formula},
	\[
		\mathbb{E}_{}[N_n(k, \lambda )]
		= k^{k-2} \binom{n}{k} \left( \frac{\lambda}{n} \right) ^{k-1} \left( 1 - \frac{\lambda}{n} \right) ^{k(n-k) + \binom{k}{2} - k + 1},
	\]
	as \(n \to \infty \), we have
	\[
		\mathbb{E}_{}[W_n]
		\to \sum_{k=1}^{\infty} \frac{k^{k-2}}{k!} \int_{0}^{\infty} \lambda ^{k-1} e^{-\lambda k} \,\mathrm{d}x
		= \sum_{k=1}^{\infty} \frac{k^{k-2}}{k!} k^{- k} (k-1)!
		= \sum_{k=1}^{\infty} \frac{1}{k^3}
		= \zeta (3).
	\]

	Now, for \(k\) and \(\lambda \) fixed, we know that as \(n \to \infty \),
	\[
		\frac{1}{n}\mathbb{E}_{}[N_n(k, \lambda )]
		\to \frac{k^{k-2}}{k!} \lambda ^{k-1} e^{-\lambda k}
		\eqqcolon m(k, \lambda )
	\]
	and
	\[
		\frac{1}{n} \Var_{}[N_n(k, \lambda )]
		\to \sigma _{k, \lambda }^2
		> 0.
	\]
	Then, we have
	\[
		\frac{N_n(k, \lambda ) - n \cdot m(k, \lambda )}{\sqrt{n} }
		\overset{D}{\to} \mathcal{N} (0, \sigma _{k, \lambda }^2).
	\]
	Moreover, we can let \(\lambda \) varies and make the above a process of \(\lambda \), and one can show that this actually converges:
	\[
		\left( \frac{N_n(k, \cdot ) - n \cdot m(k, \cdot )}{\sqrt{n} } \right) _{\lambda \in (0, \infty )}
		\overset{D}{\to} \operatorname{GP}(0, \Sigma ),
	\]
	i.e., some mean zero Gaussian process with certain covariance structure. One can further consider all \(k\), and the above further becomes
	\[
		\left( \left( \frac{N_n(\cdot , \cdot ) - n \cdot m(\cdot, \cdot )}{\sqrt{n} } \right) _{\lambda \in (0, \infty )} \right) _{k = 1, 2, \dots }
		\overset{D}{\to} \left( \operatorname{GP}(0, \Sigma ) \right) _{k = 1, 2, \dots }.
	\]
\end{proof}

\begin{exercise}
	Give a simple proof with a rate of convergence.
\end{exercise}

\subsection{Extension}
One can also consider the Gibbs measure on \(\mathcal{T} _n\): let \(0 < \beta < \infty \), then for \(\tau \in \mathcal{T} _n\), define
\[
	\mathbb{P} _{\beta , n}(\tau )
	= \frac{\exp (-\beta \cdot n \cdot W(\tau ))}{Z_n(\beta )}
\]
with the partition function \(Z_n(\beta ) = \sum_{\tau \in \mathcal{T} _n} e^{-\beta n W(\tau )}\).

\begin{intuition}
	This measure interpolates the uniform spanning tree and the minimum spanning tree:
	\begin{itemize}
		\item when \(\beta \to 0\), all \(\tau \in \mathcal{T} _n\) receives the same probability mass, hence sampling from \(\mathbb{P} _{\beta , n}\) gives a uniformly random spanning tree;
		\item when \(\beta \to \infty \), the difference between \(W(\theta )\) is amplified, hence the \hyperref[prb:MST]{MST} \(\tau ^{\ast} \) with \(W_n\) receives the largest probability mass.
	\end{itemize}
\end{intuition}

More questions can be asked in this context:

\begin{problem*}
	If we sample \(\tau _n^{\ast} \) from \(\mathbb{P} _{\beta , n}\),
	\begin{enumerate}[(a)]
		\item What is \(\lim_{n \to \infty} \mathbb{E}_{}[W(\tau _n^{\ast} )] \)?
		\item What is \(\lim_{n \to \infty} \frac{1}{n} \log Z_n(\beta )\) and the \(\sigma ^2\) in \(\frac{1}{\sqrt{n} } (\log Z_n(\beta ) - \mathbb{E}_{}[\log Z_n(\beta )] ) \overset{D}{\to} \mathcal{N} (0, \sigma ^2)\)?
		\item What is the structure of \(\tau _n^{\ast} \) (local sense/metric sense)?
	\end{enumerate}
\end{problem*}

\chapter{Other Random Graph Models}
We have a thorough discussion on \hyperref[def:Erdős-Rényi-random-graph]{Erdős-Rényi random graph} model. However, as we have seen, the degree distribution is thin-tailed: due to the independent structure of various random variables involved. Real-life networks oftentimes are heavy-tailed, especially the degree distribution. In this chapter, we aim to explore other random graph models that exhibit heavy-tailed behaviors.

\section{Some Proposed Models}
Let's begin by first looking at a closely related model called exponential random graph model, which, unfortunately, is still thin-tailed. In some sense, it tries to specific the subgraph densities for various subgraphs. Next, we look at the inhomogeneous random graph model, which tries to directly specify the degrees. However, both of the proposals fail to generate heavy-tailed behaviors.

\subsection{Exponential Random Graph}
Consider \(G \sim \operatorname{ER}(n, p) \) for some \(p \in (0, 1)\). We see that
\[
	\Pr_{}\left(G\right)
	= p^{m} (1 - p)^{\binom{n}{2} - m}
	= (1 - p)^{\binom{n}{2}} e^{\log \frac{p}{1-p} \cdot m}.
\]

\begin{notation}
	Let \(\beta (p) \coloneqq \log (p / (1 - p))\) where \(\beta \colon [0, 1] \to (-\infty , \infty )\).
\end{notation}

Hence, given \(\beta \), \(\operatorname{ER}(n, p) \) with \(p = e^\beta / (1 + e^{\beta })\), which is the same model \(\Pr_{}\left(G\right) \propto e^{\beta \cdot m}\). This motivates us to define the following random graph model:

\begin{definition}[Exponential random graph]\label{def:exponential-ranom-graph}
	Fix some finite subgraphs \(H_1, \dots , H_k\) with \(\beta _1, \dots , \beta _k \in (-\infty , \infty )\). Then, the \emph{exponential random graph}, denoted as \(G \sim \operatorname{ERGM}(\beta _1, \dots , \beta _k) \), satisfies
	\[
		\Pr_{}\left(G\right)
		\propto \exp (n^2 \sum_{i=1}^{k} \beta _i \frac{\text{\#\(H_i\) in \(G\)}}{\text{\#\(H_i\) in \(K_n\)}} )
		= \exp (n^2 \sum_{i=1}^{k} \beta _k t(H_k, G) ).
	\]
\end{definition}

\begin{intuition}
	By specifying the probability \(p(\beta _i)\), \(\operatorname{ER}(n, p(\beta _i)) \) gives a graph with probability \(e^{\beta m}\). Here, we do the similar construction such that by specifying all the desired subgraphs \(H_i\) and \(\beta _i\), \(\operatorname{ERGM}(\beta _1, \dots , \beta _k) \) encourages each \(H_i\) appear with the desired subgraph density according to \(\beta _i\).
\end{intuition}

\begin{problem*}[Open problem]
	CLT for subgraph count in \(\operatorname{ERGM}(\beta _1, \dots , \beta _k) \).
\end{problem*}