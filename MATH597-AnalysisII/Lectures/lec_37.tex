\chapter{Hilbert Spaces}
\lecture{37}{11 Apr. 11:00}{Hilbert Spaces}
\section{Inner Product Spaces}
\begin{definition}[Inner product]\label{def:inner-product}
	Let \(V\) be a (complex) vector space. An \emph{inner product} is a function
	\[
		\left< \cdot,\cdot \right> \colon V \times V \to \mathbb{C}
	\]
	such that
	\begin{itemize}
		\item \(\left< \alpha x + \beta y, z \right> = \alpha \left< x , z \right> + \beta \left< y, z \right>\) for all \(x,y,z \in V\), and \(\alpha,\beta \in \mathbb{C}\).
		\item \(\left< x,y \right> = \overline{\left< y,x \right>}\) for every \(x,y \in V\).
		\item \(\left< x,x \right> \in [0,\infty)\), and \(\left< x,x \right> = 0\) if and only if \(x = 0\).
	\end{itemize}
\end{definition}
\begin{note}
	Note that we have conjugate linearity in the second argument, i.e.,
	\[
		\left< x, \alpha y + \beta z \right> = \overline{\alpha}\left< x,y \right> + \overline{\beta}\left< x,z \right>
	\]
	for any \(x,y,z \in V\) and \(\alpha,\beta \in \mathbb{C}\).
\end{note}
\begin{explanation}
	This follows from
	\[
		\left< x, \alpha y + \beta z \right>
		= \overline{\left< \alpha y + \beta z, x \right> }
		= \overline{\alpha \left< y, x \right> + \beta \left< z, x \right> }
		= \overline{\alpha} \overline{\left< y, x \right>} + \overline{\beta} \overline{\left< z, x \right>}.
		= \overline{\alpha} \left< x, y \right> + \overline{\beta} \left< x, z \right>.
	\]
\end{explanation}

\begin{eg}
	We have the following examples.
	\begin{itemize}
		\item \(\mathbb{R}^d\) with \(\left< x,y \right> = x \cdot y = \sum_{i=1}^d x_i y_i\).
		\item \(\mathbb{C}^d\) with \(\left< x,y \right> = \sum_{i=1}^d x_i \overline{y_i}\).
		\item \(L^2(X,\mu)\) with \(\left< f,g \right> = \int_X f\overline{g}\,\mathrm{d} \mu\). Note by \autoref{thm:Holder-inequality},
		      \[
			      \left\vert \int_X f \overline{g} \right\vert \leq \left\lVert f\overline{g}\right\rVert_1 \leq \left\lVert f\right\rVert_2 \left\lVert g\right\rVert_2 < \infty
		      \]
		      because \(1/2 + 1/2 = 1\).
		\item A special case is \(\ell^2\), where we have
		      \[
			      \left< x,y \right>  = \sum_{i=1}^\infty x_i\overline{y_i}.
		      \]
	\end{itemize}
\end{eg}

\begin{note}
	Note that
	\[
		\left\lVert x + y\right\rVert^2
		= \left< x + y, x + y \right>
		= \left< x,x \right> + \left< x,y \right>  + \left< y,x \right>  + \left< y,y \right>
		= \left\lVert x\right\rVert^2 + 2\Re \left< x,y \right> + \left\lVert y\right\rVert^2
	\]
\end{note}

\begin{theorem}[Cauchy-Schwarz Inequality]\label{thm:Cauchy-Schwarz-inequality}
	Given an \hyperref[def:inner-product]{inner product} space, \(\left\vert \left< x,y \right> \right\vert \leq \left\lVert x\right\rVert \left\lVert y\right\rVert\).
\end{theorem}
\begin{proof}
	This is clear if \(\left< x,y \right> = 0\). Assume \(\left< x,y \right> \neq 0\), then for every \(\alpha \in \mathbb{C}\), we know that
	\[
		0 \leq \left\lVert \alpha x - y\right\rVert^2 = \left\vert \alpha \right\vert^2\left\lVert x\right\rVert^2 - 2\Re \alpha\left< x,y \right> + \left\lVert y\right\rVert^2.
	\]
	Write \(\left< x,y \right> = \left\vert \left< x,y \right> \right\vert e^{i\theta}\), and take \(\alpha = e^{-i\theta}t\) for arbitrary \(t \in \mathbb{R}\).
	Then, the right-hand side gives
	\[
		0 \leq \left\lVert x\right\rVert^2t^2 - 2 \left\vert \left< x,y \right> \right\vert t + \left\lVert y\right\rVert^2.
	\]
	Note this is a real quadratic function of \(t\), with at most one real root. Thus, the discriminant \(\Delta \leq 0\).

	Specifically, we have
	\[
		\Delta = 4\left\vert \left< x,y \right> \right\vert^2 - 4\left\lVert x\right\rVert^2 \left\lVert y\right\rVert^2  \leq 0
		\iff \left\vert \left< x,y \right> \right\vert^2\leq \left\lVert x\right\rVert^2 \left\lVert y\right\rVert^2
		\iff \left\vert \left< x,y \right> \right\vert\leq \left\lVert x\right\rVert\left\lVert y\right\rVert.
	\]
\end{proof}

\begin{definition}[Induced norm from inner product]\label{def:induced-norm-from-inner-product}
	Given an \hyperref[def:inner-product]{inner product} space \(V\), let
	\[
		\left\lVert x\right\rVert \coloneqq \sqrt{\left< x,x \right>},
	\]
	which is so-call the \emph{norm induced from the \hyperref[def:inner-product]{inner product}}.
\end{definition}
\begin{proof}
	We need to check that this actually defines a \hyperref[def:norm]{norm}. We check the following.
	\begin{claim}
		\(\left\lVert x\right\rVert = 0 \iff x = 0\) for all \(x\in V\).
	\end{claim}
	\begin{explanation}
		This follows from the \hyperref[def:inner-product]{definition of an inner product}.
	\end{explanation}

	\begin{claim}
		\(\left\lVert \alpha x\right\rVert = \left\vert \alpha \right\vert \left\lVert x\right\rVert \) for all \(x\in V\).
	\end{claim}
	\begin{explanation}
		This follows from
		\[
			\left\lVert \alpha x\right\rVert = \sqrt{\left< \alpha x, \alpha x \right> } = \sqrt{\alpha \overline{\alpha}\left< x,x \right> }  = \left\vert \alpha \right\vert \left\lVert x\right\rVert.
		\]
	\end{explanation}

	\begin{claim}[Triangle inequality]
		\(\left\lVert x+y\right\rVert \leq \left\lVert x\right\rVert + \left\lVert y\right\rVert \) for all \(x, y\in V\).
	\end{claim}
	\begin{explanation}
		The triangle inequality is less obvious, and comes from \autoref{thm:Cauchy-Schwarz-inequality}. Namely,
		\[
			\begin{split}
				\left\lVert x + y\right\rVert^2
				&= \left\lVert x\right\rVert^2 + 2\Re\left< x,y \right> + \left\lVert y\right\rVert^2 \\
				&\leq \left\lVert x\right\rVert^2 + 2\left\vert \left< x,y \right> \right\vert + \left\lVert y\right\rVert^2 \\
				&\leq \left\lVert x\right\rVert^2 + 2\left\lVert x\right\rVert \left\lVert y\right\rVert + \left\lVert y\right\rVert^2
				= (\left\lVert x\right\rVert + \left\lVert y\right\rVert)^2
			\end{split}
		\]
		Taking square root on both sides, we have
		\[
			\left\lVert x + y\right\rVert \leq \left\lVert x\right\rVert + \left\lVert y\right\rVert.
		\]
	\end{explanation}
\end{proof}

\begin{theorem}[Parallelogram law]\label{thm:parallelogram-law}
	Let \(V\) be a \hyperref[def:norm]{normed vector space}. Then, \(\left\lVert \cdot\right\rVert\) is
	\hyperref[def:induced-norm-from-inner-product]{induced by an inner product} if and only if
	\[
		\left\lVert x + y\right\rVert^2 + \left\lVert x-y\right\rVert^2 = 2\left\lVert x\right\rVert^2 + 2\left\lVert y\right\rVert^2
	\]
	for all \(x,y \in V\).
\end{theorem}
\begin{proof}
	We show two directions.
	\paragraph{\((\implies )\)}
	This follows from
	\[
		\left\lVert x \pm y\right\rVert^2 = \left\lVert x\right\rVert^2 \pm 2\Re\left< x,y \right> + \left\lVert y\right\rVert^2
	\]
	and
	\[
		\left\lVert x \pm iy\right\rVert ^2 = \left\lVert x\right\rVert^2 \pm 2\Im \left< x,y \right> + \left\lVert y\right\rVert^2.
	\]
	\paragraph{\((\impliedby)\)}
	Firstly, we define
	\[
		\left< x,y \right> = \frac{1}{4}\left(\left\lVert x + y\right\rVert^2 - \left\lVert x - y\right\rVert^2 + \left\lVert x + iy\right\rVert^2 - i\left\lVert x - iy\right\rVert ^2\right)
	\]
	as motivated by the above relationship.
	\begin{exercise}
		Check this \hyperref[def:induced-norm-from-inner-product]{inner product is indeed inducing the desired norm}.
	\end{exercise}
\end{proof}

\begin{eg}
	Consider \(L^p(\mathbb{R},m)\), \(f = \mathbbm{1}_{(0,1)}\), \(g = \mathbbm{1}_{(1,2)}\). We see the \hyperref[thm:parallelogram-law]{parallelogram law}
	is satisfied only when \(p = 2\).

	\begin{remark}
		Hence, \(L^p(\mathbb{R},m)\) is only an \hyperref[def:inner-product]{inner product} space when \(p = 2\).
	\end{remark}
\end{eg}

Since we're doing real analysis, we want to deal with limits. It turns out that with an \hyperref[def:inner-product]{inner product} space,
we can say something more compare to the case of a \hyperref[def:norm]{normed} vector space. We now illustrate this.
\begin{definition*}
	Given a vector space \(V\) with either a \hyperref[def:norm]{norm} or an \hyperref[def:inner-product]{inner product}, we have the followings.
	\begin{definition}[Strong convergence]\label{def:strong-convergence}
		We say that \emph{\(x_n \in V\) converges to \(x \in V\) strongly} if
		\[
			\lim\limits_{n \to \infty} \left\lVert x_{n} - x\right\rVert = 0.
		\]
	\end{definition}
	\begin{definition}[Weak convergnece]\label{def:weak-convergence}
		We say that \emph{\(x_n \in V\) converges to \(x \in V\) weakly} if for any fixed \(y \in V\),
		\[
			\lim\limits_{n \to \infty} \left< x_n -x,y \right> = 0.
		\]
	\end{definition}
\end{definition*}


\begin{lemma}[Strong convergence implies weak convergence]
	Suppose \(V\) is an \hyperref[def:inner-product]{inner product} space. If \hyperref[def:strong-convergence]{\(x_n \to x\) strongly},
	then \hyperref[def:weak-convergence]{\(x_n \to x\) \emph{weakly}}.
\end{lemma}
\begin{proof}
	By \hyperref[thm:Cauchy-Schwarz-inequality]{Cauchy-Schwarz inequality},
	\[
		0 \leq \abs{\left< x_n -x , y \right> } \leq \left\lVert x_n - x\right\rVert \cdot \left\lVert y\right\rVert.
	\]
	Since \(\left\lVert x_n - x\right\rVert\to 0\) and \(\left\lVert y\right\rVert\) is constant in \(n\), from the Squeeze theorem, we have
	\[
		\left< x_n -x, y \right> \to 0
	\]
	as \(n\to \infty \).
\end{proof}

\begin{eg}
	Consider \(\ell^2\), \(x_n = (0,\ldots,0,1,0,\ldots)\) and \(x = 0\). Then \(x_n\) does not \hyperref[def:strong-convergence]{converge strongly} to any vector,
	but it does \hyperref[def:weak-convergence]{converge to \(0\) weakly}.
\end{eg}
\begin{explanation}
	If we fix \(y \in \ell^2\), then
	\[
		\left< x_n - x, y \right> = \overline{y}_n
	\]
	which goes to \(0\) as \(n \to \infty\) because \(\sum_n \left\vert y_n \right\vert ^2 < \infty\). Therefore, \hyperref[def:weak-convergence]{\(x_n \to 0\) weakly},
	but we see that
	\[
		\left\lVert x_n - 0\right\rVert = \left\lVert x_n\right\rVert = 1.
	\]
	Thus, \hyperref[def:strong-convergence]{\(x_n \not\to 0\) strongly}.
\end{explanation}

\subsection{Orthonormal Bases}
\begin{definition}[Orthogonal]\label{def:orthogonal}
	Two vectors \(x, y\) are \emph{orthogonal} if \(\left< x,y \right> = 0\), denoted as \(x \perp y\).
\end{definition}
\begin{remark}
	Do not confuse between this notation and \autoref{def:singular}.
\end{remark}

\begin{lemma}[Pythagorean Theorem]\label{thm:Pythagorean-theorem}
	If \(x_1,\ldots,x_n \in V\), \(\left< x_i,x_j \right> = 0\) for all \(i \neq j\), then
	\[
		\left\lVert \sum\limits_{i=1}^{n} x_{i} \right\rVert ^{2} = \sum\limits_{i=1}^{n} \left\lVert x_{i} \right\rVert ^{2}.
	\]
\end{lemma}
\begin{proof}
	Use that \(\left\lVert x + y\right\rVert =\left\lVert x\right\rVert^2 + 2\Re\left< x,y \right> + \left\lVert y\right\rVert^2\) and induct.
\end{proof}

\begin{definition}[Orthonormal set]\label{def:orthonormal-set}
	We call \(\{e_i\}_{i \in I}\) an \emph{orthonormal set} if
	\[
		\left< e_i,e_j \right> = \begin{dcases}
			0, & \text{ if } i \neq j ; \\
			1, & \text{ if } i = j .
		\end{dcases}
	\]
\end{definition}