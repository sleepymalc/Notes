\lecture{15}{27 Sep.\ 9:00}{Parametric v.s.\ Non-Parametric}
\subsection{Parametric versus Non-Parametric Function Classes}
We start by asking the following question.

\begin{problem*}
	What makes a function class ``parametric'' or ``non-parametric''?
\end{problem*}
\begin{answer}
	If the function class is a vector space, we can usually use the linear algebra notion of dimensio.
\end{answer}

Consider the following (not very precise) definition in terms of the \hyperref[def:Koltchinskii-Pollard-entropy]{uniform \(L_2\) entropy}.

\begin{definition}[Parametric]\label{def:parametric}
	A function class \(\mathscr{F} \) is \emph{parametric} if there exists a notion of dimension \(\dim \mathscr{F} \) and a constant \(C\) such that
	\[
		\sup _\mu N(\mathscr{F} , L_2(\mu ), \epsilon ) \leq \left( \frac{C}{\epsilon } \right) ^{\dim(\mathscr{F} )}.
	\]
\end{definition}

\begin{eg}
	Boolean function class on \(\chi \) with finite \hyperref[def:VC-dimension]{VC dimension} is \hyperref[def:parametric]{parametric}.
\end{eg}
\begin{explanation}
	\hyperref[thm:Dudley]{Dudley's result} directly applies.
\end{explanation}

\begin{definition}[Non-parametric]\label{def:non-parametric}
	A function class \(\mathscr{F} \) is \emph{non-parametric} if there is a \(p > 0\) and a constant \(C\) such that
	\[
		\sup _\mu \log N(\mathscr{F} , L_2(\mu ), \epsilon ) \leq \left( \frac{C}{\epsilon } \right) ^p.
	\]
\end{definition}

Let's consider any \hyperref[def:parametric]{parametric} class \(\mathscr{F} \) uniformly bounded by \(1\) with \(\dim \mathscr{F} = d\). Then from \hyperref[thm:Dudley]{Dudley's theorem}, the \hyperref[def:Rademacher-complexity]{Rademacher complexity} can be bounded as
\[
	\mathbb{E}_{x}\left[\mathbb{E}_{\epsilon }\left[\sup _{f\in \mathscr{F} } \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(x_i) \right]  \right]
	\leq \frac{12}{\sqrt{n} } \int_{0}^{1} \sqrt{\sup _\mu \log N(\mathscr{F} , L_2(\mu ), \epsilon )} \,\mathrm{d}\epsilon
	\leq \frac{12}{\sqrt{n} } \int_{0}^{1} \sqrt{d \log \frac{C}{\epsilon }} \,\mathrm{d}\epsilon
	\leq C^{\prime} \sqrt{\frac{d}{n}}.
\]
Hence, we get the \hyperref[def:parametric]{parametric} rate \(O(\sqrt{d / n} )\), and this result is distribution-free.

Analogously, we want to know what we will get for a \hyperref[def:non-parametric]{non-parametric} function class (uniform bounded by \(1\))? Now, since for a \hyperref[def:non-parametric]{non-parametric} class, the \hyperref[def:Koltchinskii-Pollard-entropy]{uniform \(L_2\) entropy} is \(\leq (C / \epsilon )^p\),
\begin{align*}
	 & \mathbb{E}_{\epsilon }\left[  \sup _{f\in \mathscr{F} } \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \epsilon _i f(x_i)\right] \tag*{\(\displaystyle \frac{1}{\sqrt{n} } \sum_{i=1}^{n} \epsilon _i f(x_i) = X_f \sim \Subg(L_2(\mathbb{P} _n))\)} \\
	 & \leq \mathbb{E}_{}\left[\sup _{\substack{f, g\in \mathscr{F} \colon                                                                                                                                                                      \\ L_2(\mathbb{P} _n) (f, g) \leq \delta }} X_f - X_g \right] + \int_{\delta }^{1} \sqrt{\sup _\mu \log N(\mathscr{F} , L_2(\mu ), \epsilon )} \,\mathrm{d}\epsilon \tag*{modified \autoref{col:Dudley-integral-entropy-bound-finite-resolution}} \\
	 & \leq \sqrt{n} \cdot \delta + \int_{\delta }^{1} \left( \frac{C}{\epsilon } \right) ^{p / 2} \,\mathrm{d}\epsilon
\end{align*}

Since the choice of \(\delta \) is arbitrary, we can optimize in terms of \(\delta \). There are three cases:
\begin{itemize}
	\item \(p < 2\): Take \(\delta = 0\) because the integral converges, and we get a \hyperref[def:parametric]{parametric} rate bound for \(R_n\) with some constant \(c\):
	      \[
		      \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(x_i) \right] \leq \frac{c}{\sqrt{n} }.
	      \]
	      \begin{remark}
		      \(O(1 / \sqrt{n} )\) is a \hyperref[def:parametric]{parametric} rate.
	      \end{remark}
	      \begin{eg}
		      Consider \hyperref[def:Holder-smooth-function-class]{Hölder smooth classes}. Even though these function classes are \hyperref[def:non-parametric]{non-parametric} according to \autoref{def:non-parametric}, in terms of \(R_n\) or supremum of \hyperref[def:EP]{empirical process}, the rate is still \hyperref[def:parametric]{parametric}.
	      \end{eg}
	\item \(p > 2\): We see that for all \(\delta \in (0, 1)\),
	      \begin{align*}
		      \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(x_i) \right]
		       & \leq \delta + \frac{1}{\sqrt{n} } \int_{\delta }^{1} \left( \frac{C}{\epsilon } \right) ^{p / 2} \,\mathrm{d}\epsilon        \\
		       & = \delta + \frac{C^{p / 2}}{\sqrt{n} } \cdot \at{ \frac{\epsilon ^{-p / 2 + 1}}{1 - p / 2}}{\delta }{1}                      \\
		       & \approx \delta + \frac{1}{\sqrt{n} } \at{(-\epsilon ^{-p / 2 + 1})}{\delta }{1} \tag*{dropping constant (\(1 - p / 2 < 0\))} \\
		       & \approx \delta + \frac{1}{\sqrt{n} } \delta ^{1 - p / 2}.
	      \end{align*}
	      Now by optimizing over \(\delta \), setting \(\delta = \delta ^{1 - p / 2} / \sqrt{n} \), we get the bound \(O(n^{-1 / p})\).
	      \begin{remark}
		      \(O(n^{-1 / p})\) is a \hyperref[def:non-parametric]{non-parametric} rate, strictly slower than the \hyperref[def:parametric]{parametric} rate \(O(n^{-1 / 2})\).
	      \end{remark}

	      This upper bound is also tight for certain function classes.
	      \begin{eg}
		      For \(1\)-bounded and \(1\)-Lipschitz functions on \([0, 1]^d\), the \hyperref[def:Koltchinskii-Pollard-entropy]{uniform \(L_2\) entropy} (in fact the \(L_\infty \) \hyperref[def:metric-entropy]{entropy}) grows like \((1 / \epsilon )^d\).
	      \end{eg}
	      \begin{explanation}
		      Since \(\vert f(x) - f(y) \vert \leq \lVert x - y \rVert _2\), \(O(n^{-1 / d})\) rate here is tight for \(d > 2\).
	      \end{explanation}
	\item \(p = 2\): From the same calculation, we have
	      \[
		      \mathbb{E}_{}\left[\sup _{f\in \mathscr{F} } \frac{1}{n}\sum_{i=1}^{n} \epsilon _i f(x_i) \right]
		      \leq \delta + \frac{1}{\sqrt{n} } \int_{\delta }^{1} \frac{C}{\epsilon } \,\mathrm{d}\epsilon
		      = \delta + \frac{C}{\sqrt{n} } \ln \frac{1}{\delta}
		      = O\left( \frac{1}{\sqrt{n} } \log n \right)
	      \]
	      by setting \(\delta = O(1 / \sqrt{n} )\). Hence, the \hyperref[def:Rademacher-complexity]{Rademacher complexity} is bounded by \(\log n / \sqrt{n} \), which is ``almost'' the \hyperref[def:parametric]{parametric} rate up to the extra \(\log \) factor. This might not be tight.
\end{itemize}

\begin{remark}
	To summarize, we have the following bounds on the \hyperref[def:Rademacher-complexity]{Rademacher complexity}:
	\begin{itemize}
		\item \hyperref[def:parametric]{Parametric class}: \(C \sqrt{d / n} \).
		\item \hyperref[def:non-parametric]{Non-parametric class}:
		      \begin{itemize}
			      \item \(p < 2\): \(C \sqrt{1 / n} \);
			      \item \(p = 2\): \(C \log n / \sqrt{n} \);
			      \item \(p > 2\): \(C \cdot n^{-1 / p}\).
		      \end{itemize}
	\end{itemize}
\end{remark}

\begin{eg}[Linear function class]
	Let \(\chi = B_2^d\), and \(\mathscr{F} = \{ x \mapsto w^{\top} x \colon w\in B_2^d \} \). For a given data \(x_1, \dots , x_n \in \mathbb{R} ^d\),
	\[
		\at{\mathscr{F}}{x_1, \dots , x_n}{} = \left\{ X w \colon w\in B_2^d, X_{n \times d} = \begin{bmatrix}
			x_1 ^{\top} \\
			\vdots      \\
			x_n ^{\top} \\
		\end{bmatrix} \right\}.
	\]
	To determine whether \(\mathscr{F} \) is \hyperref[def:parametric]{parametric} or \hyperref[def:non-parametric]{non-parametric}, we need to bound \(N(\mathscr{F} , L_2(\mathbb{P} _n), \epsilon )\):
	\[
		\begin{alignedat}{4}
			& \sqrt{\frac{1}{n}\sum_{i=1}^{n} \left( \langle w, x_i \rangle - \langle w^{\prime} , x_i \rangle \right) ^2 } &                    & N(\mathscr{F} , L_2(\mathbb{P} _n), \epsilon )                \\
			& \leq \max _{i\in[n]} \vert \langle w-w^{\prime} , x_i \rangle \vert                                           &                    & \leq N(\mathscr{F} , L_\infty (\mathbb{P} _n), \epsilon )     \\
			& \leq \max _{x\in B_2^d} \vert \langle w - w^{\prime} , x \rangle  \vert                                       & \qquad \iff \qquad & \leq N(\mathscr{F} , \lVert \cdot \rVert _\infty , \epsilon ) \\
			& \leq \lVert w - w^{\prime} \rVert _2                                                                          &                    & \leq N(B_2^d, \lVert \cdot \rVert _2, \epsilon )              \\
			& \leq \epsilon,                                                                                                &                    &
		\end{alignedat}
	\]
	where \(\max _{x\in B_2^d} \vert \langle w - w^{\prime} , x \rangle \vert \leq \lVert w - w^{\prime}  \rVert _2\) since \(\lVert x \rVert _2 \leq 1\). Then, from \autoref{prop:packing-covering},
	\[
		N(B_2^d, \lVert \cdot \rVert _2, \epsilon )
		\leq \left( 1 + \frac{2}{\epsilon } \right)^d,
	\]
	so we get a \(\sqrt{d / n} \) rate since this satisfies \hyperref[def:parametric]{parametric} condition.\footnote{In high dimension situation, this bound can be loose.}
\end{eg}

It turns out that one can use the norm constraints to also show that
\[
	\sup _\mu \log N(\mathscr{F} , L_2(\mathbb{P} _n), \epsilon ) \leq \frac{C}{\epsilon ^2},
\]
i.e., a dimension-free bound, hence \(\mathscr{F} \) also behaves like a \hyperref[def:non-parametric]{non-parametric} class! This bound will be useful in high dimensional setting when \(d\) is not small compared to \(n\). Thus, we make this an important remark.

\begin{remark}
	A function class can be viewed as \hyperref[def:parametric]{parametric} and \hyperref[def:non-parametric]{non-parametric} at the same time.
\end{remark}

There are other examples.

\begin{eg}
	Neural networks are like this: we can either measure its complexity by the \emph{number of parameters}, or get dimension independent bounds on its \hyperref[def:Rademacher-complexity]{Rademacher complexity} by using \emph{norm constraints}.
\end{eg}

\begin{problem*}
	For what function classes can be bound in terms of uniform \(L_2\) \hyperref[def:metric-entropy]{entropy}?
\end{problem*}
\begin{answer}
	We have seen boolean function classes with finite \hyperref[def:VC-dimension]{VC dimension}, and \hyperref[def:Holder-smooth-function-class]{Hölder smooth function classes}.
\end{answer}